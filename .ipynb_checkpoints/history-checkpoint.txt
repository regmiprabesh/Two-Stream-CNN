 1/1: # Square root:
 1/2: # Square:
 1/3:
s = 'hello'
# Print out 'e' using indexing
 1/4:
s ='hello'
# Reverse the string using slicing
 1/5:
s ='hello'
# Print out the 'o'

# Method 1:
 1/6: # Method 2:
 1/7: # Method 1:
 1/8: # Method 2:
 1/9: list3 = [1,2,[3,4,'hello']]
1/10: list4 = [5,3,4,6,1]
1/11:
d = {'simple_key':'hello'}
# Grab 'hello'
1/12:
d = {'k1':{'k2':'hello'}}
# Grab 'hello'
1/13:
# Getting a little tricker
d = {'k1':[{'nest_key':['this is deep',['hello']]}]}

#Grab hello
1/14:
# This will be hard and annoying!
d = {'k1':[1,2,{'k2':['this is tricky',{'tough':[1,2,['hello']]}]}]}
1/15: list5 = [1,2,2,33,4,4,11,22,3,3,2]
1/16: # Square root:
1/17: # Square:
1/18:
s = 'hello'
# Print out 'e' using indexing
1/19:
s ='hello'
# Reverse the string using slicing
1/20:
s ='hello'
# Print out the 'o'

# Method 1:
1/21: # Method 2:
1/22: # Method 1:
1/23: # Method 2:
1/24: list3 = [1,2,[3,4,'hello']]
1/25: list4 = [5,3,4,6,1]
1/26:
d = {'simple_key':'hello'}
# Grab 'hello'
1/27:
d = {'k1':{'k2':'hello'}}
# Grab 'hello'
1/28:
# Getting a little tricker
d = {'k1':[{'nest_key':['this is deep',['hello']]}]}

#Grab hello
1/29:
# This will be hard and annoying!
d = {'k1':[1,2,{'k2':['this is tricky',{'tough':[1,2,['hello']]}]}]}
1/30: list5 = [1,2,2,33,4,4,11,22,3,3,2]
1/31: # Square root:
1/32: # Square:
1/33:
s = 'hello'
# Print out 'e' using indexing
1/34:
s ='hello'
# Reverse the string using slicing
1/35:
s ='hello'
# Print out the 'o'

# Method 1:
1/36: # Method 2:
1/37: # Method 1:
1/38: # Method 2:
1/39: list3 = [1,2,[3,4,'hello']]
1/40: list4 = [5,3,4,6,1]
1/41:
d = {'simple_key':'hello'}
# Grab 'hello'
1/42:
d = {'k1':{'k2':'hello'}}
# Grab 'hello'
1/43:
# Getting a little tricker
d = {'k1':[{'nest_key':['this is deep',['hello']]}]}

#Grab hello
1/44:
# This will be hard and annoying!
d = {'k1':[1,2,{'k2':['this is tricky',{'tough':[1,2,['hello']]}]}]}
1/45: list5 = [1,2,2,33,4,4,11,22,3,3,2]
1/46: # Square root:
1/47: # Square:
1/48:
s = 'hello'
# Print out 'e' using indexing
1/49:
s ='hello'
# Reverse the string using slicing
1/50:
s ='hello'
# Print out the 'o'

# Method 1:
1/51: # Method 2:
1/52: # Method 1:
1/53: # Method 2:
1/54: list3 = [1,2,[3,4,'hello']]
1/55: list4 = [5,3,4,6,1]
1/56:
d = {'simple_key':'hello'}
# Grab 'hello'
1/57:
d = {'k1':{'k2':'hello'}}
# Grab 'hello'
1/58:
# Getting a little tricker
d = {'k1':[{'nest_key':['this is deep',['hello']]}]}

#Grab hello
1/59:
# This will be hard and annoying!
d = {'k1':[1,2,{'k2':['this is tricky',{'tough':[1,2,['hello']]}]}]}
1/60: list5 = [1,2,2,33,4,4,11,22,3,3,2]
1/61:
# Answer before running cell
2 > 3
1/62:
# Answer before running cell
2 > 3
1/63:
# Answer before running cell
3 <= 2
1/64:
# Answer before running cell
2 > 3
False
1/65:
# Answer before running cell
2 > 3
False
1/66:
# Answer before running cell
3 <= 2
False
1/67:
# Answer before running cell
3 == 2.0
False
1/68:
# Answer before running cell
3.0 == 3
False
1/69:
# Answer before running cell
4**0.5 != 2
True
 2/1: equation = 1.25 + (5*2*2*8/2) - 1
 2/2: equation = 1.25 + (5*2*2*8/2) - 1
 2/3:
equation = 1.25 + (5*2*2*8/2) - 1
print(equation)
 2/4:
equation = 1.25 + (5**2*8/2) - 1
print(equation)
 2/5:
my_equation = 1.25 + (5**2*8/2) - 1
print(my_equation)
 2/6:
expression1 = 4 * (6 + 5)
print(expression1)
expression2 = 4 * 6 + 5
print(expression2)
expression3 = 4+6*5
print(expression3)
 2/7:
# Square:
#Assign value 5 to the variable number
number = 5
#Using pow() function and passing 2 for square
square_number = pow(number,2)
#Print result
print("Square is:", square_number)
 2/8:
# Square root:
#Assign value 25 to the variable number
number = 25
#Using math.sqrt() for getting square root of the given number
square_root = Math.sqrt(number)
print("Square root is:", square_root)
 2/9:
# Square root:
#Import math library
import math
#Assign value 25 to the variable number
number = 25
#Using math.sqrt() for getting square root of the given number
square_root = math.sqrt(number)
#Display Result
print("Square root is:", square_root)
2/10:
# Square root:
#Import math library
import math
#Assign value 25 to the variable number
number = 25
#Using math.sqrt() for getting square root of the given number
square_root = math.sqrt(number)
#Display Result
print("Square root is:", square_root)
2/11:
# Square:
#Assign value 5 to the variable number
number = 5
#Using pow() function and passing 2 for square
square_number = pow(number,2)
#Print result
print("Square is:", square_number)
2/12:
s = 'hello'
# Print out 'e' using indexing
print(s[1])
2/13:
s = 'hello'
# Print out 'e' using indexing
print(s[1])
2/14:
s ='hello'
# Reverse the string using slicing
#Slice the string from start to end with step 1 and reverse
reverse_of_s = s[::-1]
#Display Output
print(reverse_of_s)
2/15:
s = 'hello'
# Print out 'e' using indexing
index_of_e = 1
char_at_index = s[index_of_e]
print(char_at_index)
2/16:
s ='hello'
# Print out the 'o'
#Letter 'o' can be produced from given string with two methods of indexing i.e positive indexing and negative indexing
# Method 1(Positive Indexing):
#As index of letter of 'o' is 4 in positive indexing
letter_o = s[4]
#Display Outpur
print(letter_o)
2/17:
# Method 2(Negative Indexing):
#As index 'o' is the last character in the string so its indexing is [-1] in negative indexing
letter_o = s[-1]
#Display Output
print(letter_o)
2/18:
# Method 1:
#Repetation of elements
#Create a list with three reperation of the element [0]
my_list = [0] * 3
print(my_list)
2/19:
# Method 2:
#Using for loop
#Create an empty list
my_list = []
#Using for loop for range upto 3
for i in range(3):
    my_list.append(0)
#Display Output
print(my_list)
2/20:
# Method 2:
#Using for loop
#Create an empty list
my_list = []
#Using for loop for range upto 3
for i in range(1):
    my_list.append(0)
#Display Output
print(my_list)
2/21:
# Method 2:
#Using for loop
#Create an empty list
my_list = []
#Using for loop for range upto 3
for i in range(3):
    my_list.append(0)
#Display Output
print(my_list)
2/22:
# Method 1:
#Repetation of elements
#Create a list with three reperation of the element [0]
my_list = [0] * 3
#Display Output
print(my_list)
2/23:
list3 = [1,2,[3,4,'hello']]
#The indexing of 'hello' in the nested list is [2][2].
#Accessing and modifying the element.
list3[2][2] = 'goodbye'
#Display Output
print(list3)
2/24:
list4 = [5,3,4,6,1]

#Sort the list in ascending order
list4.sort()
#Display Output
print(list4)
#Sort the list in descending order
list4.sort(reverse=True)
#Display Output
print(list4)
2/25:
d = {'simple_key':'hello'}
# Grab 'hello'
#Access the value using 'simple_key'
value = d['simple_key']
#Display Output
print(value)
2/26:
d = {'k1':{'k2':'hello'}}
# Grab 'hello'
#Access the value using nested key['k1']['k2']
value = d['k1']['k2']
#Display Output
print(value)
2/27:
# Getting a little tricker
d = {'k1':[{'nest_key':['this is deep',['hello']]}]}

#Grab hello
#Access the value through multiple level of dictionaries and list
value = d['k1'][0]['nest_key'][1][0]
#Display Output
print(value)
2/28:
# This will be hard and annoying!
d = {'k1':[1,2,{'k2':['this is tricky',{'tough':[1,2,['hello']]}]}]}
#Access the value through multiple level of dictionaries and list
value = d['k1'][2]['k2'][1]['tough'][2][0]
#Display Output
print(value)
2/29:
list5 = [1,2,2,33,4,4,11,22,3,3,2]

#use set() to remove duplicate items
unique_values = set(list5)
#Display Output
print(unique_values)
2/30:
# Answer before running cell
2 > 3
#Answer - False
2/31:
# Answer before running cell
3 <= 2
#Answer - False
2/32:
# Answer before running cell
3 == 2.0
#Answer - False
2/33:
# Answer before running cell
3.0 == 3
#Answer - True
2/34:
# Answer before running cell
4**0.5 != 2
#Answer - False
2/35:
# two nested lists
l_one = [1,2,[3,4]]
l_two = [1,2,{'k1':4}]

# True or False?
l_one[2][0] >= l_two[2]['k1']

#Answer - False
 3/1:
#Code here
#Lowercase the sentence and split words in the given sentence
words = st.lower().split()
#Get the word starting with 's'
s_words = [word for word in words if word[0] == 's']
#Combine word starting with s
words_with_s = " ".join(s_words)
#Display Output
print(words_with_s)
 3/2: st = 'Print only the words that start with s in this sentence'
 3/3:
#Code here
#Lowercase the sentence and split words in the given sentence
words = st.lower().split()
#Get the word starting with 's'
s_words = [word for word in words if word[0] == 's']
#Combine word starting with s
words_with_s = " ".join(s_words)
#Display Output
print(words_with_s)
 3/4:
#Code Here
#Use for loop starting from 0 upto 11(Excluding 11) with increment of 2
for i in range(0,11,2):
    #Display Output
    print(i)
 3/5:
#Code Here
#Use for loop starting from 0 upto 11(Excluding 11) with increment of 2
for i in range(0,11,2):
    #Display Numbers
    print(i)
 3/6:
#Code in this cell
#Create empty list for numbers divisible by 3
number_divisible_3 = []
#Iterate from 1 to 50 with for loop
for i in range(1,51):
    #Check if number is divisible by 3
    if(i%3 == 0):
        number_divisible_3.append(i)
#Display Output
print(number_divisible_3)
 3/7:
#Code in this cell
#Split and lowercase the given sentance
words = st.lower().split()
#Add only words with even number of letters
even_length_words = [word for word in words if len(words) % 2 == 0]
#Combine words with even number of letters
even_words = " ".join(even_length_words)
#Display Output
print(even_words)
 3/8:
#Code in this cell
#Split and lowercase the given sentance
words = st.lower().split()
#Add only words with even number of letters
even_length_words = [word for word in words if len(words) % 2 == 0]
#Combine words with even number of letters
even_words = " ".join(even_length_words)
#Display Output
print(even_words)
 3/9: st = 'Print every word in this sentence that has an even number of letters'
3/10:
#Code in this cell
#Split and lowercase the given sentance
words = st.lower().split()
#Add only words with even number of letters
even_length_words = [word for word in words if len(words) % 2 == 0]
#Combine words with even number of letters
even_words = " ".join(even_length_words)
#Display Output
print(even_words)
3/11:
#Code in this cell
#Split and lowercase the given sentance
words = st.lower().split()
#Add only words with even number of letters
even_length_words = [word for word in words if len(words) % 2 == 0]
#Combine words with even number of letters
even_words = " ".join(even_length_words)
#Display Output
print(even_words)
3/12:
#Code in this cell
#Split and lowercase the given sentance
words = st.lower().split()
#Add only words with even number of letters
even_length_words = [word for word in words if len(words) % 2 == 0]
#Combine words with even number of letters
even_words = " ".join(even_length_words)
#Display Output
print(even_words)
3/13: st = 'Print every word in this sentence that has an even number of letters'
3/14:
#Code in this cell
#Split and lowercase the given sentance
words = st.lower().split()
#Add only words with even number of letters
even_length_words = [word for word in words if len(words) % 2 == 0]
#Combine words with even number of letters
even_words = " ".join(even_length_words)
#Display Output
print(even_words)
3/15:
#Code in this cell
#Split and lowercase the given sentance
words = st.lower().split()
#Add only words with even number of letters
even_length_words = [word for word in words if len(words) % 2 == 0]
#Combine words with even number of letters
even_words = " ".join(even_length_words)
#Display Output
print(even_words)
3/16:
#Code in this cell
#Split and lowercase the given sentance
words = st.lower().split()
#Add only words with even number of letters
even_length_words = [word for word in words if len(word) % 2 == 0]
#Combine words with even number of letters
even_words = " ".join(even_length_words)
#Display Output
print(even_words)
3/17:
#Code in this cell
#Split and lowercase the given sentance
words = st.lower().split()
#Add only words with even number of letters
even_length_words = [word for word in words if len(word) % 2 == 0]
#Combine words with even number of letters
even_words = " ".join(even_length_words)
#Display Output
print(even_words)
3/18:
#Code in this cell
#Use for loop starting from 0 to 100
for i in range(0,101):
    #Check if divisible by 3 & 5
    if(i % 3 == 0 and i % 5 == 0):
        print('Multiple of 3 and 5')
    #Check if divisible by 3
    elif(i % 3 == 0):
        print('Multiple of 3')
    #Check if divisible by 5
    elif(i % 5 == 0):
        print('Multiple of 5')
    #Print the number
    else:
        print(i)
3/19: #Code in this cell
3/20: st = 'Create a list of the first letters of every word in this string'
3/21:
#Code in this cell
#Split the given sentance
words = st.split()
#Get first letter from the
first_letter_word = [word[0] for word in words]
#Display Output
print(first_letter_word)
 4/1:
#Code here
#Calculate and multiply the number of seconds in a minute by number of minutes in an hour
seconds_in_hour = 60 * 60
#Assign the result from seconds_in_hour to seconds_per_hour
seconds_per_hour = seconds_in_hour
#Calculate seconds in a day using seconds_per_hour
seconds_in_day = seconds_per_hour*24
#Calculate seconds in a day using seconds_per_hour and storing in seconds_per_day
seconds_per_day = seconds_per_hour*24
#Divide seconds_per_day by seconds_per_hour using floating-point division
floating_point_division = seconds_per_day/seconds_per_hour
#Divide seconds_per_day by seconds_per_hour using integer division
integer_division = seconds_per_day // seconds_per_hour

#Display Output
print("Seconds in an hour:", seconds_per_hour)
print("Seconds in a day:",seconds_per_day)
print("Floating point division",floating_point_division)
print("Integer Division:",integer_division)
 4/2:
#Code here
#Calculate and multiply the number of seconds in a minute by number of minutes in an hour
seconds_in_hour = 60 * 60
#Assign the result from seconds_in_hour to seconds_per_hour
seconds_per_hour = seconds_in_hour
#Calculate seconds in a day using seconds_per_hour
seconds_in_day = seconds_per_hour*24
#Calculate seconds in a day using seconds_per_hour and storing in seconds_per_day
seconds_per_day = seconds_per_hour*24
#Divide seconds_per_day by seconds_per_hour using floating-point division
floating_point_division = seconds_per_day/seconds_per_hour
#Divide seconds_per_day by seconds_per_hour using integer division
integer_division = seconds_per_day // seconds_per_hour

#Display Output
print("Seconds in an hour:", seconds_per_hour)
print("Seconds in a day:",seconds_per_day)
print("Floating point division",floating_point_division)
print("Integer Division:",integer_division)
if(integer_division == int(floating_point_division)):
    print("The floating-point and integer division numbers agree excluding decimal.")
else:
    print("The floating-point and integer division numbers differ.")
 4/3:
#Code here
#Calculate and multiply the number of seconds in a minute by number of minutes in an hour
seconds_in_hour = 60 * 60
#Assign the result from seconds_in_hour to seconds_per_hour
seconds_per_hour = seconds_in_hour
#Calculate seconds in a day using seconds_per_hour
seconds_in_day = seconds_per_hour*24
#Calculate seconds in a day using seconds_per_hour and storing in seconds_per_day
seconds_per_day = seconds_per_hour*24
#Divide seconds_per_day by seconds_per_hour using floating-point division
floating_point_division = seconds_per_day/seconds_per_hour
#Divide seconds_per_day by seconds_per_hour using integer division
integer_division = seconds_per_day // seconds_per_hour

#Display Output
print("Seconds in an hour:", seconds_per_hour)
print("Seconds in a day:",seconds_per_day)
print("Floating point division",floating_point_division)
print("Integer Division:",integer_division)
if(integer_division == int(floating_point_division)):
    print("The floating-point and integer division numbers agree  aside from final .0 . ")
else:
    print("The floating-point and integer division numbers differ.")
 4/4:
#Code here
#Calculate and multiply the number of seconds in a minute by number of minutes in an hour
seconds_in_hour = 60 * 60
#Assign the result from seconds_in_hour to seconds_per_hour
seconds_per_hour = seconds_in_hour
#Calculate seconds in a day using seconds_per_hour
seconds_in_day = seconds_per_hour*24
#Calculate seconds in a day using seconds_per_hour and storing in seconds_per_day
seconds_per_day = seconds_per_hour*24
#Divide seconds_per_day by seconds_per_hour using floating-point division
floating_point_division = seconds_per_day/seconds_per_hour
#Divide seconds_per_day by seconds_per_hour using integer division
integer_division = seconds_per_day // seconds_per_hour

#Display Output
print("Seconds in an hour:", seconds_per_hour)
print("Seconds in a day:",seconds_per_day)
print("Floating point division",floating_point_division)
print("Integer Division:",integer_division)
if(integer_division == int(floating_point_division)):
    print("The floating-point and integer division numbers agree  aside from final .0.")
else:
    print("The floating-point and integer division numbers differ.")
 4/5:
#Code here
#Assign the value of 7 to the 'secret_number'
secret_number = 7
 4/6:
#Code here
#Assign the value of 7 to the 'secret_number'
secret_number = 7 
#Assign the value of 1 to the variable guess
guess = 1
#Check the guess using (if,else and elif)
if(guess < secret_number):
    print('too low')
elif(guess > secret_number):
    print('too high')
else:
    print('found it')
 4/7:
#Code here
#Input secret number from user
secret_number = input('Enter the secret number between 1 and 10')
#Input guess number from the user
guess = input("Enter your guess number between 1 and 10")
#Continue guessing until the correct number is found or guess is too high
while True:
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('found it')
        break
    #Guess again if input is too low
    guess = input("Guess again:")
 4/8:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Continue guessing until the correct number is found or guess is too high
while True:
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('found it')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
 4/9:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Continue guessing until the correct number is found or guess is too high
while True:
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('found it')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
4/10:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/11:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/12:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/13:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/14:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/15:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/16:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/17:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/18:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/19:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/20:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/21:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/22:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/23:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/24:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 2
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/25:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 2
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/26:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 2
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/27:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/28:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/29:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/30:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/31:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/32:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/33:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/34:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/35:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/36:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/37:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/38:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/39:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/40:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    attempts += 1
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/41:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    attempts += 1
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/42:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    attempts += 1
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/43:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/44:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/45:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/46:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/47:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/48:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/49:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
        print(attempts)
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        print(attempts)
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1

#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/50:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 0
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1
    print(attempts)
#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/51:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1
    print(attempts)
#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/52:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1
    print(attempts)
#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/53:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1
    print(attempts)
#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/54:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1
    print(attempts)
#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/55:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1
    print(attempts)
#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/56:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1
    print(attempts)
#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/57:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1
    print(attempts)
#Check if limit has reached without finding the secret number
if (attempts > guess_limit):
    print("You lose!")
4/58:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts <= guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1
    print(attempts)
#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/59:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1
    print(attempts)
#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/60:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1
    print(attempts)
#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/61:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Input guess number from the user
guess = int(input("Enter your guess number between 1 and 10"))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1
#Continue guessing until the correct number is found or guess is too high
while attempts < guess_limit:
    #Check the guess using (if,else and elif)
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
        break
    else:
        print('You Won!')
        break
    #Guess again if input is too low
    guess = int(input("Guess again:"))
    attempts += 1
    print(attempts)
#Check if limit has reached without finding the secret number
if (attempts >= guess_limit):
    print("You lose!")
4/62:
#Code here
birth_year = 1996
#create a list of years till fifth birthday
years_list = range(birth_year, birth_year+5)
print(years_list)
4/63:
#Code here
birth_year = 1996
#create a list of years till fifth birthday
years_list = range(birth_year, birth_year+5)
#Year of third birthday
for year in years_list:
    if year - birth_year == 2:
        third_birthday_year = year
        break
#Year you were the oldest using negative indexing
oldest_year = years_list[-1]
#Display Output
print("Year for third birthday:", third_birthday_year)
print("Oldest Year:", oldest_year)
4/64:
#Code here
birth_year = 1996
#create a list of years till fifth birthday
years_list = range(birth_year, birth_year+5)
#Year of third birthday
for year in years_list:
    if year - birth_year == 3:
        third_birthday_year = year
        break
#Year you were the oldest using negative indexing
oldest_year = years_list[-1]
#Display Output
print("Year for third birthday:", third_birthday_year)
print("Oldest Year:", oldest_year)
4/65:
#Code here
#Create an English-to-French dictionary called e2f
e2f = {
    'dog':'chien',
    'cat':'chat',
    'walrus':'morse'
}
#Print the English-to-French dictionary
print(e2f)
4/66:
#Code here
#Create an English-to-French dictionary called e2f
e2f = {
    'dog':'chien',
    'cat':'chat',
    'walrus':'morse'
}
#Print the English-to-French dictionary
print(e2f)
#Get french word for walrus
french_walrus = e2f['walrus']
#Print french word for walrus
print(french_walrus)
4/67:
#Code here
#Create an English-to-French dictionary called e2f
e2f = {
    'dog':'chien',
    'cat':'chat',
    'walrus':'morse'
}
#Print the English-to-French dictionary
print(e2f)
#Get French word for walrus
french_walrus = e2f['walrus']
#Print French word for walrus
print(french_walrus)
#Create French-to-English dictionary from e2f
f2e = {french:english for english, french in e2f.items()}
#Get the English word for word chien
english_chien = f2e['chien']
#Print English word for word chien
print(english_chien)
4/68:
#Code here
#Create an English-to-French dictionary called e2f
e2f = {
    'dog':'chien',
    'cat':'chat',
    'walrus':'morse'
}
#Print the English-to-French dictionary
print(e2f)
#Get French word for walrus
french_walrus = e2f['walrus']
#Print French word for walrus
print(french_walrus)
#Create French-to-English dictionary from e2f
f2e = {french:english for english, french in e2f.items()}
#Get the English word for word chien
english_chien = f2e['chien']
#Print English word for word chien
print(english_chien)
#Get set of english word from e2f
english_words = set(e2f.keys())
#Print the set of english words
print(english_words)
 5/1:
# Write your function
def hello_world():
    print('Hello world')
 5/2:
# Call your function
hello_world()
 5/3:
# Write your function
def myfunc(name):
    print('Hello ', name)
 5/4:
# Call your function
myfunc('Prabesh')
 5/5:
# Write your function
def myfunc(name):
    print('Hello ',name)
 5/6:
# Call your function
myfunc('Prabesh')
 5/7:
# Write your function
def myfunc(name):
    print('Hello',name)
 5/8:
# Call your function
myfunc('Prabesh')
 5/9:
# Write your function
def myfunc(is_true):
    if is_true:
        return 'Hello'
    else:
        return 'Goodbye'
5/10:
# Call your function
myfunc(True)
5/11:
# Write your function
def myfunc(x,y,z):
    if z:
        return x
    else:
        return y
5/12:
# Call your function
myfunc(10,20,True)
5/13:
# Write your function
def myfunc(x,y):
    sum = x+y
    return sum
5/14:
# Call your function
myfunc(10,30)
5/15:
# Write your function
def is_even(x):
    if(x % 2 == 0):
        return True
    else:
        return False
5/16:
# Call your function
is_even(4)
5/17:
# Call your function
is_even(5)
5/18:
# Call your function
is_greater(10,20)
5/19:
# Write your function
def is_greater(x,y):
    if(x>y):
        return True
    else:
        return False
5/20:
# Call your function
is_greater(10,20)
5/21:
# Write your function
def myfunc(*args):
    return sum(args)
5/22:
# Call your function
myfunc(1,2,3)
5/23:
# Write your function
def myfunc(*args):
    return [num for num in args if args % 2 == 0]
5/24:
# Call your function
myfunc(1,2,3,4,5,6,7,8)
5/25:
# Write your function
def myfunc(*args):
    return [num for num in args if args % 2 == 0]
5/26:
# Call your function
myfunc(1,2,3,4,5,6,7,8)
5/27:
# Write your function
def myfunc(*args):
    return [num for num in args if args % 2 == 0]
5/28:
# Call your function
myfunc(1,2,3,4,5,6,7,8)
5/29:
# Write your function
def myfunc(*args):
    return [num for num in args if num % 2 == 0]
5/30:
# Call your function
myfunc(1,2,3,4,5,6,7,8)
5/31:
# Write your function
def myfunc(string):
    return "".join([char.upper() if i % 2 == 0 else char.lower() for i, char in enumerate(string)])
5/32:
# Call your function
myfunc('hello')
5/33:
# Write your function
def unique_element(data):
    #Use set to remove duplicate items and list() to create list from set
    return list(set(data))
5/34:
# Call your function
unique_element([1,1,1,1,2,2,3,3,3,3,4,5])
5/35:
# Write your function
def list_multiplication(data):
    #Set initial multiplication value 1
    value = 1
    #Use for loop
    for num in data:
        value *= num
    return value
5/36:
# Call your function
list_multiplication([1,2,3,-4])
5/37:
# Write your function
def hello_world():
    print('Hello world')
5/38:
# Call your function
hello_world()
5/39:
# Write your function
def myfunc(name):
    print('Hello',name)
5/40:
# Call your function
myfunc('Prabesh')
5/41:
# Write your function
def myfunc(is_true):
    if is_true:
        return 'Hello'
    else:
        return 'Goodbye'
5/42:
# Call your function
myfunc(True)
4/69:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1

#Continue guessing until the correct number is found or guess limit is reached
while attempts <= guess_limit:
    guess = int(input("Enter the guess number between 1 and 10"))
    attempts += 1
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
    else:
        print('You won')
        break
    #Check if limit was reached without finding secret number
if(attempts>guess_limit):
        print("You loose")
4/70:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1

#Continue guessing until the correct number is found or guess limit is reached
while attempts <= guess_limit:
    guess = int(input("Enter the guess number between 1 and 10"))
    attempts += 1
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
    else:
        print('You won')
        break
    #Check if limit was reached without finding secret number
    if(attempts>guess_limit):
        print("You loose")
        break
4/71:
#Code here
#Input secret number from user
secret_number = int(input('Enter the secret number between 1 and 10'))
#Set maximum number of guesses
guess_limit = 3
#Initialize number of attempts
attempts = 1

#Continue guessing until the correct number is found or guess limit is reached
while attempts <= guess_limit:
    guess = int(input("Enter the guess number between 1 and 10"))
    attempts += 1
    if guess < secret_number:
        print('too low')
    elif guess > secret_number:
        print('oops')
    else:
        print('You won')
        break
    #Check if limit was reached without finding secret number
    if(attempts>guess_limit):
        print("You loose")
        break
4/72:
#Code here
birth_year = 1996
#create a list of years till fifth birthday
years_list = range(birth_year, birth_year+5)
#Year of third birthday
for year in years_list:
    if year - birth_year == 3:
        third_birthday_year = year
        break
#Year you were the oldest using negative indexing as last year is in the list is oldest year
oldest_year = years_list[-1]
#Display Output
print("Year for third birthday:", third_birthday_year)
print("Oldest Year:", oldest_year)
4/73:
#Code here
birth_year = 1996
#create a list of years till fifth birthday
years_list = range(birth_year, birth_year+5)
#Year of third birthday assuming i was 0 years of age in first year
for year in years_list:
    if year - birth_year == 3:
        third_birthday_year = year
        break
#Year you were the oldest using negative indexing as last year is in the list is oldest year
oldest_year = years_list[-1]
#Display Output
print("Year for third birthday:", third_birthday_year)
print("Oldest Year:", oldest_year)
4/74:
#Code here
birth_year = 1996
#create a list of years till fifth birthday
years_list = range(birth_year, birth_year+5)
#Year of third birthday assuming i was 0 years of age in first year
for year in years_list:
    if year - birth_year == 3:
        third_birthday_year = year
        break
#Year you were the oldest using negative indexing as last year is in the list is oldest year
oldest_year = years_list[-1]
#Display Output
print("Year for third birthday:", third_birthday_year)
print("Oldest Year:", oldest_year)
 7/1:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import stemgraphic as stem
# Given data
data = [20, 29, 52, 34, 45, 30, 48, 75, 54, 18, 25, 59, 112, 52, 52, 42, 90, 42, 48, 48, 28, 18, 55, 22]

# Calculate the five-number summary
minimum_val = np.min(data)
q1 = np.percentile(data, 25)
median = np.median(data)
q3 = np.percentile(data, 75)
maximum_val = np.max(data)

print("Five-Number Summary:")
print(f"Minimum: {minimum_val}")
print(f"Q1 (First Quartile): {q1}")
print(f"Median (Second Quartile): {median}")
print(f"Q3 (Third Quartile): {q3}")
print(f"Maximum: {maximum_val}")

# Create a stem-and-leaf plot
fig, ax = plt.subplots()
ax.stem(data, use_line_collection=True)
ax.set_title("Stem-and-Leaf Plot")
ax.set_xlabel("Stems")
ax.set_ylabel("Leaves")
# plt.show()




stem.stem_graphic(data, scale = 10)
 7/2:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import stemgraphic as stem
# Given data
data = [20, 29, 52, 34, 45, 30, 48, 75, 54, 18, 25, 59, 112, 52, 52, 42, 90, 42, 48, 48, 28, 18, 55, 22]

# Calculate the five-number summary
minimum_val = np.min(data)
q1 = np.percentile(data, 25)
median = np.median(data)
q3 = np.percentile(data, 75)
maximum_val = np.max(data)

print("Five-Number Summary:")
print(f"Minimum: {minimum_val}")
print(f"Q1 (First Quartile): {q1}")
print(f"Median (Second Quartile): {median}")
print(f"Q3 (Third Quartile): {q3}")
print(f"Maximum: {maximum_val}")

# Create a stem-and-leaf plot
fig, ax = plt.subplots()
ax.stem(data, use_line_collection=True)
ax.set_title("Stem-and-Leaf Plot")
ax.set_xlabel("Stems")
ax.set_ylabel("Leaves")
# plt.show()




stem.stem_graphic(data, scale = 10)
 7/3:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import stemgraphic as stem
# Given data
data = [20, 29, 52, 34, 45, 30, 48, 75, 54, 18, 25, 59, 112, 52, 52, 42, 90, 42, 48, 48, 28, 18, 55, 22]

# Calculate the five-number summary
minimum_val = np.min(data)
q1 = np.percentile(data, 25)
median = np.median(data)
q3 = np.percentile(data, 75)
maximum_val = np.max(data)

print("Five-Number Summary:")
print(f"Minimum: {minimum_val}")
print(f"Q1 (First Quartile): {q1}")
print(f"Median (Second Quartile): {median}")
print(f"Q3 (Third Quartile): {q3}")
print(f"Maximum: {maximum_val}")

# Create a stem-and-leaf plot
fig, ax = plt.subplots()
ax.stem(data, use_line_collection=True)
ax.set_title("Stem-and-Leaf Plot")
ax.set_xlabel("Stems")
ax.set_ylabel("Leaves")
plt.show()




stem.stem_graphic(data, scale = 10)
 7/4:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import stemgraphic as stem
# Given data
data = [20, 29, 52, 34, 45, 30, 48, 75, 54, 18, 25, 59, 112, 52, 52, 42, 90, 42, 48, 48, 28, 18, 55, 22]

# Calculate the five-number summary
minimum_val = np.min(data)
q1 = np.percentile(data, 25)
median = np.median(data)
q3 = np.percentile(data, 75)
maximum_val = np.max(data)

print("Five-Number Summary:")
print(f"Minimum: {minimum_val}")
print(f"Q1 (First Quartile): {q1}")
print(f"Median (Second Quartile): {median}")
print(f"Q3 (Third Quartile): {q3}")
print(f"Maximum: {maximum_val}")

# Create a stem-and-leaf plot
fig, ax = plt.subplots()
ax.stem(data, use_line_collection=True)
ax.set_title("Stem-and-Leaf Plot")
ax.set_xlabel("Stems")
ax.set_ylabel("Leaves")
plt.show()




stem.stem_graphic(data, scale = 10)
 7/5:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import stemgraphic as stem
# Given data
data = [20, 29, 52, 34, 45, 30, 48, 75, 54, 18, 25, 59, 112, 52, 52, 42, 90, 42, 48, 48, 28, 18, 55, 22]

# Calculate the five-number summary
minimum_val = np.min(data)
q1 = np.percentile(data, 25)
median = np.median(data)
q3 = np.percentile(data, 75)
maximum_val = np.max(data)

print("Five-Number Summary:")
print(f"Minimum: {minimum_val}")
print(f"Q1 (First Quartile): {q1}")
print(f"Median (Second Quartile): {median}")
print(f"Q3 (Third Quartile): {q3}")
print(f"Maximum: {maximum_val}")

# Create a stem-and-leaf plot
fig, ax = plt.subplots()
ax.stem(data, use_line_collection=True)
ax.set_title("Stem-and-Leaf Plot")
ax.set_xlabel("Stems")
ax.set_ylabel("Leaves")
plt.show(block=False)




stem.stem_graphic(data, scale = 10)
 8/1:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import stemgraphic as stem
# Given data
data = [17, 19, 19, 21, 21, 22, 23, 23, 24, 24, 24, 25, 28, 29, 29, 30, 30, 31, 31, 32, 32, 32, 34, 35, 36, 39, 39, 40, 40, 41, 42, 43, 43, 43, 43, 45, 47, 47, 48, 48, 50, 52, 54, 56, 56, 60, 60, 65, 68, 72]

# Calculate the five-number summary
minimum_val = np.min(data)
q1 = np.percentile(data, 25)
median = np.median(data)
q3 = np.percentile(data, 75)
maximum_val = np.max(data)

print("Five-Number Summary:")
print(f"Minimum: {minimum_val}")
print(f"Q1 (First Quartile): {q1}")
print(f"Median (Second Quartile): {median}")
print(f"Q3 (Third Quartile): {q3}")
print(f"Maximum: {maximum_val}")

# Create a stem-and-leaf plot
fig, ax = plt.subplots()
ax.stem(data, use_line_collection=True)
ax.set_title("Stem-and-Leaf Plot")
ax.set_xlabel("Stems")
ax.set_ylabel("Leaves")
plt.show(block=False)




stem.stem_graphic(data, scale = 10)
 8/2:
import matplotlib.pyplot as plt
import numpy as np

# Given data
data = [29, 40, 25, 31, 60, 68, 39, 42, 60, 43, 28, 52, 30, 32, 48, 17, 40, 21, 32, 30,
        21, 72, 22, 29, 19, 43, 43, 36, 50, 32, 43, 41, 34, 45, 47, 23, 48, 24, 31, 35,
        24, 23, 19, 24, 54, 56, 56, 65, 47, 39]

# Calculate quartiles
q1 = np.percentile(data, 25)
q3 = np.percentile(data, 75)

# Create a box and whisker plot
plt.figure(figsize=(8, 6))
plt.boxplot(data, vert=False, patch_artist=True)
plt.title("Box and Whisker Plot")
plt.xlabel("Waiting Time (minutes)")
plt.yticks([])
plt.grid(axis="x")
plt.show()

# Create a stem and leaf plot
def stem_and_leaf(data):
    stems = {}
    for value in data:
        stem, leaf = divmod(value, 10)
        if stem not in stems:
            stems[stem] = []
        stems[stem].append(leaf)
    
    for stem, leaves in sorted(stems.items()):
        print(f"{stem} | {' '.join(map(str, sorted(leaves)))}")

print("\nStem and Leaf Display:")
stem_and_leaf(data)

# Print quartiles
print(f"\n1st Quartile (Q1): {q1:.2f}")
print(f"3rd Quartile (Q3): {q3:.2f}")
 8/3:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import stemgraphic as stem
# Given data
data = [17, 19, 19, 21, 21, 22, 23, 23, 24, 24, 24, 25, 28, 29, 29, 30, 30, 31, 31, 32, 32, 32, 34, 35, 36, 39, 39, 40, 40, 41, 42, 43, 43, 43, 43, 45, 47, 47, 48, 48, 50, 52, 54, 56, 56, 60, 60, 65, 68, 72]

# Calculate the five-number summary
minimum_val = np.min(data)
q1 = np.percentile(data, 25)
median = np.median(data)
q3 = np.percentile(data, 75)
maximum_val = np.max(data)

print("Five-Number Summary:")
print(f"Minimum: {minimum_val}")
print(f"Q1 (First Quartile): {q1}")
print(f"Median (Second Quartile): {median}")
print(f"Q3 (Third Quartile): {q3}")
print(f"Maximum: {maximum_val}")

# Create a stem-and-leaf plot
fig, ax = plt.subplots()
ax.stem(data, use_line_collection=True)
ax.set_title("Stem-and-Leaf Plot")
ax.set_xlabel("Stems")
ax.set_ylabel("Leaves")
plt.show(block=False)




stem.stem_graphic(data, scale = 10)
 8/4:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import stemgraphic as stem
# Given data
data = [17, 19, 19, 21, 21, 22, 23, 23, 24, 24, 24, 25, 28, 29, 29, 30, 30, 31, 31, 32, 32, 32, 34, 35, 36, 39, 39, 40, 40, 41, 42, 43, 43, 43, 43, 45, 47, 47, 48, 48, 50, 52, 54, 56, 56, 60, 60, 65, 68, 72]

# Calculate the five-number summary
minimum_val = np.min(data)
q1 = np.percentile(data, 25)
median = np.median(data)
q3 = np.percentile(data, 75)
maximum_val = np.max(data)

print("Five-Number Summary:")
print(f"Minimum: {minimum_val}")
print(f"Q1 (First Quartile): {q1}")
print(f"Median (Second Quartile): {median}")
print(f"Q3 (Third Quartile): {q3}")
print(f"Maximum: {maximum_val}")

# Create a stem-and-leaf plot
fig, ax = plt.subplots()
ax.stem(data, use_line_collection=True)
ax.set_title("Stem-and-Leaf Plot")
ax.set_xlabel("Stems")
ax.set_ylabel("Leaves")
plt.show(block=False)

# Create a box and whisker plot
plt.figure(figsize=(8, 6))
plt.boxplot(data, vert=False, patch_artist=True)
plt.title("Box and Whisker Plot")
plt.xlabel("Waiting Time (minutes)")
plt.yticks([])
plt.grid(axis="x")
plt.show()



stem.stem_graphic(data, scale = 10)
 9/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
10/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
10/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
10/3:
data_train_path = 'Fruits_Vegetables/train'
data_train_test = 'Fruits_Vegetables/test'
data_train_val = 'Fruits_Vegetables/validation'
10/4:
data_train_path = 'Fruits_Vegetables/train'
data_train_test = 'Fruits_Vegetables/test'
data_train_val = 'Fruits_Vegetables/validation'
10/5:
img_width = 180
img_height = 180
10/6: data_train = tf.keras.utils.image_dataset_from_directory(data_train_path,
10/7:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height)
10/8:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
10/9: data_cat = data_train.class_names
10/10:
data_train_path = 'Fruits_Vegetables/train'
data_test_path = 'Fruits_Vegetables/test'
data_val_path = 'Fruits_Vegetables/validation'
10/11:
data_val = tf.keras.utils.image_dataset_from_directory(
    data_val_path,
    shuffle=False,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
10/12:
data_test = tf.keras.utils.image_dataset_from_directory(
    data_test_path,
    shuffle=False,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
10/13:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].nparray.astype('uint8'))
        plt.title(data_cat[labels[i]])
10/14:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy.astype('uint8'))
        plt.title(data_cat[labels[i]])
10/15:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
10/16:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
10/17: from tf.keras.models import Sequential
10/18: from tensorflow.keras.models import Sequential
10/19: data_train
10/20:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3,padding='same', activation='relu'),
    layers.MaxPool2D(),
    layers.Conv2D(64,3,padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0,2),
    layers.Dense(128),
    layers.Dense(units=len(data_cat))
])
10/21: model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),metrics=['accuracy'])
10/22: model.summary()
10/23: epochs_size = 25
10/24:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val,epochs= epochs_size)
10/25: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])
10/26:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val,epochs= epochs_size)
10/27:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val,epochs=epochs_size)
10/28:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val,epochs=epochs_size)
10/29:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val,epochs=25)
10/30:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val,epochs=25)
10/31: model.summary()
10/32: model.summary()
10/33:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val,epochs=25)
10/34:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
10/35:
data_train_path = 'Fruits_Vegetables/train'
data_test_path = 'Fruits_Vegetables/test'
data_val_path = 'Fruits_Vegetables/validation'
10/36:
img_width = 180
img_height =180
10/37:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
10/38: data_cat = data_train.class_names
10/39: data_cat
10/40:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
10/41:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
10/42:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
10/43: from tensorflow.keras.models import Sequential
10/44: data_train
10/45:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3,padding='same', activation='relu'),
    layers.MaxPool2D(),
    layers.Conv2D(64,3,padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0,2),
    layers.Dense(128),
    layers.Dense(units=len(data_cat))
])
10/46: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])
10/47:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3,padding='same', activation='relu'),
    layers.MaxPool2D(),
    layers.Conv2D(64,3,padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0,2),
    layers.Dense(128),
    layers.Dense(units=len(data_cat))
])
10/48: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])
10/49:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val,epochs=25)
11/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
11/2:
data_train_path = 'Fruits_Vegetables/train'
data_test_path = 'Fruits_Vegetables/test'
data_val_path = 'Fruits_Vegetables/validation'
11/3:
img_width = 180
img_height =180
11/4:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
11/5: data_cat = data_train.class_names
11/6:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
11/7:
data_train_path = 'Fruits_Vegetables/train'
data_test_path = 'Fruits_Vegetables/test'
data_val_path = 'Fruits_Vegetables/validation'
11/8:
img_width = 180
img_height =180
11/9:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
11/10: data_cat = data_train.class_names
11/11: data_cat
11/12:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
11/13:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
11/14:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
11/15: from tensorflow.keras.models import Sequential
11/16: data_train
11/17:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
11/18: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
11/19:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
12/1:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
12/2:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
12/3: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
12/4:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
12/5:
data_train_path = 'Fruits_Vegetables/train'
data_test_path = 'Fruits_Vegetables/test'
data_val_path = 'Fruits_Vegetables/validation'
12/6:
img_width = 180
img_height =180
12/7:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
12/8: data_cat = data_train.class_names
12/9: data_cat
12/10:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
12/11:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
12/12:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
12/13: from tensorflow.keras.models import Sequential
12/14: data_train
12/15:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
12/16: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
12/17:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
12/18:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
12/19:
image = 'corn.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
12/20: predict = model.predict(img_bat)
12/21: score = tf.nn.softmax(predict)
12/22: print('Veg/Fruit in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
12/23: model.save('Image_classify.keras')
12/24: model.save('Image_classify.keras')
12/25:
image = 'abc.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
12/26: predict = model.predict(img_bat)
12/27: score = tf.nn.softmax(predict)
12/28: print('Veg/Fruit in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
13/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
13/3:
input_folder = "RawData/"
output = "Dataset/"
split_folders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Fruits_Vegetables/train'
data_test_path = 'Fruits_Vegetables/test'
data_val_path = 'Fruits_Vegetables/validation'
13/4:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Fruits_Vegetables/train'
data_test_path = 'Fruits_Vegetables/test'
data_val_path = 'Fruits_Vegetables/validation'
13/5:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/validation'
13/6:
img_width = 180
img_height =180
13/7:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
13/8: data_cat = data_train.class_names
13/9: data_cat
13/10:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
13/11:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=90,
                                                        shuffle=False,
                                                       validation_split=False)
13/12:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
13/13:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=90,
                                                        shuffle=False,
                                                       validation_split=False)
13/14:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/validation'
13/15:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=90,
                                                        shuffle=False,
                                                       validation_split=False)
13/16:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=90,
                                                        shuffle=False,
                                                       validation_split=False)
13/17:
data_train_path = 'Fruits_Vegetables/train'
data_test_path = 'Fruits_Vegetables/test'
data_val_path = 'Fruits_Vegetables/validation'
13/18:
img_width = 180
img_height =180
13/19:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
13/20:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/validation'
13/21:
img_width = 180
img_height =180
13/22:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
13/23:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
13/24: data_cat = data_train.class_names
13/25: data_cat = data_train.class_names
13/26: data_cat
13/27:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
13/28:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
13/29:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/30:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
13/31:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
13/32:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
13/33:
img_width = 180
img_height =180
13/34:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
13/35: data_cat = data_train.class_names
13/36: data_cat
13/37:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
13/38:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
13/39:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/40:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(6):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/41:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/42:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=5,
    validation_split=False
)
13/43:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/44:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=5,
    validation_split=False)
13/45: data_cat = data_train.class_names
13/46: data_cat
13/47:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
13/48:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=5,
    validation_split=False
)
13/49:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/50:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(5):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/51:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
13/52: data_cat = data_train.class_names
13/53: data_cat
13/54:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
13/55:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
13/56:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/57: from tensorflow.keras.models import Sequential
13/58: data_train
13/59:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
13/60: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
13/61:
epochs_size = 2
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
13/62:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
13/63:
image = 'a.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/64: predict = model.predict(img_bat)
13/65: score = tf.nn.softmax(predict)
13/66: print('Veg/Fruit in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/67: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/68:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
13/69:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
13/70:
image = 'a.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/71: score = tf.nn.softmax(predict)
13/72: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/73: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/74: model.save('Image_classify.keras')
13/75:
image = 'b.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/76: predict = model.predict(img_bat)
13/77: score = tf.nn.softmax(predict)
13/78: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/79: model.save('Image_classify.keras')
13/80:
image = 'c.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/81: predict = model.predict(img_bat)
13/82: score = tf.nn.softmax(predict)
13/83: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/84: model.save('Image_classify.keras')
13/85:
image = 'r.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/86:
image = 'r.jpeg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/87: predict = model.predict(img_bat)
13/88: score = tf.nn.softmax(predict)
13/89: score = tf.nn.softmax(predict)
13/90: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/91: model.save('Image_classify.keras')
13/92:
image = 't.jpeg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/93:
image = 't.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/94: predict = model.predict(img_bat)
13/95: score = tf.nn.softmax(predict)
13/96: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/97: model.save('Image_classify.keras')
13/98:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
13/99:
img_width = 180
img_height =180
13/100:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
13/101: data_cat = data_train.class_names
13/102: data_cat
13/103:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
13/104:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
13/105:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/106: from tensorflow.keras.models import Sequential
13/107: data_train
13/108:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
13/109: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
13/110:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
13/111:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
13/112:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/113: predict = model.predict(img_bat)
13/114: score = tf.nn.softmax(predict)
13/115: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/116:
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/117: predict = model.predict(img_bat)
13/118: score = tf.nn.softmax(predict)
13/119: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/120:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/121: predict = model.predict(img_bat)
13/122: score = tf.nn.softmax(?predict)
13/123: score = tf.nn.softmax(predict)
13/124: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/125:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/126: predict = model.predict(img_bat)
13/127: score = tf.nn.softmax(predict)
13/128: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/129:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/130: predict = model.predict(img_bat)
13/131: score = tf.nn.softmax(predict)
13/132: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/133:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/134: predict = model.predict(img_bat)
13/135: score = tf.nn.softmax(predict)
13/136: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/137:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/138: predict = model.predict(img_bat)
13/139: score = tf.nn.softmax(predict)
13/140: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/141:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/142: predict = model.predict(img_bat)
13/143: score = tf.nn.softmax(predict)
13/144: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/145:
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/146: predict = model.predict(img_bat)
13/147: score = tf.nn.softmax(predict)
13/148: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/149: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/150: model.save('Image_classify.keras')
13/151: from mlxtend.plotting import plot_confusion_matrix
13/152: from mlxtend.plotting import plot_confusion_matrix
13/153: from sklearn.metrics import confusion_matrix
13/154: from mlxtend.plotting import plot_confusion_matrix
13/155:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
13/156:
mat = confustion_matrix(y_test,y_pred)
plot_confusion_matrix(conf_mat=mat)
13/157:
mat = confustion_matrix(data_test,pred)
plot_confusion_matrix(conf_mat=mat)
13/158:
mat = confusion_matrix(data_test,pred)
plot_confusion_matrix(conf_mat=mat)
13/159:
mat = confusion_matrix(data_test,score)
plot_confusion_matrix(conf_mat=mat)
13/160:
mat = confusion_matrix(data_test,predict)
plot_confusion_matrix(conf_mat=mat)
13/161:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
13/162: data_test
13/163:
predict = model.predict(img_bat)
predict
13/164: predict = model.predict(img_bat)
13/165:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
13/166:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/167:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/168:
test_images = []
test_labels = []
for images, labels in data_test:
    for i in range(len(images)):
        test_images.append(images[i].numpy())
        test_labels.append(labels[i].numpy())

test_images = np.array(test_images)
test_labels = np.array(test_labels)
13/169:
test_images = []
test_labels = []
for images, labels in data_test:
    for i in range(len(images)):
        test_images.append(images[i].numpy())
        test_labels.append(labels[i].numpy())

test_images = np.array(test_images)
test_labels = np.array(test_labels)
13/170:
mat = confusion_matrix(test_images,predict)
plot_confusion_matrix(conf_mat=mat)
13/171:
test_images = []
test_labels = []
for images, labels in data_test:
    for i in range(len(images)):
        test_images.append(images[i].numpy())
        test_labels.append(labels[i].numpy())

test_images = np.array(test_images)
test_labels = np.array(test_labels)
test_labels
13/172:
test_images = []
test_labels = []
for images, labels in data_test:
    for i in range(len(images)):
        test_images.append(images[i].numpy())
        test_labels.append(labels[i].numpy())

test_images = np.array(test_images)
test_labels = np.array(test_labels)
test_labels
test_images
13/173:
mat = confusion_matrix(test_labels,predict)
plot_confusion_matrix(conf_mat=mat)
13/174: predict = model.predict(img_bat)
13/175:
predict = model.predict(img_bat)
predicted_labels = np.argmax(predictions, axis=1)
13/176:
predict = model.predict(img_bat)
predicted_labels = np.argmax(predict, axis=1)
13/177:
test_labels
mat = confusion_matrix(test_labels,predict)
plot_confusion_matrix(conf_mat=mat)
13/178:
test_labels
mat = confusion_matrix(test_labels,predicted_labels)
plot_confusion_matrix(conf_mat=mat)
13/179:
test_labels
confusion_matrix(test_labels,predicted_labels)
plot_confusion_matrix(conf_mat=mat)
13/180: mat = confusion_matrix(data_test,predict)
13/181: predict = model.predict(img_bat)
13/182: mat = confusion_matrix(data_test,predict)
13/183: data_test
13/184:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(6):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/185:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/186:
plt.figure(figsize=(10,10))
for image, labels in data_train.take():
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/187:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/188:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(len(image)):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/189:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(3):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/190:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(3):
        plt.subplot(3,3,i)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/191:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(len(image)):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/192: from tensorflow.keras.models import Sequential
13/193:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/194:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[0].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/195:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.title(data_cat[labels[i]])
        # plt.axis('off')
13/196:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.title(data_cat[labels[i]])
        # plt.axis('off')
13/197:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        # plt.title(data_cat[labels[i]])
        # plt.axis('off')
13/198:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        # plt.subplot(3,3,i+1)
        # plt.title(data_cat[labels[i]])
        # plt.axis('off')
13/199:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        # plt.title(data_cat[labels[i]])
        # plt.axis('off')
13/200:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        print(i)
        # plt.subplot(3,3,i+1)
        # plt.title(data_cat[labels[i]])
        # plt.axis('off')
13/201:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    # for i in range(9):
        # plt.subplot(3,3,i+1)
        # plt.title(data_cat[labels[i]])
        # plt.axis('off')
13/202:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    print(image)
    # for i in range(9):
        # plt.subplot(3,3,i+1)
        # plt.title(data_cat[labels[i]])
        # plt.axis('off')
13/203:
plt.figure(figsize=(10,10))
# for image, labels in data_train.take(1):
    # for i in range(9):
        # plt.subplot(3,3,i+1)
        # plt.title(data_cat[labels[i]])
        # plt.axis('off')
13/204: from tensorflow.keras.models import Sequential
13/205: data_train
13/206:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
13/207:
plt.figure(figsize=(10,10))
# for image, labels in data_train.take(1):
    # for i in range(9):
        # plt.subplot(3,3,i+1)
        # plt.title(data_cat[labels[i]])
        # plt.axis('off')
13/208:
plt.figure(figsize=(10,10))
# for image, labels in data_train.take(1):
    # for i in range(9):
        # plt.subplot(3,3,i+1)
        # plt.title(data_cat[labels[i]])
        # plt.axis('off')
13/209:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/210:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/211:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/212:
plt.figure(figsize=(10,10))
# for image, labels in data_train.take(1):
#     for i in range(9):
#         plt.subplot(3,3,i+1)
#         plt.imshow(image[i].numpy().astype('uint8'))
#         plt.title(data_cat[labels[i]])
#         plt.axis('off')
13/213:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/214:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/215:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
13/216: classification_report(data_train.classes, predict)
13/217: classification_report(data_train.classes, score')
13/218: classification_report(data_train.classes, score)
13/219:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/220: classification_report(data_train.classes, score)
13/221: classification_report(data_train.classes, prediction)
13/222: data_train
13/223: data_train.classes
13/224: img_bat.classes
13/225:
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/226:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/227: score = tf.nn.softmax(predict)
13/228: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/229: model.save('Image_classify.keras')
13/230:
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/231:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/232: score = tf.nn.softmax(predict)
13/233: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/234: model.save('Image_classify.keras')
13/235:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
13/236: img_bat.classes
13/237: print(img_bat)
13/238: print(img_bat).classes
13/239: print(img_bat).classes
13/240: print(img_bat.classes)
13/241: data_train
13/242: data_train.class_names
13/243: data_cat = data_train.class_names
13/244: data_cat
13/245: classification_report(data_test,data_test.class_names)
13/246: classification_report(img_bat,data_test.class_names)
13/247:
# Get true labels
true_labels = []
for _, labels in data_test:
    true_labels.extend(labels.numpy())

# Get predicted labels
predicted_labels = []
for images, _ in data_test:
    preds = model.predict(images)
    preds = np.argmax(preds, axis=1)
    predicted_labels.extend(preds)

# Create confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
13/248:
# Get true labels
true_labels = []
for _, labels in data_test:
    true_labels.extend(labels.numpy())

# Get predicted labels
predicted_labels = []
for images, _ in data_test:
    preds = model.predict(images)
    preds = np.argmax(preds, axis=1)
    predicted_labels.extend(preds)

# Create confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
13/249:
# Get true labels
true_labels = []
for _, labels in data_test:
    true_labels.extend(labels.numpy())

# Get predicted labels
predicted_labels = []
for images, _ in data_test:
    preds = model.predict(images)
    preds = np.argmax(preds, axis=1)
    predicted_labels.extend(preds)
true_labels
predicted_labels
# Create confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
13/250:
# Get true labels
true_labels = []
for _, labels in data_test:
    true_labels.extend(labels.numpy())

# Get predicted labels
predicted_labels = []
for images, _ in data_test:
    preds = model.predict(images)
    preds = np.argmax(preds, axis=1)
    predicted_labels.extend(preds)
true_labels
predicted_labels
# Create confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
13/251: true_labels
13/252: predicted_labels
13/253:
# Get true labels
true_labels = []
for _, labels in data_test:
    true_labels.extend(labels.numpy())
# Create confusion matrix
cm = confusion_matrix(true_labels, prediction)
13/254:
# Get true labels
true_labels = []
for _, labels in data_test:
    true_labels.extend(labels.numpy())
# Create confusion matrix
cm = confusion_matrix(true_labels, predict)
13/255:
# Get true labels
true_labels = []
for _, labels in data_test:
    true_labels.extend(labels.numpy())

# Get predicted labels
predicted_labels = []
for images, _ in data_test:
    preds = model.predict(images)
    preds = np.argmax(preds, axis=1)
    predicted_labels.extend(preds)
true_labels
predicted_labels
# Create confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
13/256:
# Get true labels
true_labels = []
for _, labels in data_test:
    true_labels.extend(labels.numpy())

# Get predicted labels
predicted_labels = []
for images, _ in data_test:
    preds = model.predict(images)
    preds = np.argmax(preds, axis=1)
    predicted_labels.extend(preds)
true_labels
predicted_labels
# Create confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
13/257:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/258:
plt.figure(figsize=(10,10))
for image, labels in data_test.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/259:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/260:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for j in range(9):
        plt.subplot(3,3,j+1)
        plt.imshow(image[j].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/261:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for j in range(9):
        plt.subplot(3,3,j+1)
        plt.imshow(image[j].numpy().astype('uint8'))
        plt.title(data_cat[labels[j]])
        plt.axis('off')
13/262:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/263:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
13/264:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
13/265:
img_width = 180
img_height =180
13/266:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
13/267: data_cat = data_train.class_names
13/268: data_cat
13/269:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
13/270:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
13/271:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/272:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/273:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/274:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/275:
# Get true labels
true_labels = []
for _, labels in data_test:
    true_labels.extend(labels.numpy())

# Get predicted labels
predicted_labels = []
for images, _ in data_test:
    preds = model.predict(images)
    preds = np.argmax(preds, axis=1)
    predicted_labels.extend(preds)
true_labels
predicted_labels
# Create confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
13/276:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # ERROR
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
13/277:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # ERROR
13/278:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
13/279:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
13/280:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # ERROR
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
13/281:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
13/282:
img_width = 180
img_height =180
13/283:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
13/284: data_cat = data_train.class_names
13/285:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
13/286:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
13/287:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/288:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
13/289:
# Get true labels
true_labels = []
for _, labels in data_test:
    true_labels.extend(labels.numpy())

# Get predicted labels
predicted_labels = []
for images, _ in data_test:
    preds = model.predict(images)
    preds = np.argmax(preds, axis=1)
    predicted_labels.extend(preds)
true_labels
predicted_labels
# Create confusion matrix
cm = confusion_matrix(true_labels, data_test.class_names)
13/290:
# Get true labels
true_labels = []
for _, labels in data_test:
    true_labels.extend(labels.numpy())

# Get predicted labels
predicted_labels = []
for images, _ in data_test:
    preds = model.predict(images)
    preds = np.argmax(preds, axis=1)
    predicted_labels.extend(preds)
true_labels
predicted_labels
# Create confusion matrix
cm = confusion_matrix(true_labels, data_test)
13/291:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
13/292:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
13/293:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
13/294:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/295:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/296: score = tf.nn.softmax(predict)
13/297: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/298: model.save('Image_classify.keras')
13/299:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
13/300:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
13/301:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/302:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/303:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/304: score = tf.nn.softmax(predict)
13/305: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/306:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/307:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/308: score = tf.nn.softmax(predict)
13/309: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/310: model.save('Image_classify.keras')
13/311:
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/312:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/313: score = tf.nn.softmax(predict)
13/314: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/315:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/316:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/317: score = tf.nn.softmax(predict)
13/318: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/319:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
13/320:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
13/321:
img_width = 180
img_height =180
13/322:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
13/323: data_cat = data_train.class_names
13/324: data_cat
13/325:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
13/326:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
13/327:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/328: from tensorflow.keras.models import Sequential
13/329: data_train
13/330:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
13/331: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
13/332:
epochs_size = 10
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
13/333:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
13/334:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/335:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/336: score = tf.nn.softmax(predict)
13/337: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/338: model.save('Image_classify.keras')
13/339:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
13/340:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
13/341:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/342:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/343: score = tf.nn.softmax(predict)
13/344: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/345:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/346:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/347: score = tf.nn.softmax(predict)
13/348: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/349:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/350:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/351: score = tf.nn.softmax(predict)
13/352: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/353: model.save('Image_classify.keras')
13/354:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
13/355:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
13/356:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
13/357:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/358:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/359: score = tf.nn.softmax(predict)
13/360: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/361:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/362:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/363: score = tf.nn.softmax(predict)
13/364: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/365:
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/366:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/367: score = tf.nn.softmax(predict)
13/368: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/369:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/370:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/371: score = tf.nn.softmax(predict)
13/372: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/373: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/374:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
13/375:
image = 'test.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/376:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/377: score = tf.nn.softmax(predict)
13/378: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/379: model.save('Image_classify.keras')
13/380:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
13/381:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
13/382:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/383:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/384: score = tf.nn.softmax(predict)
13/385: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/386: model.save('Image_classify.keras')
13/387:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
13/388:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
13/389:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
13/390:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/391:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/392: score = tf.nn.softmax(predict)
13/393: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/394: model.save('Image_classify.keras')
13/395:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
13/396:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
13/397:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
predicted_labels
# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
13/398: predicted_labels
13/399: true_labels
13/400: true_labels
13/401:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
13/402:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
13/403:
img_width = 180
img_height =180
13/404:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
13/405: data_cat = data_train.class_names
13/406: data_cat
13/407:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
13/408:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
13/409:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/410:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
13/411:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
13/412:
img_width = 180
img_height =180
13/413:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
13/414: data_cat = data_train.class_names
13/415: data_cat
13/416:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
13/417:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
13/418:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/419:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
13/420:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
13/421:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
13/422:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
13/423: score = tf.nn.softmax(predict)
13/424: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
13/425: model.save('Image_classify.keras')
13/426:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
13/427:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
14/1:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
14/2:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
14/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
14/4:
input_folder = "RawData/"
output = "Dataset/"
splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
14/5:
img_width = 180
img_height =180
14/6:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
14/7: data_cat = data_train.class_names
14/8: data_cat
14/9:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
14/10:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
14/11:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
14/12: from tensorflow.keras.models import Sequential
14/13: data_train
14/14:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
14/15: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
14/16:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
14/17:
epochs_size = 10
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
14/18:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
14/19:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
14/20:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
14/21: score = tf.nn.softmax(predict)
14/22: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
14/23:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
14/24:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
14/25: score = tf.nn.softmax(predict)
14/26: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/1: ### Importing Libraries
15/2: ## Importing Libraries
15/3: Importing Libraries
15/4: ####Importing Libraries
15/5: #### Importing Libraries
15/6: [//]: # "Your comment in here."
15/7: //Ju
15/8: #Jupyter
15/9: # Jupyter
15/10:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
15/11:
import os
import shutil

# # Creating Train / Val / Test folders (One time use)
root_dir = 'RawData'
posCls = '/DPN+'
negCls = '/DPN-'

os.makedirs(root_dir +'/train' + posCls)
os.makedirs(root_dir +'/train' + negCls)
os.makedirs(root_dir +'/val' + posCls)
os.makedirs(root_dir +'/val' + negCls)
os.makedirs(root_dir +'/test' + posCls)
os.makedirs(root_dir +'/test' + negCls)

# Creating partitions of the data after shuffeling
currentCls = posCls
src = "Data2"+currentCls # Folder to copy images from

allFileNames = os.listdir(src)
np.random.shuffle(allFileNames)
train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                          [int(len(allFileNames)*0.7), int(len(allFileNames)*0.85)])


train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]
val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]
test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]

print('Total images: ', len(allFileNames))
print('Training: ', len(train_FileNames))
print('Validation: ', len(val_FileNames))
print('Testing: ', len(test_FileNames))

# Copy-pasting images
for name in train_FileNames:
    shutil.copy(name, "Data2/train"+currentCls)

for name in val_FileNames:
    shutil.copy(name, "Data2/val"+currentCls)

for name in test_FileNames:
    shutil.copy(name, "Data2/test"+currentCls)
15/12:
import os
import shutil

# # Creating Train / Val / Test folders (One time use)
root_dir = 'Data2'
posCls = '/DPN+'
negCls = '/DPN-'

os.makedirs(root_dir +'/train' + posCls)
os.makedirs(root_dir +'/train' + negCls)
os.makedirs(root_dir +'/val' + posCls)
os.makedirs(root_dir +'/val' + negCls)
os.makedirs(root_dir +'/test' + posCls)
os.makedirs(root_dir +'/test' + negCls)

# Creating partitions of the data after shuffeling
currentCls = posCls
src = "Data2"+currentCls # Folder to copy images from

allFileNames = os.listdir(src)
np.random.shuffle(allFileNames)
train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                          [int(len(allFileNames)*0.7), int(len(allFileNames)*0.85)])


train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]
val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]
test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]

print('Total images: ', len(allFileNames))
print('Training: ', len(train_FileNames))
print('Validation: ', len(val_FileNames))
print('Testing: ', len(test_FileNames))

# Copy-pasting images
for name in train_FileNames:
    shutil.copy(name, "Data2/train"+currentCls)

for name in val_FileNames:
    shutil.copy(name, "Data2/val"+currentCls)

for name in test_FileNames:
    shutil.copy(name, "Data2/test"+currentCls)
15/13:
import os
import shutil

# # Creating Train / Val / Test folders (One time use)
root_dir = 'Data2'
posCls = '/DPN+'
negCls = '/DPN-'

os.makedirs(root_dir +'/train' + posCls)
os.makedirs(root_dir +'/train' + negCls)
os.makedirs(root_dir +'/val' + posCls)
os.makedirs(root_dir +'/val' + negCls)
os.makedirs(root_dir +'/test' + posCls)
os.makedirs(root_dir +'/test' + negCls)

# Creating partitions of the data after shuffeling
currentCls = posCls
src = "RawData"+currentCls # Folder to copy images from

allFileNames = os.listdir(src)
np.random.shuffle(allFileNames)
train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                          [int(len(allFileNames)*0.7), int(len(allFileNames)*0.85)])


train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]
val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]
test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]

print('Total images: ', len(allFileNames))
print('Training: ', len(train_FileNames))
print('Validation: ', len(val_FileNames))
print('Testing: ', len(test_FileNames))

# Copy-pasting images
for name in train_FileNames:
    shutil.copy(name, "Data2/train"+currentCls)

for name in val_FileNames:
    shutil.copy(name, "Data2/val"+currentCls)

for name in test_FileNames:
    shutil.copy(name, "Data2/test"+currentCls)
15/14:
import os
import shutil

# # Creating Train / Val / Test folders (One time use)
root_dir = 'Data2'
posCls = '/DPN+'
negCls = '/DPN-'

os.makedirs(root_dir +'/train' + posCls)
os.makedirs(root_dir +'/train' + negCls)
os.makedirs(root_dir +'/val' + posCls)
os.makedirs(root_dir +'/val' + negCls)
os.makedirs(root_dir +'/test' + posCls)
os.makedirs(root_dir +'/test' + negCls)

# Creating partitions of the data after shuffeling
currentCls = posCls
src = "RawData"+currentCls # Folder to copy images from

allFileNames = os.listdir(src)
np.random.shuffle(allFileNames)
train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                          [int(len(allFileNames)*0.7), int(len(allFileNames)*0.85)])


train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]
val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]
test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]

print('Total images: ', len(allFileNames))
print('Training: ', len(train_FileNames))
print('Validation: ', len(val_FileNames))
print('Testing: ', len(test_FileNames))

# Copy-pasting images
for name in train_FileNames:
    shutil.copy(name, "Data2/train"+currentCls)

for name in val_FileNames:
    shutil.copy(name, "Data2/val"+currentCls)

for name in test_FileNames:
    shutil.copy(name, "Data2/test"+currentCls)
15/15:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
15/16:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    #data_csv = pd.read_csv("DataSet_Final.csv") ##Use if you have classes saved in any .csv file

    root_dir = 'New_folder_to_be_created'
    classes_dir = ['class 1', 'class 2', 'class 3', 'class 4']

    #for name in data_csv['names'].unique()[:10]:
    #    classes_dir.append(name)

    processed_dir = 'Existing_folder_to_take_images_from'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Creating partitions of the data after shuffeling
        print("$$$$$$$ Class Name " + cls + " $$$$$$$")
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Creating Train / Val / Test folders (One time use)
        os.makedirs(root_dir + '/train//' + cls)
        os.makedirs(root_dir + '/val//' + cls)
        os.makedirs(root_dir + '/test//' + cls)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/17:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    #data_csv = pd.read_csv("DataSet_Final.csv") ##Use if you have classes saved in any .csv file

    root_dir = 'New_folder_to_be_created'
    classes_dir = ['class 1', 'class 2', 'class 3', 'class 4']

    #for name in data_csv['names'].unique()[:10]:
    #    classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Creating partitions of the data after shuffeling
        print("$$$$$$$ Class Name " + cls + " $$$$$$$")
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Creating Train / Val / Test folders (One time use)
        os.makedirs(root_dir + '/train//' + cls)
        os.makedirs(root_dir + '/val//' + cls)
        os.makedirs(root_dir + '/test//' + cls)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/18:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    #data_csv = pd.read_csv("DataSet_Final.csv") ##Use if you have classes saved in any .csv file

    cls = 'NewData'

    #for name in data_csv['names'].unique()[:10]:
    #    classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    # Creating partitions of the data after shuffeling
    print("$$$$$$$ Class Name " + cls + " $$$$$$$")
    src = processed_dir +"//" + cls  # Folder to copy images from

    allFileNames = os.listdir(src)
    np.random.shuffle(allFileNames)
    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                              [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                               int(len(allFileNames) * (1 - val_ratio)),
                                                               ])

    train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
    val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
    test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

    print('Total images: '+ str(len(allFileNames)))
    print('Training: '+ str(len(train_FileNames)))
    print('Validation: '+  str(len(val_FileNames)))
    print('Testing: '+ str(len(test_FileNames)))

    # # Creating Train / Val / Test folders (One time use)
    os.makedirs(root_dir + '/train//' + cls)
    os.makedirs(root_dir + '/val//' + cls)
    os.makedirs(root_dir + '/test//' + cls)

    # Copy-pasting images
    for name in train_FileNames:
        shutil.copy(name, root_dir + '/train//' + cls)

    for name in val_FileNames:
        shutil.copy(name, root_dir + '/val//' + cls)

    for name in test_FileNames:
        shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/19:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    #data_csv = pd.read_csv("DataSet_Final.csv") ##Use if you have classes saved in any .csv file

    cls = 'NewData'

    #for name in data_csv['names'].unique()[:10]:
    #    classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    # Creating partitions of the data after shuffeling
    print("$$$$$$$ Class Name " + cls + " $$$$$$$")
    src = cls  # Folder to copy images from

    allFileNames = os.listdir(src)
    np.random.shuffle(allFileNames)
    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                              [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                               int(len(allFileNames) * (1 - val_ratio)),
                                                               ])

    train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
    val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
    test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

    print('Total images: '+ str(len(allFileNames)))
    print('Training: '+ str(len(train_FileNames)))
    print('Validation: '+  str(len(val_FileNames)))
    print('Testing: '+ str(len(test_FileNames)))

    # # Creating Train / Val / Test folders (One time use)
    os.makedirs(root_dir + '/train//' + cls)
    os.makedirs(root_dir + '/val//' + cls)
    os.makedirs(root_dir + '/test//' + cls)

    # Copy-pasting images
    for name in train_FileNames:
        shutil.copy(name, root_dir + '/train//' + cls)

    for name in val_FileNames:
        shutil.copy(name, root_dir + '/val//' + cls)

    for name in test_FileNames:
        shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/20:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    #data_csv = pd.read_csv("DataSet_Final.csv") ##Use if you have classes saved in any .csv file

    cls = 'NewData'

    #for name in data_csv['names'].unique()[:10]:
    #    classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    # Creating partitions of the data after shuffeling
    print("$$$$$$$ Class Name " + cls + " $$$$$$$")
    src = cls  # Folder to copy images from

    allFileNames = os.listdir(src)
    np.random.shuffle(allFileNames)
    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                              [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                               int(len(allFileNames) * (1 - val_ratio)),
                                                               ])

    train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
    val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
    test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

    print('Total images: '+ str(len(allFileNames)))
    print('Training: '+ str(len(train_FileNames)))
    print('Validation: '+  str(len(val_FileNames)))
    print('Testing: '+ str(len(test_FileNames)))

    # # Creating Train / Val / Test folders (One time use)
    os.makedirs(root_dir + '/train//' + cls)
    os.makedirs(root_dir + '/val//' + cls)
    os.makedirs(root_dir + '/test//' + cls)

    # Copy-pasting images
    for name in train_FileNames:
        shutil.copy(name, root_dir + '/train//' + cls)

    for name in val_FileNames:
        shutil.copy(name, root_dir + '/val//' + cls)

    for name in test_FileNames:
        shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/21:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    #data_csv = pd.read_csv("DataSet_Final.csv") ##Use if you have classes saved in any .csv file

    cls = 'NewData'

    #for name in data_csv['names'].unique()[:10]:
    #    classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

        # Creating partitions of the data after shuffeling
        print("$$$$$$$ Class Name " + cls + " $$$$$$$")
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Creating Train / Val / Test folders (One time use)
        os.makedirs(root_dir + '/train//' + cls)
        os.makedirs(root_dir + '/val//' + cls)
        os.makedirs(root_dir + '/test//' + cls)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/22:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    #data_csv = pd.read_csv("DataSet_Final.csv") ##Use if you have classes saved in any .csv file

    root_dir = 'NewData'
    classes_dir = ['cat', 'dog', 'elephant', 'tiger']

    #for name in data_csv['names'].unique()[:10]:
    #    classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Creating partitions of the data after shuffeling
        print("$$$$$$$ Class Name " + cls + " $$$$$$$")
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Creating Train / Val / Test folders (One time use)
        os.makedirs(root_dir + '/train//' + cls)
        os.makedirs(root_dir + '/val//' + cls)
        os.makedirs(root_dir + '/test//' + cls)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/23:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
15/24:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    #data_csv = pd.read_csv("DataSet_Final.csv") ##Use if you have classes saved in any .csv file

    root_dir = 'NewData'
    classes_dir = ['cat', 'dog', 'elephant', 'tiger']

    #for name in data_csv['names'].unique()[:10]:
    #    classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Creating partitions of the data after shuffeling
        print("$$$$$$$ Class Name " + cls + " $$$$$$$")
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Creating Train / Val / Test folders (One time use)
        os.makedirs(root_dir + '/train//' + cls)
        os.makedirs(root_dir + '/val//' + cls)
        os.makedirs(root_dir + '/test//' + cls)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/25:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
15/26:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    #data_csv = pd.read_csv("DataSet_Final.csv") ##Use if you have classes saved in any .csv file

    root_dir = 'NewData'
    classes_dir = ['cat', 'dog', 'elephant', 'tiger']

    #for name in data_csv['names'].unique()[:10]:
    #    classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Creating partitions of the data after shuffeling
        print("$$$$$$$ Class Name " + cls + " $$$$$$$")
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Creating Train / Val / Test folders (One time use)
        os.makedirs(root_dir + '/train//' + cls)
        os.makedirs(root_dir + '/val//' + cls)
        os.makedirs(root_dir + '/test//' + cls)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/27:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    data_csv = pd.read_csv("dataset.csv") ##Use if you have classes saved in any .csv file

    root_dir = 'NewData'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Creating partitions of the data after shuffeling
        print("$$$$$$$ Class Name " + cls + " $$$$$$$")
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Creating Train / Val / Test folders (One time use)
        os.makedirs(root_dir + '/train//' + cls)
        os.makedirs(root_dir + '/val//' + cls)
        os.makedirs(root_dir + '/test//' + cls)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/28:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    data_csv = pd.read_csv("dataset.csv") ##Use if you have classes saved in any .csv file

    root_dir = 'NewData'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Creating partitions of the data after shuffeling
        print("$$$$$$$ Class Name " + cls + " $$$$$$$")
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Creating Train / Val / Test folders (One time use)
        os.makedirs(root_dir + '/train//' + cls)
        os.makedirs(root_dir + '/val//' + cls)
        os.makedirs(root_dir + '/test//' + cls)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/29:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    data_csv = pd.read_csv("dataset.csv") ##Use if you have classes saved in any .csv file

    root_dir = 'NewData'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Creating partitions of the data after shuffeling
        print("$$$$$$$ Class Name " + cls + " $$$$$$$")
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Creating Train / Val / Test folders (One time use)
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/30:
import os
import shutil

def train_test_split():
    print("########### Train Test Val Script started ###########")
    data_csv = pd.read_csv("dataset.csv") ##Use if you have classes saved in any .csv file

    root_dir = 'NewData'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Creating partitions of the data after shuffeling
        print("$$$$$$$ Class Name " + cls + " $$$$$$$")
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Creating Train / Val / Test folders (One time use)
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("########### Train Test Val Script Ended ###########")

train_test_split()
15/31:
import os
import shutil

def train_test_split():
    print("#Spliting Data#")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Creating partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Creating Train / Val / Test folders (One time use)
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed#")

train_test_split()
15/32:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Creating partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Creating Train / Val / Test folders (One time use)
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/33:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/34:
data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/35:
img_width = 180
img_height =180
15/36:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/37:
# data_train = tf.keras.utils.image_dataset_from_directory(
#     data_train_path,
#     shuffle=True,
#     image_size=(img_width, img_height),
#     batch_size=32,
#     validation_split=False)
from PIL import Image

def load_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        img = Image.open(os.path.join(folder, filename))
        if img is not None:
            img = img.resize((img_width, img_height))
            images.append(np.array(img))
    return images



data_train = load_images_from_folder(data_train_path)
15/38:
# data_train = tf.keras.utils.image_dataset_from_directory(
#     data_train_path,
#     shuffle=True,
#     image_size=(img_width, img_height),
#     batch_size=32,
#     validation_split=False)
from PIL import Image

def load_images_from_folder(folder):
    images = []
    labels = []
    for filename in os.listdir(folder):
        img = Image.open(os.path.join(folder, filename))
        if img is not None:
            img = img.resize((img_width, img_height))  # Resize the image
            images.append(np.array(img))
            labels.append(folder)  # Use folder name as label
    return images, labels




data_train = load_images_from_folder(data_train_path)
15/39:
# data_train = tf.keras.utils.image_dataset_from_directory(
#     data_train_path,
#     shuffle=True,
#     image_size=(img_width, img_height),
#     batch_size=32,
#     validation_split=False)
from PIL import Image

def load_images_from_folder(folder):
    images = []
    labels = []
    for filename in os.listdir(folder):
        img = Image.open(os.path.join(folder, filename))
        if img is not None:
            img = img.resize((img_width, img_height))  # Resize the image
            images.append(np.array(img))
            labels.append(folder)  # Use folder name as label
    return images, labels




# data_train = load_images_from_folder(data_train_path)
15/40:
# data_train = tf.keras.utils.image_dataset_from_directory(
#     data_train_path,
#     shuffle=True,
#     image_size=(img_width, img_height),
#     batch_size=32,
#     validation_split=False)
from PIL import Image

def load_images_from_folder(folder):
    images = []
    labels = []
    for filename in os.listdir(folder):
        img = Image.open(os.path.join(folder, filename))
        if img is not None:
            img = img.resize((img_width, img_height))  # Resize the image
            images.append(np.array(img))
            labels.append(folder)  # Use folder name as label
    return images, labels




data_train = load_images_from_folder(data_train_path)
15/41:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/42:
data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/43:
img_width = 180
img_height =180
15/44:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/45: data_cat = data_train.class_names
15/46: data_cat
15/47:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/48:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
15/49:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/50: from tensorflow.keras.models import Sequential
15/51: data_train
15/52:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/53: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
15/54:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
15/55:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/56:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/57:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/58: score = tf.nn.softmax(predict)
15/59: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/60: model.save('Image_classify.keras')
15/61:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
15/62:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
15/63:
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/64:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/65: score = tf.nn.softmax(predict)
15/66: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/67:
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/68:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/69: score = tf.nn.softmax(predict)
15/70: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/71:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/72:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/73: score = tf.nn.softmax(predict)
15/74: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/75:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/76:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/77: score = tf.nn.softmax(predict)
15/78: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/79:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/80:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/81: score = tf.nn.softmax(predict)
15/82: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/83:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/84:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/85: score = tf.nn.softmax(predict)
15/86: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/87:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/88:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/89: score = tf.nn.softmax(predict)
15/90: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/91:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/92:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/93: score = tf.nn.softmax(predict)
15/94: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/95:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/96:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/97: score = tf.nn.softmax(predict)
15/98: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/99:
epochs_range = range(epochs_size)
plt.figure(figsize=(16,16))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/100:
epochs_range = range(epochs_size)
plt.figure(figsize=(4,4))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/101:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/102:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(2,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/103:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/104:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(2,2,2)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/105:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/106:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/107:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/108:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(2,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/109:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(2,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(2,2,1)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/110:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(2,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/111:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(2,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/112:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/113:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
15/114:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/115:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/116:
img_width = 180
img_height =180
15/117:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/118: data_cat = data_train.class_names
15/119: data_cat
15/120:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/121:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
15/122:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/123: from tensorflow.keras.models import Sequential
15/124: data_train
15/125:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/126: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
15/127:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
15/128:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/129:
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/130:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/131: score = tf.nn.softmax(predict)
15/132: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/133: model.save('Image_classify.keras')
15/134:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
15/135:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
15/136:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/137:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/138: score = tf.nn.softmax(predict)
15/139: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/140:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/141:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/142: score = tf.nn.softmax(predict)
15/143: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/144:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/145:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/146: score = tf.nn.softmax(predict)
15/147: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/148:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/149:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/150: score = tf.nn.softmax(predict)
15/151: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/152:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
15/153:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/154:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/155:
img_width = 180
img_height =180
15/156:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/157: data_cat = data_train.class_names
15/158: data_cat
15/159:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/160:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
15/161:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/162: from tensorflow.keras.models import Sequential
15/163: data_train
15/164:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/165: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
15/166:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
15/167:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/168:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/169:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/170: score = tf.nn.softmax(predict)
15/171: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/172:
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/173:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/174: score = tf.nn.softmax(predict)
15/175: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/176:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/177:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/178: score = tf.nn.softmax(predict)
15/179: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/180:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/181:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/182: score = tf.nn.softmax(predict)
15/183: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/184:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/185:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/186: score = tf.nn.softmax(predict)
15/187: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/188:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/189:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/190: score = tf.nn.softmax(predict)
15/191: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/192:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/193:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/194: score = tf.nn.softmax(predict)
15/195: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/196:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/197:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/198: score = tf.nn.softmax(predict)
15/199: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/200:
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/201:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/202: score = tf.nn.softmax(predict)
15/203: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/204:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/205:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/206: score = tf.nn.softmax(predict)
15/207: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/208:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/209:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/210: score = tf.nn.softmax(predict)
15/211: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/212: model.save('Image_classify.keras')
15/213:
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/214:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/215: score = tf.nn.softmax(predict)
15/216: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/217:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/218:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/219: score = tf.nn.softmax(predict)
15/220: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/221:
image = 'c3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/222:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/223: score = tf.nn.softmax(predict)
15/224: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/225:
image = 'c4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/226:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/227: score = tf.nn.softmax(predict)
15/228: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/229:
image = 'c3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/230:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/231: score = tf.nn.softmax(predict)
15/232: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/233:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/234:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/235: score = tf.nn.softmax(predict)
15/236: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/237:
image = 'd3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/238:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/239: score = tf.nn.softmax(predict)
15/240: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/241:
image = 'd4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/242:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/243: score = tf.nn.softmax(predict)
15/244: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/245:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/246:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/247: score = tf.nn.softmax(predict)
15/248: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/249:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/250:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/251: score = tf.nn.softmax(predict)
15/252: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/253:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
15/254:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/255:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/256:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/257:
img_width = 180
img_height =180
15/258:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/259: data_cat = data_train.class_names
15/260: data_cat
15/261:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/262:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
15/263:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/264:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
15/265:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/266:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/267:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/268:
img_width = 180
img_height =180
15/269:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/270: data_cat = data_train.class_names
15/271: data_cat
15/272:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/273:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
15/274:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/275:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
15/276:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/277:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/278:
img_width = 180
img_height =180
15/279:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=False,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/280: data_cat = data_train.class_names
15/281: data_cat
15/282:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/283:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
15/284:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
16/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
16/2:
data_train_path = 'Fruits_Vegetables/train'
data_test_path = 'Fruits_Vegetables/test'
data_val_path = 'Fruits_Vegetables/validation'
16/3:
img_width = 180
img_height =180
16/4:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
16/5: data_cat = data_train.class_names
16/6: data_cat
16/7:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
16/8:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
16/9:
plt.figure(figsize=(10,10))
for image, labels in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(image[i].numpy().astype('uint8'))
        plt.title(data_cat[labels[i]])
        plt.axis('off')
16/10: from tensorflow.keras.models import Sequential
16/11: data_train
16/12:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
16/13: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
16/14:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
15/285:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/286:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/287:
img_width = 180
img_height =180
15/288:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=False,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/289: data_cat = data_train.class_names
15/290: data_cat
15/291:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/292:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=True,
    batch_size=32,
    validation_split=False
)
15/293:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/294:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/295:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=True,
    batch_size=32,
    validation_split=False
)
15/296:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/297:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/298:
img_width = 180
img_height =180
15/299:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=False,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/300: data_cat = data_train.class_names
15/301: data_cat
15/302:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/303:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=True,
    batch_size=32,
    validation_split=False
)
15/304:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/305: from tensorflow.keras.models import Sequential
15/306: data_train
15/307:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/308: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
15/309:
epochs_size = 15
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
15/310:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/311:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/312:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/313:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/314:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/315:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/316: score = tf.nn.softmax(predict)
15/317: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/318:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
15/319:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing

import splitfolders
15/320:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import splitfolders
15/321:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing

import splitfolders
15/322:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
15/323:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
15/324:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/325:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
15/326:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/327:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/328:
img_width = 180
img_height =180
15/329:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=False,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/330: data_cat = data_train.class_names
15/331: data_cat
15/332:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/333:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=True,
    batch_size=32,
    validation_split=False
)
15/334:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/335:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
15/336:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/337:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/338:
img_width = 180
img_height =180
15/339:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=False,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/340: data_cat = data_train.class_names
15/341: data_cat
15/342:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/343:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=True,
    batch_size=32,
    validation_split=False
)
15/344:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/345:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/346:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
15/347:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
15/348:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/349:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/350:
img_width = 180
img_height =180
15/351:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/352: data_cat = data_train.class_names
15/353: data_cat
15/354:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/355:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
15/356:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/357: from tensorflow.keras.models import Sequential
15/358: data_train
15/359:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(128, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(256, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/360: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
15/361:
epochs_size = 15
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
15/362:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/363:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/364:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/365: score = tf.nn.softmax(predict)
15/366: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/367:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/368:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/369: score = tf.nn.softmax(predict)
15/370: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/371:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/372:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/373: score = tf.nn.softmax(predict)
15/374: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/375:
image = 'e3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/376:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/377: score = tf.nn.softmax(predict)
15/378: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/379:
image = 'e4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/380:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/381:
image = 'c4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/382:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/383: score = tf.nn.softmax(predict)
15/384: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/385:
image = 'c3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/386:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/387: score = tf.nn.softmax(predict)
15/388: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/389:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/390:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/391: score = tf.nn.softmax(predict)
15/392: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/393:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/394:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/395: score = tf.nn.softmax(predict)
15/396: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/397:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/398:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/399: score = tf.nn.softmax(predict)
15/400: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/401:
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/402:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/403: score = tf.nn.softmax(predict)
15/404: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/405:
image = 't3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/406:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/407:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/408:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/409: score = tf.nn.softmax(predict)
15/410: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/411:
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/412:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/413: score = tf.nn.softmax(predict)
15/414: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/415:
image = 'd3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/416:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/417: score = tf.nn.softmax(predict)
15/418: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/419:
image = 'd4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/420:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/421: score = tf.nn.softmax(predict)
15/422: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/423:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
15/424:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/425:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/426:
img_width = 180
img_height =180
15/427:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/428: data_cat = data_train.class_names
15/429: data_cat
15/430:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/431:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
15/432:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/433: from tensorflow.keras.models import Sequential
15/434: data_train
15/435:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(128, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(256, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/436: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
15/437:
epochs_size = 15
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
15/438:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/439:
image = 'd4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/440:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/441: score = tf.nn.softmax(predict)
15/442: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/443: model.save('Image_classify.keras')
15/444:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
15/445:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
15/446:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/447:
image = 'd4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/448:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/449: score = tf.nn.softmax(predict)
15/450: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/451:
image = 'd3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/452:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/453: score = tf.nn.softmax(predict)
15/454: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/455:
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/456:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/457: score = tf.nn.softmax(predict)
15/458: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/459:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/460:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/461: score = tf.nn.softmax(predict)
15/462: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/463:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
15/464:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/465:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
15/466:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/467: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
15/468:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
15/469: from tensorflow.keras.models import Sequential
15/470: data_train
15/471:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/472: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
15/473:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
15/474:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
15/475:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/476:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/477:
img_width = 180
img_height =180
15/478:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/479:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/480: data_cat = data_train.class_names
15/481: data_cat
15/482:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/483:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
15/484:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/485: from tensorflow.keras.models import Sequential
15/486: data_train
15/487:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/488: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
15/489:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
15/490:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/491:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.2),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/492: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
15/493:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
15/494:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/495:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/496:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/497: score = tf.nn.softmax(predict)
15/498: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/499:
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/500:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/501: score = tf.nn.softmax(predict)
15/502: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/503:
image = 'd3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/504:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/505: score = tf.nn.softmax(predict)
15/506: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/507:
image = 'd4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/508:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/509: score = tf.nn.softmax(predict)
15/510: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/511:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/512:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/513: score = tf.nn.softmax(predict)
15/514: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/515:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/516:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/517: score = tf.nn.softmax(predict)
15/518: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/519:
image = 'e3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/520:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/521: score = tf.nn.softmax(predict)
15/522: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/523:
image = 'e4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/524:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/525:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/526:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/527: score = tf.nn.softmax(predict)
15/528: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/529:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/530:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/531: score = tf.nn.softmax(predict)
15/532: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/533:
image = 'c3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/534:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/535: score = tf.nn.softmax(predict)
15/536: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/537:
image = 'c4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/538:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/539: score = tf.nn.softmax(predict)
15/540: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/541:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/542:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/543: score = tf.nn.softmax(predict)
15/544: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/545:
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/546:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/547: score = tf.nn.softmax(predict)
15/548: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/549:
image = 't3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/550:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/551:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
15/552:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
15/553: score = tf.nn.softmax(predict)
15/554: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
15/555:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
15/556:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.2x0
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/557:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/558:
img_width = 180
img_height =180
15/559:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/560: data_cat = data_train.class_names
15/561:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
15/562:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
15/563:
img_width = 180
img_height =180
15/564:
data_train = tf.keras.utils.image_dataset_from_directory(
    data_train_path,
    shuffle=True,
    image_size=(img_width, img_height),
    batch_size=32,
    validation_split=False)
15/565: data_cat = data_train.class_names
15/566: data_cat
15/567:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
15/568:
data_test = tf.keras.utils.image_dataset_from_directory(
data_test_path,
    image_size=(img_height,img_width),
    shuffle=False,
    batch_size=32,
    validation_split=False
)
15/569:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/570: from tensorflow.keras.models import Sequential
15/571: data_train
15/572:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.4),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/573: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
15/574:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
15/575:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
15/576: from tensorflow.keras.models import Sequential
15/577: data_train
15/578:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.4),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
15/579: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
15/580:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
17/1:
import pandas as pd

weather = pd.read_csv("local_weather.csv", index_col="DATE")
17/2: weather
17/3:
core_weather = weather[["PRCP", "SNOW", "SNWD", "TMAX", "TMIN"]].copy()
core_weather.columns = ["precip", "snow", "snow_depth", "temp_max", "temp_min"]
17/4: core_weather.apply(pd.isnull).sum()
17/5: core_weather["snow"].value_counts()
17/6: core_weather["snow_depth"].value_counts()
17/7: del core_weather["snow"]
17/8: del core_weather["snow_depth"]
17/9: core_weather[pd.isnull(core_weather["precip"])]
17/10: core_weather.loc["2013-12-15",:]
17/11: core_weather["precip"].value_counts() / core_weather.shape[0]
17/12: core_weather["precip"] = core_weather["precip"].fillna(0)
17/13: core_weather.apply(pd.isnull).sum()
17/14: core_weather[pd.isnull(core_weather["temp_min"])]
17/15: core_weather.loc["2011-12-18":"2011-12-28"]
17/16: core_weather = core_weather.fillna(method="ffill")
17/17: core_weather.apply(pd.isnull).sum()
17/18:
# Check for missing value defined in data documentation
core_weather.apply(lambda x: (x == 9999).sum())
17/19: core_weather.dtypes
17/20: core_weather.index
17/21: core_weather.index = pd.to_datetime(core_weather.index)
17/22: core_weather.index
17/23: core_weather.index.year
17/24: core_weather[["temp_max", "temp_min"]].plot()
17/25: core_weather.index.year.value_counts().sort_index()
17/26: core_weather["precip"].plot()
17/27: core_weather.groupby(core_weather.index.year).apply(lambda x: x["precip"].sum()).plot()
17/28: core_weather["target"] = core_weather.shift(-1)["temp_max"]
17/29: core_weather
17/30: core_weather = core_weather.iloc[:-1,:].copy()
17/31: core_weather
17/32:
from sklearn.linear_model import Ridge

reg = Ridge(alpha=.1)
17/33: predictors = ["precip", "temp_max", "temp_min"]
17/34:
train = core_weather.loc[:"2020-12-31"]
test = core_weather.loc["2021-01-01":]
17/35: train
17/36: test
17/37: reg.fit(train[predictors], train["target"])
17/38: predictions = reg.predict(test[predictors])
17/39:
from sklearn.metrics import mean_squared_error

mean_squared_error(test["target"], predictions)
17/40:
combined = pd.concat([test["target"], pd.Series(predictions, index=test.index)], axis=1)
combined.columns = ["actual", "predictions"]
17/41: combined
17/42: combined.plot()
17/43: reg.coef_
17/44:
core_weather["month_max"] = core_weather["temp_max"].rolling(30).mean()

core_weather["month_day_max"] = core_weather["month_max"] / core_weather["temp_max"]

core_weather["max_min"] = core_weather["temp_max"] / core_weather["temp_min"]
17/45: core_weather = core_weather.iloc[30:,:].copy()
17/46:
def create_predictions(predictors, core_weather, reg):
    train = core_weather.loc[:"2020-12-31"]
    test = core_weather.loc["2021-01-01":]

    reg.fit(train[predictors], train["target"])
    predictions = reg.predict(test[predictors])

    error = mean_squared_error(test["target"], predictions)
    
    combined = pd.concat([test["target"], pd.Series(predictions, index=test.index)], axis=1)
    combined.columns = ["actual", "predictions"]
    return error, combined
17/47:
predictors = ["precip", "temp_max", "temp_min", "month_day_max", "max_min"]

error, combined = create_predictions(predictors, core_weather, reg)
error
17/48: combined.plot()
17/49:
core_weather["monthly_avg"] = core_weather["temp_max"].groupby(core_weather.index.month).apply(lambda x: x.expanding(1).mean())
core_weather["day_of_year_avg"] = core_weather["temp_max"].groupby(core_weather.index.day_of_year).apply(lambda x: x.expanding(1).mean())
17/50:
core_weather["monthly_avg"] = core_weather["temp_max"].groupby(core_weather.index.month).apply(lambda x: x.expanding(1).mean())
core_weather["day_of_year_avg"] = core_weather["temp_max"].groupby(core_weather.index.day_of_year).apply(lambda x: x.expanding(1).mean())
17/51:
error, combined = create_predictions(predictors + ["monthly_avg", "day_of_year_avg"], core_weather, reg)
error
17/52: reg.coef_
17/53: core_weather.corr()["target"]
17/54: combined["diff"] = (combined["actual"] - combined["predictions"]).abs()
17/55: combined.sort_values("diff", ascending=False).head(10)
18/1:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))
def load_images_from_folder(folder):
    images = []
    labels = []
    label = 0
    for filename in os.listdir(folder):
        category_folder = os.path.join(folder, filename)
        if os.path.isdir(category_folder):
            for image_name in os.listdir(category_folder):
                img_path = os.path.join(category_folder, image_name)
                img = load_img(img_path, target_size=(img_height, img_width))
                img_array = img_to_array(img)
                images.append(img_array)
                labels.append(label)
            label += 1
    return np.array(images), np.array(labels)

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/2:
img_width = 180
img_height =180
18/3:
x_train, y_train = load_images_from_folder(data_train_path)
x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
18/4:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
18/5:
import os
import shutil

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
18/6:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))
def load_images_from_folder(folder):
    images = []
    labels = []
    label = 0
    for filename in os.listdir(folder):
        category_folder = os.path.join(folder, filename)
        if os.path.isdir(category_folder):
            for image_name in os.listdir(category_folder):
                img_path = os.path.join(category_folder, image_name)
                img = load_img(img_path, target_size=(img_height, img_width))
                img_array = img_to_array(img)
                images.append(img_array)
                labels.append(label)
            label += 1
    return np.array(images), np.array(labels)

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/7:
img_width = 180
img_height =180
18/8:
x_train, y_train = load_images_from_folder(data_train_path)
x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
18/9:
import os
import shutil
from tensorflow.keras.preprocessing.image import load_img, img_to_array

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
18/10:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))
def load_images_from_folder(folder):
    images = []
    labels = []
    label = 0
    for filename in os.listdir(folder):
        category_folder = os.path.join(folder, filename)
        if os.path.isdir(category_folder):
            for image_name in os.listdir(category_folder):
                img_path = os.path.join(category_folder, image_name)
                img = load_img(img_path, target_size=(img_height, img_width))
                img_array = img_to_array(img)
                images.append(img_array)
                labels.append(label)
            label += 1
    return np.array(images), np.array(labels)

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/11:
img_width = 180
img_height =180
18/12:
x_train, y_train = load_images_from_folder(data_train_path)
x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
18/13:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))
def load_images_from_folder(folder):
    images = []
    labels = []
    label = 0
    for subdir in os.listdir(folder):
        subdir_path = os.path.join(folder, subdir)
        if os.path.isdir(subdir_path):
            for filename in os.listdir(subdir_path):
                if filename.endswith((".jpg", ".png", ".jpeg")):  # check if the file is an image
                    img_path = os.path.join(subdir_path, filename)
                    try:
                        img = load_img(img_path, target_size=(img_height, img_width))
                        img_array = img_to_array(img)
                        images.append(img_array)
                        labels.append(label)
                    except:
                        print(f"Can't read the file {img_path}")
            label += 1
    return np.array(images), np.array(labels)

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/14:
img_width = 180
img_height =180
18/15:
x_train, y_train = load_images_from_folder(data_train_path)
x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
18/16:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))
def load_images_from_folder(folder):
    images = []
    labels = []
    label = 0
    for subdir in os.listdir(folder):
        subdir_path = os.path.join(folder, subdir)
        if os.path.isdir(subdir_path):
            for filename in os.listdir(subdir_path):
                if filename.endswith((".jpg", ".png", ".jpeg")):  # check if the file is an image
                    img_path = os.path.join(subdir_path, filename)
                    try:
                        img = load_img(img_path, target_size=(img_height, img_width))
                        img_array = img_to_array(img)
                        images.append(img_array)
                        labels.append(label)
                    except:
                        print(f"Can't read the file {img_path}")
            label += 1
    return np.array(images), np.array(labels)

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/17:
img_width = 180
img_height =180
18/18:
x_train, y_train = load_images_from_folder(data_train_path)
x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
18/19:
# data_train = tf.keras.utils.image_dataset_from_directory(
#     data_train_path,
#     shuffle=True,
#     image_size=(img_width, img_height),
#     batch_size=32,
#     validation_split=False)
18/20:
# data_train = tf.keras.utils.image_dataset_from_directory(
#     data_train_path,
#     shuffle=True,
#     image_size=(img_width, img_height),
#     batch_size=32,
#     validation_split=False)
data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
18/21: data_cat = data_train.class_names
18/22: # data_cat = data_train.class_names
18/23: data_cat
18/24: # data_cat
18/25:
data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
                                                       image_size=(img_height,img_width),
                                                       batch_size=32,
                                                        shuffle=False,
                                                       validation_split=False)
18/26:
# data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
#                                                        image_size=(img_height,img_width),
#                                                        batch_size=32,
#                                                         shuffle=False,
#                                                        validation_split=False)
18/27:
# data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
#                                                        image_size=(img_height,img_width),
#                                                        batch_size=32,
#                                                         shuffle=False,
#                                                        validation_split=False)
data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)
18/28:
# data_test = tf.keras.utils.image_dataset_from_directory(
# data_test_path,
#     image_size=(img_height,img_width),
#     shuffle=False,
#     batch_size=32,
#     validation_split=False
# )
data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
18/29:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
18/30:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(classes_dir[labs[i]])
        plt.axis('off')
18/31:
# data_cat = data_train.class_names
data_cat = classes_dir
18/32:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
18/33:
import os
import shutil
from tensorflow.keras.preprocessing.image import load_img, img_to_array

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
18/34:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))
def load_images_from_folder(folder):
    images = []
    labels = []
    label = 0
    for subdir in os.listdir(folder):
        subdir_path = os.path.join(folder, subdir)
        if os.path.isdir(subdir_path):
            for filename in os.listdir(subdir_path):
                if filename.endswith((".jpg", ".png", ".jpeg")):  # check if the file is an image
                    img_path = os.path.join(subdir_path, filename)
                    try:
                        img = load_img(img_path, target_size=(img_height, img_width))
                        img_array = img_to_array(img)
                        images.append(img_array)
                        labels.append(label)
                    except:
                        print(f"Can't read the file {img_path}")
            label += 1
    return np.array(images), np.array(labels)

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/35:
img_width = 180
img_height =180
18/36:
x_train, y_train = load_images_from_folder(data_train_path)
x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
18/37:
# data_train = tf.keras.utils.image_dataset_from_directory(
#     data_train_path,
#     shuffle=True,
#     image_size=(img_width, img_height),
#     batch_size=32,
#     validation_split=False)
data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
18/38:
# data_cat = data_train.class_names
data_cat = classes_dir
18/39: data_cat
18/40:
# data_cat = data_train.class_names
for name in data_csv['names'].unique()[:10]:
   classes_dir.append(name)
data_cat = classes_dir
18/41:
# data_cat = data_train.class_names
for name in data_csv['names'].unique()[:10]:
   classes_dir.append(name)
data_cat = classes_dir
18/42:
# data_cat = data_train.class_names
classes_dir = []
for name in data_csv['names'].unique()[:10]:
   classes_dir.append(name)
data_cat = classes_dir
18/43:
# data_cat = data_train.class_names
classes_dir = []
for name in data_csv['names'].unique()[:10]:
   classes_dir.append(name)
data_cat = classes_dir
18/44:
# data_cat = data_train.class_names
classes_dir = []
for name in data_csv['names'].unique()[:10]:
   classes_dir.append(name)
data_cat = classes_dir
18/45:
# data_cat = data_train.class_names
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/46:
# data_cat = data_train.class_names
data_csv = pd.read_csv("dataset.csv")
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/47: data_cat
18/48:
# data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
#                                                        image_size=(img_height,img_width),
#                                                        batch_size=32,
#                                                         shuffle=False,
#                                                        validation_split=False)
data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)
18/49:
# data_test = tf.keras.utils.image_dataset_from_directory(
# data_test_path,
#     image_size=(img_height,img_width),
#     shuffle=False,
#     batch_size=32,
#     validation_split=False
# )
data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
18/50:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
18/51: from tensorflow.keras.models import Sequential
18/52: data_train
18/53:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.4),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
18/54: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
18/55:
epochs_size = 10
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
18/56:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
18/57:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/58:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/59: score = tf.nn.softmax(predict)
18/60: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/61:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/62:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/63: score = tf.nn.softmax(predict)
18/64: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/65:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/66:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/67: score = tf.nn.softmax(predict)
18/68: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/69:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/70:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/71: score = tf.nn.softmax(predict)
18/72: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/73:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
18/74:
import os
import shutil
from tensorflow.keras.preprocessing.image import load_img, img_to_array

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
18/75:
# input_folder = "RawData/"
# output = "Dataset/"
# splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1))
def load_images_from_folder(folder):
    images = []
    labels = []
    label = 0
    for subdir in os.listdir(folder):
        subdir_path = os.path.join(folder, subdir)
        if os.path.isdir(subdir_path):
            for filename in os.listdir(subdir_path):
                if filename.endswith((".jpg", ".png", ".jpeg")):  # check if the file is an image
                    img_path = os.path.join(subdir_path, filename)
                    try:
                        img = load_img(img_path, target_size=(img_height, img_width))
                        img_array = img_to_array(img)
                        images.append(img_array)
                        labels.append(label)
                    except:
                        print(f"Can't read the file {img_path}")
            label += 1
    return np.array(images), np.array(labels)

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/76:
img_width = 180
img_height =180
18/77:
# x_train, y_train = load_images_from_folder(data_train_path)
# Load and shuffle the training data
x_train, y_train = load_images_from_folder(data_train_path)
indices = np.arange(x_train.shape[0])
np.random.shuffle(indices)
x_train = x_train[indices]
y_train = y_train[indices]

x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
18/78:
# data_train = tf.keras.utils.image_dataset_from_directory(
#     data_train_path,
#     shuffle=True,
#     image_size=(img_width, img_height),
#     batch_size=32,
#     validation_split=False)
data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
18/79:
# data_cat = data_train.class_names
data_csv = pd.read_csv("dataset.csv")
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/80: data_cat
18/81:
# data_val = tf.keras.utils.image_dataset_from_directory(data_val_path,
#                                                        image_size=(img_height,img_width),
#                                                        batch_size=32,
#                                                         shuffle=False,
#                                                        validation_split=False)
data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)
18/82:
# data_test = tf.keras.utils.image_dataset_from_directory(
# data_test_path,
#     image_size=(img_height,img_width),
#     shuffle=False,
#     batch_size=32,
#     validation_split=False
# )
data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
18/83:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
18/84: from tensorflow.keras.models import Sequential
18/85: data_train
18/86:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.4),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
18/87: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
18/88:
epochs_size = 10
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
18/89:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
18/90:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/91:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/92: score = tf.nn.softmax(predict)
18/93: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/94:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/95:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/96: score = tf.nn.softmax(predict)
18/97: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/98:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/99:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/100: score = tf.nn.softmax(predict)
18/101: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/102:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/103:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/104: score = tf.nn.softmax(predict)
18/105: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/106:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/107:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/108: score = tf.nn.softmax(predict)
18/109: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/110:
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/111:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/112: score = tf.nn.softmax(predict)
18/113: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/114:
image = 'd3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/115:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/116: score = tf.nn.softmax(predict)
18/117: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/118:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/119:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/120: score = tf.nn.softmax(predict)
18/121: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/122:
image = 'c3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/123:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/124: score = tf.nn.softmax(predict)
18/125: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/126:
image = 'c4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/127:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/128: score = tf.nn.softmax(predict)
18/129: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/130:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/131:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/132: score = tf.nn.softmax(predict)
18/133: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/134:
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/135:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/136: score = tf.nn.softmax(predict)
18/137: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/138:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/139:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/140: score = tf.nn.softmax(predict)
18/141: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/142:
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/143:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/144: score = tf.nn.softmax(predict)
18/145: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/146:
image = 'd3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/147:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/148: score = tf.nn.softmax(predict)
18/149: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/150:
image = 'd3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/151:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/152: score = tf.nn.softmax(predict)
18/153: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/154:
image = 'd4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/155:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/156: score = tf.nn.softmax(predict)
18/157: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/158: data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
18/159:
# data_cat = data_train.class_names
data_csv = pd.read_csv("dataset.csv")
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/160:
data_csv = pd.read_csv("dataset.csv")
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/161: data_cat
18/162: data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)
18/163: data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
18/164:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
18/165: from tensorflow.keras.models import Sequential
18/166: data_train
18/167:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.4),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
18/168: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
18/169:
epochs_size = 10
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
18/170:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
18/171:
image = 'd4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/172:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/173: score = tf.nn.softmax(predict)
18/174: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/175:
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/176:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/177: score = tf.nn.softmax(predict)
18/178: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/179:
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/180:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/181: score = tf.nn.softmax(predict)
18/182: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/183:
image = 'c3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/184:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/185: score = tf.nn.softmax(predict)
18/186: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/187:
image = 'c4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/188:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/189: score = tf.nn.softmax(predict)
18/190: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/191:
image = 'd4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/192:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/193: score = tf.nn.softmax(predict)
18/194: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/195:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/196:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/197: score = tf.nn.softmax(predict)
18/198: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/199:
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/200:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/201: score = tf.nn.softmax(predict)
18/202: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/203:
image = 'd3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/204:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/205: score = tf.nn.softmax(predict)
18/206: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/207:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/208:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/209: score = tf.nn.softmax(predict)
18/210: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/211:
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/212:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/213: score = tf.nn.softmax(predict)
18/214: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/215:
image = 'e3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/216:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/217: score = tf.nn.softmax(predict)
18/218: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/219:
image = 'e4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/220:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/221:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/222:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/223: score = tf.nn.softmax(predict)
18/224: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/225:
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/226:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/227: score = tf.nn.softmax(predict)
18/228: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/229:
image = 't3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/230:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/231:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
18/232:
import os
import shutil
from tensorflow.keras.preprocessing.image import load_img, img_to_array

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
18/233:
def load_images_from_folder(folder):
    images = []
    labels = []
    label = 0
    for subdir in os.listdir(folder):
        subdir_path = os.path.join(folder, subdir)
        if os.path.isdir(subdir_path):
            for filename in os.listdir(subdir_path):
                if filename.endswith((".jpg", ".png", ".jpeg")):  # check if the file is an image
                    img_path = os.path.join(subdir_path, filename)
                    try:
                        img = load_img(img_path, target_size=(img_height, img_width))
                        img_array = img_to_array(img)
                        images.append(img_array)
                        labels.append(label)
                    except:
                        print(f"Can't read the file {img_path}")
            label += 1
    return np.array(images), np.array(labels)

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/234:
img_width = 180
img_height =180
18/235:
x_train, y_train = load_images_from_folder(data_train_path)
indices = np.arange(x_train.shape[0])
np.random.shuffle(indices)
x_train = x_train[indices]
y_train = y_train[indices]

x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
18/236: data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
18/237:
data_csv = pd.read_csv("dataset.csv")
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/238: data_cat
18/239: data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)
18/240: data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
18/241:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
18/242: from tensorflow.keras.models import Sequential
18/243: data_train
18/244:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.4),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
18/245: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
18/246:
epochs_size = 25
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
18/247:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
18/248:
image = 't3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/249:
image = 't3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/250:
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/251:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/252: score = tf.nn.softmax(predict)
18/253: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/254:
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/255:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/256: score = tf.nn.softmax(predict)
18/257: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/258:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/259:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/260: score = tf.nn.softmax(predict)
18/261: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/262:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import splitfolders
18/263:
import os
import shutil
from tensorflow.keras.preprocessing.image import load_img, img_to_array

def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
18/264:
def load_images_from_folder(folder):
    images = []
    labels = []
    label = 0
    for subdir in os.listdir(folder):
        subdir_path = os.path.join(folder, subdir)
        if os.path.isdir(subdir_path):
            for filename in os.listdir(subdir_path):
                if filename.endswith((".jpg", ".png", ".jpeg")):  # check if the file is an image
                    img_path = os.path.join(subdir_path, filename)
                    try:
                        img = load_img(img_path, target_size=(img_height, img_width))
                        img_array = img_to_array(img)
                        images.append(img_array)
                        labels.append(label)
                    except:
                        print(f"Can't read the file {img_path}")
            label += 1
    return np.array(images), np.array(labels)

data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/265:
img_width = 180
img_height =180
18/266:
x_train, y_train = load_images_from_folder(data_train_path)
indices = np.arange(x_train.shape[0])
np.random.shuffle(indices)
x_train = x_train[indices]
y_train = y_train[indices]

x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
18/267: data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
18/268:
data_csv = pd.read_csv("dataset.csv")
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/269: data_cat
18/270: data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)
18/271: data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
18/272:
plt.figure(figsize=(10,10))
for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
18/273: from tensorflow.keras.models import Sequential
18/274: data_train
18/275:
data_augmentation = keras.Sequential(
    [
        RandomFlip("horizontal"),
        RandomRotation(0.1),
        RandomZoom(0.1),
    ]
)
model = Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.4),
    layers.Dense(128),
    layers.Dense(len(data_cat))
                  
])
18/276: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
18/277:
epochs_size = 10
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
18/278:
epochs_range = range(epochs_size)
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
18/279:
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image)
img_bat=tf.expand_dims(img_arr,0)
18/280:
predict = model.predict(img_bat)
prediction = np.argmax(predict, axis=1)
18/281: score = tf.nn.softmax(predict)
18/282: print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/283: model.save('Image_classify.keras')
18/284:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
18/285:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
18/286:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import os
import shutil
from tensorflow.keras.preprocessing.image import load_img, img_to_array
18/287:
def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
18/288:
def load_images_from_folder(folder):
    images = []
    labels = []
    label = 0
    for subdir in os.listdir(folder):
        subdir_path = os.path.join(folder, subdir)
        if os.path.isdir(subdir_path):
            for filename in os.listdir(subdir_path):
                if filename.endswith((".jpg", ".png", ".jpeg")):  # check if the file is an image
                    img_path = os.path.join(subdir_path, filename)
                    try:
                        img = load_img(img_path, target_size=(img_height, img_width))
                        img_array = img_to_array(img)
                        images.append(img_array)
                        labels.append(label)
                    except:
                        print(f"Can't read the file {img_path}")
            label += 1
    return np.array(images), np.array(labels)
#Path for train, test and validation data
data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/289:
img_width = 180
img_height =180
18/290:
img_width = 180
img_height =180
18/291:
# Function to load images from a given folder
def load_images_from_folder(folder):
    # Initialize empty lists to store images and labels
    images = []
    labels = []
    label = 0  # Initialize label counter

    # Loop over all subdirectories in the given folder
    for subdir in os.listdir(folder):
        subdir_path = os.path.join(folder, subdir)  # Get the path of the subdirectory

        # Check if the path is a directory
        if os.path.isdir(subdir_path):
            # Loop over all files in the subdirectory
            for filename in os.listdir(subdir_path):
                # Check if the file is an image
                if filename.endswith((".jpg", ".png", ".jpeg")):
                    img_path = os.path.join(subdir_path, filename)  # Get the path of the image

                    try:
                        # Load the image and resize it to the target size
                        img = load_img(img_path, target_size=(img_height, img_width))
                        # Convert the image to an array
                        img_array = img_to_array(img)
                        # Append the image array and label to the respective lists
                        images.append(img_array)
                        labels.append(label)
                    except:
                        # Print an error message if the file can't be read
                        print(f"Can't read the file {img_path}")

            # Increment the label counter for the next subdirectory
            label += 1

    # Return the images and labels as numpy arrays
    return np.array(images), np.array(labels)

# Paths for train, test and validation data
data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/292:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import os
import shutil
from tensorflow.keras.preprocessing.image import load_img, img_to_array
18/293:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import os
import shutil
from tensorflow.keras.preprocessing.image import load_img, img_to_array
18/294:
#Import Necessary Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import os
import shutil
from tensorflow.keras.preprocessing.image import load_img, img_to_array
18/295:
#Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import os
import shutil
from tensorflow.keras.preprocessing.image import load_img, img_to_array
18/296:
def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
18/297:
# Function to load images from a given folder
def load_images_from_folder(folder):
    # Initialize empty lists to store images and labels
    images = []
    labels = []
    label = 0  # Initialize label counter

    # Loop over all subdirectories in the given folder
    for subdir in os.listdir(folder):
        subdir_path = os.path.join(folder, subdir)  # Get the path of the subdirectory

        # Check if the path is a directory
        if os.path.isdir(subdir_path):
            # Loop over all files in the subdirectory
            for filename in os.listdir(subdir_path):
                # Check if the file is an image
                if filename.endswith((".jpg", ".png", ".jpeg")):
                    img_path = os.path.join(subdir_path, filename)  # Get the path of the image

                    try:
                        # Load the image and resize it to the target size
                        img = load_img(img_path, target_size=(img_height, img_width))
                        # Convert the image to an array
                        img_array = img_to_array(img)
                        # Append the image array and label to the respective lists
                        images.append(img_array)
                        labels.append(label)
                    except:
                        # Print an error message if the file can't be read
                        print(f"Can't read the file {img_path}")

            # Increment the label counter for the next subdirectory
            label += 1

    # Return the images and labels as numpy arrays
    return np.array(images), np.array(labels)

# Paths for train, test and validation data
data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/298:
img_width = 180
img_height =180
18/299:
# Set the dimensions of the images to be loaded
img_width = 180
img_height = 180
18/300:
# Load the training images and labels
x_train, y_train = load_images_from_folder(data_train_path)

# Create an array of indices and shuffle them
indices = np.arange(x_train.shape[0])
np.random.shuffle(indices)

# Use the shuffled indices to shuffle the training images and labels
x_train = x_train[indices]
y_train = y_train[indices]

# Load the validation and test images and labels
x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
18/301:
# Create TensorFlow datasets for the training, validation, and test data
data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)
data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
18/302:
# Load a CSV file into a pandas DataFrame
data_csv = pd.read_csv("dataset.csv")

# Get the unique names from the 'names' column of the DataFrame and add the first 10 to a list
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/303:
# Print the list of names
data_cat
18/304:
# Create a 3x3 grid of subplots
plt.figure(figsize=(10,10))

# Take one batch of the training data
for img, labs in data_train.take(1):
    # Loop over the first 9 images in the batch
    for i in range(9):
        # Add a subplot for the current image
        plt.subplot(3,3,i+1)
        # Display the image
        plt.imshow(img[i].numpy().astype('uint8'))
        # Set the title of the subplot to the corresponding name
        plt.title(data_cat[labs[i]])
        # Remove the axes
        plt.axis('off')
18/305:
#Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

import os
import shutil
from tensorflow.keras.preprocessing.image import load_img, img_to_array
18/306:
# Compile the model with the Adam optimizer, sparse categorical crossentropy loss, and accuracy metric
model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
18/307:
# Set the number of epochs for training
epochs_size = 10

# Train the model on the training data and validate it on the validation data
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
18/308:
# Create a range of numbers up to the number of epochs for plotting
epochs_range = range(epochs_size)

# Create a new figure for the plots
plt.figure(figsize=(8,8))

# Add a subplot for the accuracy
plt.subplot(1,2,1)
# Plot the training and validation accuracy
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

# Add a subplot for the loss
plt.subplot(1,2,2)
# Plot the training and validation loss
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
18/309:
# Load an image for prediction
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/310:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/311:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/312:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/313:
# Save the model
model.save('Image_classify.keras')
18/314:
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
18/315:
#Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Sequential

import os
import shutil
18/316:
#Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Sequential

import os
import shutil

from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
18/317:
def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
18/318:
# Function to load images from a given folder
def load_images_from_folder(folder):
    # Initialize empty lists to store images and labels
    images = []
    labels = []
    label = 0  # Initialize label counter

    # Loop over all subdirectories in the given folder
    for subdir in os.listdir(folder):
        subdir_path = os.path.join(folder, subdir)  # Get the path of the subdirectory

        # Check if the path is a directory
        if os.path.isdir(subdir_path):
            # Loop over all files in the subdirectory
            for filename in os.listdir(subdir_path):
                # Check if the file is an image
                if filename.endswith((".jpg", ".png", ".jpeg")):
                    img_path = os.path.join(subdir_path, filename)  # Get the path of the image

                    try:
                        # Load the image and resize it to the target size
                        img = load_img(img_path, target_size=(img_height, img_width))
                        # Convert the image to an array
                        img_array = img_to_array(img)
                        # Append the image array and label to the respective lists
                        images.append(img_array)
                        labels.append(label)
                    except:
                        # Print an error message if the file can't be read
                        print(f"Can't read the file {img_path}")

            # Increment the label counter for the next subdirectory
            label += 1

    # Return the images and labels as numpy arrays
    return np.array(images), np.array(labels)

# Paths for train, test and validation data
data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/319:
# Set the dimensions of the images to be loaded
img_width = 180
img_height = 180
18/320:
# Load the training images and labels
x_train, y_train = load_images_from_folder(data_train_path)

# Create an array of indices and shuffle them
indices = np.arange(x_train.shape[0])
np.random.shuffle(indices)

# Use the shuffled indices to shuffle the training images and labels
x_train = x_train[indices]
y_train = y_train[indices]

# Load the validation and test images and labels
x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
18/321:
# Create TensorFlow datasets for the training, validation, and test data
data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)
data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
18/322:
# Load a CSV file into a pandas DataFrame
data_csv = pd.read_csv("dataset.csv")

# Get the unique names from the 'names' column of the DataFrame and add the first 10 to a list
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/323:
# Print the list of names
data_cat
18/324:
# Create a 3x3 grid of subplots
plt.figure(figsize=(10,10))

# Take one batch of the training data
for img, labs in data_train.take(1):
    # Loop over the first 9 images in the batch
    for i in range(9):
        # Add a subplot for the current image
        plt.subplot(3,3,i+1)
        # Display the image
        plt.imshow(img[i].numpy().astype('uint8'))
        # Set the title of the subplot to the corresponding name
        plt.title(data_cat[labs[i]])
        # Remove the axes
        plt.axis('off')
18/325:
# Define a Sequential model for data augmentation
data_augmentation = keras.Sequential(
    [
        # Flip the images horizontally
        RandomFlip("horizontal"),
        # Rotate the images by 10%
        RandomRotation(0.1),
        # Zoom into the images by 10%
        RandomZoom(0.1),
    ]
)
# Define a Sequential model for the neural network
model = Sequential([
    # Apply the data augmentation
    data_augmentation,
    # Rescale the pixel values to be between 0 and 1
    layers.Rescaling(1./255),
    # Add a 2D convolution layer with 16 filters, a 3x3 kernel, same padding, and ReLU activation
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    # Add a max pooling layer with a 2x2 pool size
    layers.MaxPooling2D(),
    # Add another 2D convolution layer with 32 filters
    layers.Conv2D(32,3, padding='same',activation='relu'),
    # Add another max pooling layer
    layers.MaxPooling2D(),
    # Add a third 2D convolution layer with 64 filters
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    # Add a third max pooling layer
    layers.MaxPooling2D(),
    # Flatten the tensor output from the previous layer
    layers.Flatten(),
    # Add a dropout layer with a rate of 0.4
    layers.Dropout(0.4),
    # Add a dense layer with 128 units
    layers.Dense(128),
    # Add a final dense layer with a number of units equal to the number of categories
    layers.Dense(len(data_cat))
])
18/326:
# Compile the model with the Adam optimizer, sparse categorical crossentropy loss, and accuracy metric
model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
18/327:
# Set the number of epochs for training
epochs_size = 10

# Train the model on the training data and validate it on the validation data
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
18/328:
# Create a range of numbers up to the number of epochs for plotting
epochs_range = range(epochs_size)

# Create a new figure for the plots
plt.figure(figsize=(8,8))

# Add a subplot for the accuracy
plt.subplot(1,2,1)
# Plot the training and validation accuracy
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

# Add a subplot for the loss
plt.subplot(1,2,2)
# Plot the training and validation loss
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
18/329:
# Load an image for prediction
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/330:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/331:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/332:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/333:
# Save the model
model.save('Image_classify.keras')
18/334:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
18/335:
# Create TensorFlow datasets for the training, validation, and test data
data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(20)
data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(20)
data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(20)
18/336:
# Load a CSV file into a pandas DataFrame
data_csv = pd.read_csv("dataset.csv")

# Get the unique names from the 'names' column of the DataFrame and add the first 10 to a list
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/337:
# Print the list of names
data_cat
18/338:
# Create a 3x3 grid of subplots
plt.figure(figsize=(10,10))

# Take one batch of the training data
for img, labs in data_train.take(1):
    # Loop over the first 9 images in the batch
    for i in range(9):
        # Add a subplot for the current image
        plt.subplot(3,3,i+1)
        # Display the image
        plt.imshow(img[i].numpy().astype('uint8'))
        # Set the title of the subplot to the corresponding name
        plt.title(data_cat[labs[i]])
        # Remove the axes
        plt.axis('off')
18/339:
# Define a Sequential model for data augmentation
data_augmentation = keras.Sequential(
    [
        # Flip the images horizontally
        RandomFlip("horizontal"),
        # Rotate the images by 10%
        RandomRotation(0.1),
        # Zoom into the images by 10%
        RandomZoom(0.1),
    ]
)
# Define a Sequential model for the neural network
model = Sequential([
    # Apply the data augmentation
    data_augmentation,
    # Rescale the pixel values to be between 0 and 1
    layers.Rescaling(1./255),
    # Add a 2D convolution layer with 16 filters, a 3x3 kernel, same padding, and ReLU activation
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    # Add a max pooling layer with a 2x2 pool size
    layers.MaxPooling2D(),
    # Add another 2D convolution layer with 32 filters
    layers.Conv2D(32,3, padding='same',activation='relu'),
    # Add another max pooling layer
    layers.MaxPooling2D(),
    # Add a third 2D convolution layer with 64 filters
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    # Add a third max pooling layer
    layers.MaxPooling2D(),
    # Flatten the tensor output from the previous layer
    layers.Flatten(),
    # Add a dropout layer with a rate of 0.4
    layers.Dropout(0.4),
    # Add a dense layer with 128 units
    layers.Dense(128),
    # Add a final dense layer with a number of units equal to the number of categories
    layers.Dense(len(data_cat))
])
18/340:
# Compile the model with the Adam optimizer, sparse categorical crossentropy loss, and accuracy metric
model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
18/341:
# Set the number of epochs for training
epochs_size = 10

# Train the model on the training data and validate it on the validation data
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
18/342:
# Create a range of numbers up to the number of epochs for plotting
epochs_range = range(epochs_size)

# Create a new figure for the plots
plt.figure(figsize=(8,8))

# Add a subplot for the accuracy
plt.subplot(1,2,1)
# Plot the training and validation accuracy
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

# Add a subplot for the loss
plt.subplot(1,2,2)
# Plot the training and validation loss
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
18/343:
# Load an image for prediction
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/344:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/345:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/346:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/347:
# Load an image for prediction
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/348:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/349:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/350:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/351:
# Load an image for prediction
image = 'd3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/352:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/353:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/354:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/355:
# Load an image for prediction
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/356:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/357:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/358:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/359:
# Load an image for prediction
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/360:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/361:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/362:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/363:
# Load an image for prediction
image = 'e3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/364:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/365:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/366:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/367:
# Load an image for prediction
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/368:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/369:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/370:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/371:
# Load an image for prediction
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/372:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/373:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/374:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/375:
# Load an image for prediction
image = 'c1?.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/376:
# Load an image for prediction
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/377:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/378:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/379:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/380:
# Load an image for prediction
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/381:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/382:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/383:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/384:
# Load an image for prediction
image = 'c3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/385:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/386:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/387:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/388:
# Load an image for prediction
image = 'c4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/389:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/390:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/391:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/392:
# Create TensorFlow datasets for the training, validation, and test data
data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(64)
data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(64)
data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)
18/393:
# Load a CSV file into a pandas DataFrame
data_csv = pd.read_csv("dataset.csv")

# Get the unique names from the 'names' column of the DataFrame and add the first 10 to a list
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/394:
# Print the list of names
data_cat
18/395:
# Create a 3x3 grid of subplots
plt.figure(figsize=(10,10))

# Take one batch of the training data
for img, labs in data_train.take(1):
    # Loop over the first 9 images in the batch
    for i in range(9):
        # Add a subplot for the current image
        plt.subplot(3,3,i+1)
        # Display the image
        plt.imshow(img[i].numpy().astype('uint8'))
        # Set the title of the subplot to the corresponding name
        plt.title(data_cat[labs[i]])
        # Remove the axes
        plt.axis('off')
18/396:
# Define a Sequential model for data augmentation
data_augmentation = keras.Sequential(
    [
        # Flip the images horizontally
        RandomFlip("horizontal"),
        # Rotate the images by 10%
        RandomRotation(0.1),
        # Zoom into the images by 10%
        RandomZoom(0.1),
    ]
)
# Define a Sequential model for the neural network
model = Sequential([
    # Apply the data augmentation
    data_augmentation,
    # Rescale the pixel values to be between 0 and 1
    layers.Rescaling(1./255),
    # Add a 2D convolution layer with 16 filters, a 3x3 kernel, same padding, and ReLU activation
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    # Add a max pooling layer with a 2x2 pool size
    layers.MaxPooling2D(),
    # Add another 2D convolution layer with 32 filters
    layers.Conv2D(32,3, padding='same',activation='relu'),
    # Add another max pooling layer
    layers.MaxPooling2D(),
    # Add a third 2D convolution layer with 64 filters
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    # Add a third max pooling layer
    layers.MaxPooling2D(),
    # Flatten the tensor output from the previous layer
    layers.Flatten(),
    # Add a dropout layer with a rate of 0.4
    layers.Dropout(0.4),
    # Add a dense layer with 128 units
    layers.Dense(128),
    # Add a final dense layer with a number of units equal to the number of categories
    layers.Dense(len(data_cat))
])
18/397:
# Compile the model with the Adam optimizer, sparse categorical crossentropy loss, and accuracy metric
model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
18/398:
# Set the number of epochs for training
epochs_size = 10

# Train the model on the training data and validate it on the validation data
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
18/399:
# Create a range of numbers up to the number of epochs for plotting
epochs_range = range(epochs_size)

# Create a new figure for the plots
plt.figure(figsize=(8,8))

# Add a subplot for the accuracy
plt.subplot(1,2,1)
# Plot the training and validation accuracy
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

# Add a subplot for the loss
plt.subplot(1,2,2)
# Plot the training and validation loss
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
18/400:
# Load an image for prediction
image = 'c4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/401:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/402:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/403:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/404:
# Load an image for prediction
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/405:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/406:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/407:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/408:
# Load an image for prediction
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/409:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/410:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/411:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/412:
# Load an image for prediction
image = 'c3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/413:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/414:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/415:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/416:
# Load an image for prediction
image = 'd1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/417:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/418:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/419:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/420:
# Load an image for prediction
image = 'd2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/421:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/422:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/423:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/424:
# Load an image for prediction
image = 'd3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/425:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/426:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/427:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/428:
# Load an image for prediction
image = 'd4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/429:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/430:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/431:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/432:
# Load an image for prediction
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/433:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/434:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/435:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/436:
# Load an image for prediction
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/437:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/438:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/439:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/440:
# Load an image for prediction
image = 'e3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/441:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/442:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/443:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/444:
# Load an image for prediction
image = 'e4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/445:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/446:
# Load an image for prediction
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/447:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/448:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/449:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/450:
# Load an image for prediction
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/451:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/452:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/453:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/454:
#Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Sequential

import os
import shutil

from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
18/455:
def train_test_split():
    print("# Spliting Data #")
    data_csv = pd.read_csv("dataset.csv") ##Read classes from dataset.csv file

    root_dir = 'DataSet'
    classes_dir = []

    for name in data_csv['names'].unique()[:10]:
       classes_dir.append(name)

    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        # Create partitions of the data after shuffeling
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        # np.random.shuffle(allFileNames)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        # # Create Train / Val / Test folders
        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        # Copy-pasting images
        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
18/456:
# Function to load images from a given folder
def load_images_from_folder(folder):
    # Initialize empty lists to store images and labels
    images = []
    labels = []
    label = 0  # Initialize label counter

    # Loop over all subdirectories in the given folder
    for subdir in os.listdir(folder):
        subdir_path = os.path.join(folder, subdir)  # Get the path of the subdirectory

        # Check if the path is a directory
        if os.path.isdir(subdir_path):
            # Loop over all files in the subdirectory
            for filename in os.listdir(subdir_path):
                # Check if the file is an image
                if filename.endswith((".jpg", ".png", ".jpeg")):
                    img_path = os.path.join(subdir_path, filename)  # Get the path of the image

                    try:
                        # Load the image and resize it to the target size
                        img = load_img(img_path, target_size=(img_height, img_width))
                        # Convert the image to an array
                        img_array = img_to_array(img)
                        # Append the image array and label to the respective lists
                        images.append(img_array)
                        labels.append(label)
                    except:
                        # Print an error message if the file can't be read
                        print(f"Can't read the file {img_path}")

            # Increment the label counter for the next subdirectory
            label += 1

    # Return the images and labels as numpy arrays
    return np.array(images), np.array(labels)

# Paths for train, test and validation data
data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
18/457:
# Set the dimensions of the images to be loaded
img_width = 180
img_height = 180
18/458:
# Load the training images and labels
x_train, y_train = load_images_from_folder(data_train_path)

# Create an array of indices and shuffle them
indices = np.arange(x_train.shape[0])
np.random.shuffle(indices)

# Use the shuffled indices to shuffle the training images and labels
x_train = x_train[indices]
y_train = y_train[indices]

# Load the validation and test images and labels
x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
18/459:
# Create TensorFlow datasets for the training, validation, and test data
data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(64)
data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(64)
data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)
18/460:
# Load a CSV file into a pandas DataFrame
data_csv = pd.read_csv("dataset.csv")

# Get the unique names from the 'names' column of the DataFrame and add the first 10 to a list
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/461:
# Print the list of names
data_cat
18/462:
# Create a 3x3 grid of subplots
plt.figure(figsize=(10,10))

# Take one batch of the training data
for img, labs in data_train.take(1):
    # Loop over the first 9 images in the batch
    for i in range(9):
        # Add a subplot for the current image
        plt.subplot(3,3,i+1)
        # Display the image
        plt.imshow(img[i].numpy().astype('uint8'))
        # Set the title of the subplot to the corresponding name
        plt.title(data_cat[labs[i]])
        # Remove the axes
        plt.axis('off')
18/463:
# Define a Sequential model for data augmentation
data_augmentation = keras.Sequential(
    [
        # Flip the images horizontally
        RandomFlip("horizontal"),
        # Rotate the images by 10%
        RandomRotation(0.1),
        # Zoom into the images by 10%
        RandomZoom(0.1),
    ]
)
# Define a Sequential model for the neural network
model = Sequential([
    # Apply the data augmentation
    data_augmentation,
    # Rescale the pixel values to be between 0 and 1
    layers.Rescaling(1./255),
    # Add a 2D convolution layer with 16 filters, a 3x3 kernel, same padding, and ReLU activation
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    # Add a max pooling layer with a 2x2 pool size
    layers.MaxPooling2D(),
    # Add another 2D convolution layer with 32 filters
    layers.Conv2D(32,3, padding='same',activation='relu'),
    # Add another max pooling layer
    layers.MaxPooling2D(),
    # Add a third 2D convolution layer with 64 filters
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    # Add a third max pooling layer
    layers.MaxPooling2D(),
    # Flatten the tensor output from the previous layer
    layers.Flatten(),
    # Add a dropout layer with a rate of 0.4
    layers.Dropout(0.4),
    # Add a dense layer with 128 units
    layers.Dense(128),
    # Add a final dense layer with a number of units equal to the number of categories
    layers.Dense(len(data_cat))
])
18/464:
# Compile the model with the Adam optimizer, sparse categorical crossentropy loss, and accuracy metric
model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
18/465:
# Set the number of epochs for training
epochs_size = 10

# Train the model on the training data and validate it on the validation data
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
18/466:
# Create a range of numbers up to the number of epochs for plotting
epochs_range = range(epochs_size)

# Create a new figure for the plots
plt.figure(figsize=(8,8))

# Add a subplot for the accuracy
plt.subplot(1,2,1)
# Plot the training and validation accuracy
plt.plot(epochs_range,history.history['accuracy'],label = 'Training Accuracy')
plt.plot(epochs_range, history.history['val_accuracy'],label = 'Validation Accuracy')
plt.title('Accuracy')

# Add a subplot for the loss
plt.subplot(1,2,2)
# Plot the training and validation loss
plt.plot(epochs_range,history.history['loss'],label = 'Training Loss')
plt.plot(epochs_range, history.history['val_loss'],label = 'Validation Loss')
plt.title('Loss')
18/467:
# Load an image for prediction
image = 't2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/468:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/469:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/470:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/471:
# Load an image for prediction
image = 't1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/472:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/473:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/474:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/475:
# Load an image for prediction
image = 'c1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/476:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/477:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/478:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/479:
# Load an image for prediction
image = 'c2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/480:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/481:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/482:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/483:
# Load an image for prediction
image = 'c3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/484:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/485:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/486:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/487:
# Load an image for prediction
image = 'c4.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/488:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/489:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/490:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/491:
# Load an image for prediction
image = 'e1.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/492:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/493:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/494:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/495:
# Load an image for prediction
image = 'e2.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/496:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/497:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/498:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/499:
# Save the model
model.save('Image_classify.keras')
18/500:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
18/501:
# Load an image for prediction
image = 'tiger.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/502:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/503:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/504:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/505:
# Save the model
model.save('Image_classify.keras')
18/506:
# Load an image for prediction
image = 'elephant.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/507:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/508:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/509:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/510:
# Load an image for prediction
image = 'cat.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/511:
# Load an image for prediction
image = 'cat.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/512:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/513:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/514:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/515:
# Load an image for prediction
image = '3.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/516:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/517:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/518:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/519:
# Load an image for prediction
image = '113.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/520:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/521:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/522:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/523:
# Load an image for prediction
image = '40.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/524:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/525:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/526:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/527:
# Create TensorFlow datasets for the training, validation, and test data
data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)
data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
18/528:
# Load a CSV file into a pandas DataFrame
data_csv = pd.read_csv("dataset.csv")

# Get the unique names from the 'names' column of the DataFrame and add the first 10 to a list
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)
18/529:
# Print the list of names
data_cat
18/530:
# Create a 3x3 grid of subplots
plt.figure(figsize=(10,10))

# Take one batch of the training data
for img, labs in data_train.take(1):
    # Loop over the first 9 images in the batch
    for i in range(9):
        # Add a subplot for the current image
        plt.subplot(3,3,i+1)
        # Display the image
        plt.imshow(img[i].numpy().astype('uint8'))
        # Set the title of the subplot to the corresponding name
        plt.title(data_cat[labs[i]])
        # Remove the axes
        plt.axis('off')
18/531:
# Define a Sequential model for data augmentation
data_augmentation = keras.Sequential(
    [
        # Flip the images horizontally
        RandomFlip("horizontal"),
        # Rotate the images by 10%
        RandomRotation(0.1),
        # Zoom into the images by 10%
        RandomZoom(0.1),
    ]
)
# Define a Sequential model for the neural network
model = Sequential([
    # Apply the data augmentation
    data_augmentation,
    # Rescale the pixel values to be between 0 and 1
    layers.Rescaling(1./255),
    # Add a 2D convolution layer with 16 filters, a 3x3 kernel, same padding, and ReLU activation
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    # Add a max pooling layer with a 2x2 pool size
    layers.MaxPooling2D(),
    # Add another 2D convolution layer with 32 filters
    layers.Conv2D(32,3, padding='same',activation='relu'),
    # Add another max pooling layer
    layers.MaxPooling2D(),
    # Add a third 2D convolution layer with 64 filters
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    # Add a third max pooling layer
    layers.MaxPooling2D(),
    # Flatten the tensor output from the previous layer
    layers.Flatten(),
    # Add a dropout layer with a rate of 0.4
    layers.Dropout(0.4),
    # Add a dense layer with 128 units
    layers.Dense(128),
    # Add a final dense layer with a number of units equal to the number of categories
    layers.Dense(len(data_cat))
])
18/532:
# Compile the model with the Adam optimizer, sparse categorical crossentropy loss, and accuracy metric
model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
18/533:
# Set the number of epochs for training
epochs_size = 10

# Train the model on the training data and validate it on the validation data
history = model.fit(data_train, validation_data=data_val, epochs=epochs_size)
18/534:
# Load an image for prediction
image = '40.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/535:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/536:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/537:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/538:
# Load an image for prediction
image = 'cat.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/539:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/540:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/541:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/542:
# Load an image for prediction
image = 'elephant.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/543:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/544:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/545:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/546:
# Load an image for prediction
image = 'tiger.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/547:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/548:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/549:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/550:
# Load an image for prediction
image = 'dog.jpg'
image = tf.keras.utils.load_img(image, target_size=(img_height,img_width))

# Convert the image to an array
img_arr = tf.keras.utils.array_to_img(image)

# Expand the dimensions of the image array
img_bat=tf.expand_dims(img_arr,0)
18/551:
# Use the model to predict the class of the image
predict = model.predict(img_bat)

# Get the index of the class with the highest predicted probability
prediction = np.argmax(predict, axis=1)
18/552:
# Apply the softmax function to the model's prediction to get the probabilities
score = tf.nn.softmax(predict)
18/553:
# Print the predicted class and its probability
print('Animal in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)],np.max(score)*100))
18/554:
# Save the model
model.save('Image_classify.keras')
18/555:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
18/556:
# Get the true labels
true_labels = []
for images, labels in data_test:
    true_labels.extend(labels.numpy())

# Get the predicted labels
predicted_labels = []
for images, labels in data_test:
    predictions = model.predict(images)
    predicted_labels.extend(np.argmax(predictions, axis=1))
# Compute the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)
# Plot the confusion matrix
plot_confusion_matrix(conf_mat=cm,
                      colorbar=True,
                      show_absolute=True,
                      show_normed=True,
                      class_names=data_cat)
19/1:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import  load_model
import streamlit as st
import numpy as np 
import pandas as pd

st.header('Image Classification Model')
model = load_model('Image_classify.keras')

# Load a CSV file into a pandas DataFrame
data_csv = pd.read_csv("dataset.csv")

# Get the unique names from the 'names' column of the DataFrame and add the first 10 to a list
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)

img_height = 180
img_width = 180
image =st.text_input('Enter Image name','cat.jpg')

image_load = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image_load)
img_bat=tf.expand_dims(img_arr,0)

predict = model.predict(img_bat)

score = tf.nn.softmax(predict)
st.image(image, width=200)
st.write('Animal in image is ' + data_cat[np.argmax(score)])
st.write('With accuracy of ' + str(np.max(score)*100))
19/2:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import  load_model
import streamlit as st
import numpy as np 
import pandas as pd

st.header('Image Classification Model')
model = load_model('Image_classify.keras')

# Load a CSV file into a pandas DataFrame
data_csv = pd.read_csv("dataset.csv")

# Get the unique names from the 'names' column of the DataFrame and add the first 10 to a list
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)

img_height = 180
img_width = 180
image =st.text_input('Enter Image name','cat.jpg')

image_load = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image_load)
img_bat=tf.expand_dims(img_arr,0)

predict = model.predict(img_bat)

score = tf.nn.softmax(predict)
st.image(image, width=200)
st.write('Animal in image is ' + data_cat[np.argmax(score)])
st.write('With accuracy of ' + str(np.max(score)*100))
19/3:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import  load_model
import streamlit as st
import numpy as np 
import pandas as pd

st.header('Image Classification Model')
model = load_model('Image_classify.keras')

# Load a CSV file into a pandas DataFrame
data_csv = pd.read_csv("dataset.csv")

# Get the unique names from the 'names' column of the DataFrame and add the first 10 to a list
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)

img_height = 180
img_width = 180
image =st.text_input('Enter Image name','cat.jpg')

image_load = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image_load)
img_bat=tf.expand_dims(img_arr,0)

predict = model.predict(img_bat)

score = tf.nn.softmax(predict)
st.image(image, width=200)
st.write('Animal in image is ' + data_cat[np.argmax(score)])
st.write('With accuracy of ' + str(np.max(score)*100))
19/4:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import  load_model
import streamlit as st
import numpy as np 
import pandas as pd

st.header('Image Classification Model')
model = load_model('Image_classify.keras')

# Load a CSV file into a pandas DataFrame
data_csv = pd.read_csv("dataset.csv")

# Get the unique names from the 'names' column of the DataFrame and add the first 10 to a list
data_cat = []
for name in data_csv['names'].unique()[:10]:
   data_cat.append(name)

img_height = 180
img_width = 180
image =st.text_input('Enter Image name','cat.jpg')

image_load = tf.keras.utils.load_img(image, target_size=(img_height,img_width))
img_arr = tf.keras.utils.array_to_img(image_load)
img_bat=tf.expand_dims(img_arr,0)

predict = model.predict(img_bat)

score = tf.nn.softmax(predict)
st.image(image, width=200)
st.write('Animal in image is ' + data_cat[np.argmax(score)])
st.write('With accuracy of ' + str(np.max(score)*100))
20/1: data_cat = ['cat','dog','tiger','elephant']
20/2: data_cat
20/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Sequential

import os
import shutil

from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
20/4:
def train_test_split():
    print("# Spliting Data #")

    root_dir = 'DataSet'
    classes_dir = ['cat','dog','tiger','elephant']


    processed_dir = 'RawData'

    val_ratio = 0.20
    test_ratio = 0.20

    for cls in classes_dir:
        print("Class Name " + cls)
        src = processed_dir +"//" + cls  # Folder to copy images from

        allFileNames = os.listdir(src)
        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                                  [int(len(allFileNames) * (1 - (val_ratio + test_ratio))),
                                                                   int(len(allFileNames) * (1 - val_ratio)),
                                                                   ])

        train_FileNames = [src + '//' + name for name in train_FileNames.tolist()]
        val_FileNames = [src + '//' + name for name in val_FileNames.tolist()]
        test_FileNames = [src + '//' + name for name in test_FileNames.tolist()]

        print('Total images: '+ str(len(allFileNames)))
        print('Training: '+ str(len(train_FileNames)))
        print('Validation: '+  str(len(val_FileNames)))
        print('Testing: '+ str(len(test_FileNames)))

        for dir_type in ['train', 'val', 'test']:
            dir_path = os.path.join(root_dir, dir_type, cls)
            if os.path.exists(dir_path):
                shutil.rmtree(dir_path)  # Removes all the subdirectories!
            os.makedirs(dir_path)

        for name in train_FileNames:
            shutil.copy(name, root_dir + '/train//' + cls)

        for name in val_FileNames:
            shutil.copy(name, root_dir + '/val//' + cls)

        for name in test_FileNames:
            shutil.copy(name, root_dir + '/test//' + cls)

    print("# Data Spliting Completed #")

train_test_split()
20/5:
# Function to load images from a given folder
def load_images_from_folder(folder):
    # Initialize empty lists to store images and labels
    images = []
    labels = []
    label = 0  # Initialize label counter

    # Loop over all subdirectories in the given folder
    for subdir in os.listdir(folder):
        subdir_path = os.path.join(folder, subdir)  # Get the path of the subdirectory

        # Check if the path is a directory
        if os.path.isdir(subdir_path):
            # Loop over all files in the subdirectory
            for filename in os.listdir(subdir_path):
                # Check if the file is an image
                if filename.endswith((".jpg", ".png", ".jpeg")):
                    img_path = os.path.join(subdir_path, filename)  # Get the path of the image

                    try:
                        # Load the image and resize it to the target size
                        img = load_img(img_path, target_size=(img_height, img_width))
                        # Convert the image to an array
                        img_array = img_to_array(img)
                        # Append the image array and label to the respective lists
                        images.append(img_array)
                        labels.append(label)
                    except:
                        # Print an error message if the file can't be read
                        print(f"Can't read the file {img_path}")

            # Increment the label counter for the next subdirectory
            label += 1

    # Return the images and labels as numpy arrays
    return np.array(images), np.array(labels)

# Paths for train, test and validation data
data_train_path = 'Dataset/train'
data_test_path = 'Dataset/test'
data_val_path = 'Dataset/val'
20/6:
img_width = 180
img_height = 180
20/7:
x_train, y_train = load_images_from_folder(data_train_path)

indices = np.arange(x_train.shape[0])
np.random.shuffle(indices)

x_train = x_train[indices]
y_train = y_train[indices]

x_val, y_val = load_images_from_folder(data_val_path)
x_test, y_test = load_images_from_folder(data_test_path)
20/8:
data_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
data_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)
data_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)
20/9: data_cat = ['cat','dog','tiger','elephant']
20/10: data_cat
20/11:
plt.figure(figsize=(10,10))

for img, labs in data_train.take(1):
    for i in range(9):
        plt.subplot(3,3,i+1)
        plt.imshow(img[i].numpy().astype('uint8'))
        plt.title(data_cat[labs[i]])
        plt.axis('off')
20/12:
model = Sequential([
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3, padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dropout(0.4),
    layers.Dense(128),
    layers.Dense(len(data_cat))
])
20/13: model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
23/1:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
23/2:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
23/3:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
23/4:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
23/5:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
23/6:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = 'dataset/train'

label_types = os.listdir('dataset/train')
print (label_types)
23/7:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = 'dataset/train'

label_types = 'dataset/train'
print (label_types)
23/8:
rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/train' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))
    
# Build a dataframe        
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())
23/9:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
23/10:
rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/train' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))
    
# Build a dataframe        
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())
23/11:
df = train_df.loc[:,['video_name','tag']]
df
df.to_csv('train.csv')
23/12:
dataset_path = os.listdir('dataset/test')
print(dataset_path)

room_types = os.listdir('dataset/test')
print("Types of activities found: ", len(dataset_path))

rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/test' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/test' + '/' +item) + '/' + room))
    
# Build a dataframe        
test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(test_df.head())
print(test_df.tail())

df = test_df.loc[:,['video_name','tag']]
df
df.to_csv('test.csv')
23/13:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
23/14:
rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/train' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))
    
# Build a dataframe        
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())
23/15:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
23/16:
rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/train' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))
    
# Build a dataframe        
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())
23/17:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
23/18:
rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/train' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))
    
# Build a dataframe        
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())
23/19:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
23/20:
rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/train' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))
    
# Build a dataframe        
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())
23/21:
df = train_df.loc[:,['video_name','tag']]
df
df.to_csv('train.csv')
23/22:
dataset_path = os.listdir('dataset/test')
print(dataset_path)

room_types = os.listdir('dataset/test')
print("Types of activities found: ", len(dataset_path))

rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/test' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/test' + '/' +item) + '/' + room))
    
# Build a dataframe        
test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(test_df.head())
print(test_df.tail())

df = test_df.loc[:,['video_name','tag']]
df
df.to_csv('test.csv')
23/23:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
23/24:
rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/train' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))
    
# Build a dataframe        
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())
23/25:
df = train_df.loc[:,['video_name','tag']]
df
df.to_csv('train.csv')
23/26:
dataset_path = os.listdir('dataset/test')
print(dataset_path)

room_types = os.listdir('dataset/test')
print("Types of activities found: ", len(dataset_path))

rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/test' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/test' + '/' +item) + '/' + room))
    
# Build a dataframe        
test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(test_df.head())
print(test_df.tail())

df = test_df.loc[:,['video_name','tag']]
df
df.to_csv('test.csv')
23/27: #!pip install git+https://github.com/tensorflow/docs
23/28:
from tensorflow_docs.vis import embed
from tensorflow import keras
from imutils import paths

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os
23/29:
from tensorflow_docs.vis import embed
from tensorflow import keras
from imutils import paths

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os
23/30: !pip install git+https://github.com/tensorflow/docs
23/31: # !pip install git+https://github.com/tensorflow/docs
23/32:
from tensorflow_docs.vis import embed
from tensorflow import keras
from imutils import paths

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os
23/33:
from tensorflow_docs.vis import embed
from tensorflow import keras
from imutils import paths

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os
23/34:
from tensorflow_docs.vis import embed
from tensorflow import keras
from imutils import paths

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os
23/35:
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])
  except RuntimeError as e:
    print(e)
23/36:
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")

print(f"Total videos for training: {len(train_df)}")
print(f"Total videos for testing: {len(test_df)}")


train_df.sample(10)
23/37:
# The following two methods are taken from this tutorial:
# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub
IMG_SIZE = 224


def crop_center_square(frame):
    y, x = frame.shape[0:2]
    min_dim = min(y, x)
    start_x = (x // 2) - (min_dim // 2)
    start_y = (y // 2) - (min_dim // 2)
    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]


def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):
    cap = cv2.VideoCapture(path)
    frames = []
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame = crop_center_square(frame)
            frame = cv2.resize(frame, resize)
            frame = frame[:, :, [2, 1, 0]]
            frames.append(frame)

            if len(frames) == max_frames:
                break
    finally:
        cap.release()
    return np.array(frames)
23/38:
def build_feature_extractor():
    feature_extractor = keras.applications.InceptionV3(
        weights="imagenet",
        include_top=False,
        pooling="avg",
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
    )
    preprocess_input = keras.applications.inception_v3.preprocess_input

    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))
    preprocessed = preprocess_input(inputs)

    outputs = feature_extractor(preprocessed)
    return keras.Model(inputs, outputs, name="feature_extractor")


feature_extractor = build_feature_extractor()
23/39:
label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df["tag"]))
print(label_processor.get_vocabulary())

labels = train_df["tag"].values
labels = label_processor(labels[..., None]).numpy()
labels
23/40:
print(train_data[0].shape)
train_data[0]
23/41:
#print(train_data[0].shape)
#train_data[0]
23/42:
#Define hyperparameters

IMG_SIZE = 224
BATCH_SIZE = 64
EPOCHS = 100

MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048
23/43:
def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["video_name"].values.tolist()
    
    ##take all classlabels from train_df column named 'tag' and store in labels
    labels = df["tag"].values
    
    #convert classlabels to label encoding
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool") # 145,20
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32") #145,20,2048

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_df, "train")
test_data, test_labels = prepare_all_videos(test_df, "test")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")



print(f"train_labels in train set: {train_labels.shape}")

print(f"test_labels in train set: {test_labels.shape}")

# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters
23/44:
print(train_data[0].shape)
train_data[0]
23/45:
#Define hyperparameters

IMG_SIZE = 224
BATCH_SIZE = 64
EPOCHS = 100

MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048
23/46:
def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["video_name"].values.tolist()
    
    ##take all classlabels from train_df column named 'tag' and store in labels
    labels = df["tag"].values
    
    #convert classlabels to label encoding
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool") # 145,20
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32") #145,20,2048

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_df, "train")
test_data, test_labels = prepare_all_videos(test_df, "test")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")



print(f"train_labels in train set: {train_labels.shape}")

print(f"test_labels in train set: {test_labels.shape}")

# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters
23/47:
# Utility for our sequence model.
def get_sequence_model():
    class_vocab = label_processor.get_vocabulary()

    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))
    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype="bool")

    # Refer to the following tutorial to understand the significance of using `mask`:
    # https://keras.io/api/layers/recurrent_layers/gru/
    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)
    x = keras.layers.GRU(8)(x)
    x = keras.layers.Dropout(0.4)(x)
    x = keras.layers.Dense(8, activation="relu")(x)
    output = keras.layers.Dense(len(class_vocab), activation="softmax")(x)

    rnn_model = keras.Model([frame_features_input, mask_input], output)

    rnn_model.compile(
        loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"]
    )
    return rnn_model

EPOCHS = 30
# Utility for running experiments.
def run_experiment():
    filepath = "./tmp/video_classifier"
    checkpoint = keras.callbacks.ModelCheckpoint(
        filepath, save_weights_only=True, save_best_only=True, verbose=1
    )

    seq_model = get_sequence_model()
    history = seq_model.fit(
        [train_data[0], train_data[1]],
        train_labels,
        validation_split=0.3,
        epochs=EPOCHS,
        callbacks=[checkpoint],
    )

    seq_model.load_weights(filepath)
    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)
    print(f"Test accuracy: {round(accuracy * 100, 2)}%")

    return history, seq_model


_, sequence_model = run_experiment()
23/48:
def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["video_name"].values.tolist()
    
    ##take all classlabels from train_df column named 'tag' and store in labels
    labels = df["tag"].values
    
    #convert classlabels to label encoding
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool") # 145,20
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32") #145,20,2048

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_df, "")
test_data, test_labels = prepare_all_videos(test_df, "")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")



print(f"train_labels in train set: {train_labels.shape}")

print(f"test_labels in train set: {test_labels.shape}")

# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters
23/49:
def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["video_name"].values.tolist()
    
    ##take all classlabels from train_df column named 'tag' and store in labels
    labels = df["tag"].values
    
    #convert classlabels to label encoding
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool") # 145,20
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32") #145,20,2048

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_df, "")
test_data, test_labels = prepare_all_videos(test_df, "")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")



print(f"train_labels in train set: {train_labels.shape}")

print(f"test_labels in train set: {test_labels.shape}")

# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters
23/50:
# Utility for our sequence model.
def get_sequence_model():
    class_vocab = label_processor.get_vocabulary()

    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))
    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype="bool")

    # Refer to the following tutorial to understand the significance of using `mask`:
    # https://keras.io/api/layers/recurrent_layers/gru/
    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)
    x = keras.layers.GRU(8)(x)
    x = keras.layers.Dropout(0.4)(x)
    x = keras.layers.Dense(8, activation="relu")(x)
    output = keras.layers.Dense(len(class_vocab), activation="softmax")(x)

    rnn_model = keras.Model([frame_features_input, mask_input], output)

    rnn_model.compile(
        loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"]
    )
    return rnn_model

EPOCHS = 30
# Utility for running experiments.
def run_experiment():
    filepath = "./tmp/video_classifier"
    checkpoint = keras.callbacks.ModelCheckpoint(
        filepath, save_weights_only=True, save_best_only=True, verbose=1
    )

    seq_model = get_sequence_model()
    history = seq_model.fit(
        [train_data[0], train_data[1]],
        train_labels,
        validation_split=0.3,
        epochs=EPOCHS,
        callbacks=[checkpoint],
    )

    seq_model.load_weights(filepath)
    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)
    print(f"Test accuracy: {round(accuracy * 100, 2)}%")

    return history, seq_model


_, sequence_model = run_experiment()
24/1:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
24/2:
rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/train' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))
    
# Build a dataframe        
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())
24/3:
df = train_df.loc[:,['video_name','tag']]
df
df.to_csv('train.csv')
24/4:
dataset_path = os.listdir('dataset/test')
print(dataset_path)

room_types = os.listdir('dataset/test')
print("Types of activities found: ", len(dataset_path))

rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/test' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/test' + '/' +item) + '/' + room))
    
# Build a dataframe        
test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(test_df.head())
print(test_df.tail())

df = test_df.loc[:,['video_name','tag']]
df
df.to_csv('test.csv')
24/5: # !pip install git+https://github.com/tensorflow/docs
24/6:
from tensorflow_docs.vis import embed
from tensorflow import keras
from imutils import paths

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os
24/7:
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])
  except RuntimeError as e:
    print(e)
24/8:
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")

print(f"Total videos for training: {len(train_df)}")
print(f"Total videos for testing: {len(test_df)}")


train_df.sample(10)
24/9:
# The following two methods are taken from this tutorial:
# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub
IMG_SIZE = 224


def crop_center_square(frame):
    y, x = frame.shape[0:2]
    min_dim = min(y, x)
    start_x = (x // 2) - (min_dim // 2)
    start_y = (y // 2) - (min_dim // 2)
    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]


def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):
    cap = cv2.VideoCapture(path)
    frames = []
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame = crop_center_square(frame)
            frame = cv2.resize(frame, resize)
            frame = frame[:, :, [2, 1, 0]]
            frames.append(frame)

            if len(frames) == max_frames:
                break
    finally:
        cap.release()
    return np.array(frames)
24/10:
def build_feature_extractor():
    feature_extractor = keras.applications.InceptionV3(
        weights="imagenet",
        include_top=False,
        pooling="avg",
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
    )
    preprocess_input = keras.applications.inception_v3.preprocess_input

    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))
    preprocessed = preprocess_input(inputs)

    outputs = feature_extractor(preprocessed)
    return keras.Model(inputs, outputs, name="feature_extractor")


feature_extractor = build_feature_extractor()
24/11:
label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df["tag"]))
print(label_processor.get_vocabulary())

labels = train_df["tag"].values
labels = label_processor(labels[..., None]).numpy()
labels
24/12:
print(train_data[0].shape)
train_data[0]
24/13:
#Define hyperparameters

IMG_SIZE = 224
BATCH_SIZE = 64
EPOCHS = 100

MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048
24/14:
print(train_data[0].shape)
train_data[0]
24/15:
#Define hyperparameters

IMG_SIZE = 224
BATCH_SIZE = 64
EPOCHS = 100

MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048
24/16:
def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["video_name"].values.tolist()
    
    ##take all classlabels from train_df column named 'tag' and store in labels
    labels = df["tag"].values
    
    #convert classlabels to label encoding
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool") # 145,20
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32") #145,20,2048

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_df, "")
test_data, test_labels = prepare_all_videos(test_df, "")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")



print(f"train_labels in train set: {train_labels.shape}")

print(f"test_labels in train set: {test_labels.shape}")

# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters
24/17:
def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["video_name"].values.tolist()
    
    ##take all classlabels from train_df column named 'tag' and store in labels
    labels = df["tag"].values
    
    #convert classlabels to label encoding
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool") # 145,20
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32") #145,20,2048

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_df, "")
test_data, test_labels = prepare_all_videos(test_df, "")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")



print(f"train_labels in train set: {train_labels.shape}")

print(f"test_labels in train set: {test_labels.shape}")

# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters
24/18:
# Utility for our sequence model.
def get_sequence_model():
    class_vocab = label_processor.get_vocabulary()

    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))
    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype="bool")

    # Refer to the following tutorial to understand the significance of using `mask`:
    # https://keras.io/api/layers/recurrent_layers/gru/
    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)
    x = keras.layers.GRU(8)(x)
    x = keras.layers.Dropout(0.4)(x)
    x = keras.layers.Dense(8, activation="relu")(x)
    output = keras.layers.Dense(len(class_vocab), activation="softmax")(x)

    rnn_model = keras.Model([frame_features_input, mask_input], output)

    rnn_model.compile(
        loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"]
    )
    return rnn_model

EPOCHS = 30
# Utility for running experiments.
def run_experiment():
    filepath = "./tmp/video_classifier"
    checkpoint = keras.callbacks.ModelCheckpoint(
        filepath, save_weights_only=True, save_best_only=True, verbose=1
    )

    seq_model = get_sequence_model()
    history = seq_model.fit(
        [train_data[0], train_data[1]],
        train_labels,
        validation_split=0.3,
        epochs=EPOCHS,
        callbacks=[checkpoint],
    )

    seq_model.load_weights(filepath)
    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)
    print(f"Test accuracy: {round(accuracy * 100, 2)}%")

    return history, seq_model


_, sequence_model = run_experiment()
24/19:
def prepare_single_video(frames):
    frames = frames[None, ...]
    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32")

    for i, batch in enumerate(frames):
        video_length = batch.shape[0]
        length = min(MAX_SEQ_LENGTH, video_length)
        for j in range(length):
            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])
        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

    return frame_features, frame_mask


def sequence_prediction(path):
    class_vocab = label_processor.get_vocabulary()

    frames = load_video(os.path.join("test", path))
    frame_features, frame_mask = prepare_single_video(frames)
    probabilities = sequence_model.predict([frame_features, frame_mask])[0]

    for i in np.argsort(probabilities)[::-1]:
        print(f"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%")
    return frames

test_video = np.random.choice(test_df["video_name"].values.tolist())
print(f"Test video path: {test_video}")

test_frames = sequence_prediction(test_video)
24/20:
from IPython.display import HTML

HTML("""
    <video alt="test" width="520" height="440" controls>
        <source src="dataset/test/dancing/dancing (23.mp4" type="video/mp4" style="height:300px;width:300px">
    </video>
""")
24/21:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
24/22:
rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/train' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))
    
# Build a dataframe        
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())
24/23:
df = train_df.loc[:,['video_name','tag']]
df
df.to_csv('train.csv')
24/24:
dataset_path = os.listdir('dataset/test')
print(dataset_path)

room_types = os.listdir('dataset/test')
print("Types of activities found: ", len(dataset_path))

rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/test' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/test' + '/' +item) + '/' + room))
    
# Build a dataframe        
test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(test_df.head())
print(test_df.tail())

df = test_df.loc[:,['video_name','tag']]
df
df.to_csv('test.csv')
24/25: # !pip install git+https://github.com/tensorflow/docs
24/26:
from tensorflow_docs.vis import embed
from tensorflow import keras
from imutils import paths

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os
24/27:
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])
  except RuntimeError as e:
    print(e)
24/28:
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")

print(f"Total videos for training: {len(train_df)}")
print(f"Total videos for testing: {len(test_df)}")


train_df.sample(10)
24/29:
# The following two methods are taken from this tutorial:
# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub
IMG_SIZE = 224


def crop_center_square(frame):
    y, x = frame.shape[0:2]
    min_dim = min(y, x)
    start_x = (x // 2) - (min_dim // 2)
    start_y = (y // 2) - (min_dim // 2)
    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]


def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):
    cap = cv2.VideoCapture(path)
    frames = []
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame = crop_center_square(frame)
            frame = cv2.resize(frame, resize)
            frame = frame[:, :, [2, 1, 0]]
            frames.append(frame)

            if len(frames) == max_frames:
                break
    finally:
        cap.release()
    return np.array(frames)
24/30:
def build_feature_extractor():
    feature_extractor = keras.applications.InceptionV3(
        weights="imagenet",
        include_top=False,
        pooling="avg",
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
    )
    preprocess_input = keras.applications.inception_v3.preprocess_input

    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))
    preprocessed = preprocess_input(inputs)

    outputs = feature_extractor(preprocessed)
    return keras.Model(inputs, outputs, name="feature_extractor")


feature_extractor = build_feature_extractor()
24/31:
label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df["tag"]))
print(label_processor.get_vocabulary())

labels = train_df["tag"].values
labels = label_processor(labels[..., None]).numpy()
labels
24/32:
print(train_data[0].shape)
train_data[0]
24/33:
#Define hyperparameters

IMG_SIZE = 224
BATCH_SIZE = 64
EPOCHS = 100

MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048
24/34:
def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["video_name"].values.tolist()
    
    ##take all classlabels from train_df column named 'tag' and store in labels
    labels = df["tag"].values
    
    #convert classlabels to label encoding
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool") # 145,20
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32") #145,20,2048

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_df, "")
test_data, test_labels = prepare_all_videos(test_df, "")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")



print(f"train_labels in train set: {train_labels.shape}")

print(f"test_labels in train set: {test_labels.shape}")

# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters
24/35:
# Utility for our sequence model.
def get_sequence_model():
    class_vocab = label_processor.get_vocabulary()

    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))
    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype="bool")

    # Refer to the following tutorial to understand the significance of using `mask`:
    # https://keras.io/api/layers/recurrent_layers/gru/
    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)
    x = keras.layers.GRU(8)(x)
    x = keras.layers.Dropout(0.4)(x)
    x = keras.layers.Dense(8, activation="relu")(x)
    output = keras.layers.Dense(len(class_vocab), activation="softmax")(x)

    rnn_model = keras.Model([frame_features_input, mask_input], output)

    rnn_model.compile(
        loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"]
    )
    return rnn_model

EPOCHS = 30
# Utility for running experiments.
def run_experiment():
    filepath = "./tmp/video_classifier"
    checkpoint = keras.callbacks.ModelCheckpoint(
        filepath, save_weights_only=True, save_best_only=True, verbose=1
    )

    seq_model = get_sequence_model()
    history = seq_model.fit(
        [train_data[0], train_data[1]],
        train_labels,
        validation_split=0.3,
        epochs=EPOCHS,
        callbacks=[checkpoint],
    )

    seq_model.load_weights(filepath)
    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)
    print(f"Test accuracy: {round(accuracy * 100, 2)}%")

    return history, seq_model


_, sequence_model = run_experiment()
25/1:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
25/2:
rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/train' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))
    
# Build a dataframe        
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())
25/3:
df = train_df.loc[:,['video_name','tag']]
df
df.to_csv('train.csv')
25/4:
dataset_path = os.listdir('dataset/test')
print(dataset_path)

room_types = os.listdir('dataset/test')
print("Types of activities found: ", len(dataset_path))

rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/test' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/test' + '/' +item) + '/' + room))
    
# Build a dataframe        
test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(test_df.head())
print(test_df.tail())

df = test_df.loc[:,['video_name','tag']]
df
df.to_csv('test.csv')
25/5: # !pip install git+https://github.com/tensorflow/docs
25/6:
from tensorflow_docs.vis import embed
from tensorflow import keras
from imutils import paths

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os
25/7:
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])
  except RuntimeError as e:
    print(e)
25/8:
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")

print(f"Total videos for training: {len(train_df)}")
print(f"Total videos for testing: {len(test_df)}")


train_df.sample(10)
25/9:
# The following two methods are taken from this tutorial:
# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub
IMG_SIZE = 224


def crop_center_square(frame):
    y, x = frame.shape[0:2]
    min_dim = min(y, x)
    start_x = (x // 2) - (min_dim // 2)
    start_y = (y // 2) - (min_dim // 2)
    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]


def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):
    cap = cv2.VideoCapture(path)
    frames = []
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame = crop_center_square(frame)
            frame = cv2.resize(frame, resize)
            frame = frame[:, :, [2, 1, 0]]
            frames.append(frame)

            if len(frames) == max_frames:
                break
    finally:
        cap.release()
    return np.array(frames)
25/10:
def build_feature_extractor():
    feature_extractor = keras.applications.InceptionV3(
        weights="imagenet",
        include_top=False,
        pooling="avg",
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
    )
    preprocess_input = keras.applications.inception_v3.preprocess_input

    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))
    preprocessed = preprocess_input(inputs)

    outputs = feature_extractor(preprocessed)
    return keras.Model(inputs, outputs, name="feature_extractor")


feature_extractor = build_feature_extractor()
25/11:
label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df["tag"]))
print(label_processor.get_vocabulary())

labels = train_df["tag"].values
labels = label_processor(labels[..., None]).numpy()
labels
25/12:
print(train_data[0].shape)
train_data[0]
25/13:
print(train_data[0].shape)
train_data[0]
25/14:
#Define hyperparameters

IMG_SIZE = 224
BATCH_SIZE = 64
EPOCHS = 100

MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048
25/15:
#Define hyperparameters

IMG_SIZE = 224
BATCH_SIZE = 64
EPOCHS = 2

MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048
25/16:
print(train_data[0].shape)
train_data[0]
25/17:
#Define hyperparameters

IMG_SIZE = 224
BATCH_SIZE = 64
EPOCHS = 2

MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048
25/18:
def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["video_name"].values.tolist()
    
    ##take all classlabels from train_df column named 'tag' and store in labels
    labels = df["tag"].values
    
    #convert classlabels to label encoding
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool") # 145,20
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32") #145,20,2048

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_df, "")
test_data, test_labels = prepare_all_videos(test_df, "")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")



print(f"train_labels in train set: {train_labels.shape}")

print(f"test_labels in train set: {test_labels.shape}")

# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters
26/1:

import os
import pathlib
import sys

app = r'/Users/prabeshregmi/Downloads/Video-Classifier-Using-CNN-and-RNN-main/video_classifier_working.ipynb'
os.chdir(str(pathlib.Path(app).parent))
sys.path = [os.getcwd()] + sys.path[1:]

from panel.io.jupyter_executor import PanelExecutor
executor = PanelExecutor(app, 'eyJzZXNzaW9uX2lkIjogIm9sUk1FeGE0a01jTVhnUWpBSFZpR2dGWE9aSXRUdTFzT2g2em1MSURURkhQIiwgInNlc3Npb25fZXhwaXJ5IjogMTcxNjQ1ODU2NiwgIl9fYmtfX3psaWJfIjogImVOck5VMXRQMnpBVV9pdFdub0ExYlp6RXVSVDFvVnZIWlJEUUtCMVRoWVJjNXlReFRaeVF1RlVINGJfUGNWdllKdkctUEVULXpzM25mT2Z6aTBIcmRGV0FrSTB4UkMtdlBXUmtRR09vTlRUT3lrYXFnNUdYak9hWkFzTkFmWWFLbWdJelQwQ3l6Snh5Q1YyTUtBVjBuaTlsdWVUYXNtcWdGclFBOHkzZDdOSkg5NGJkNHFIVlltdUlmZXk1SkxBOTB0ck84SU9FMW5XSDEzZno0bW95SnRIZHQtS3FtTGxYay05NF9wZ1Y4OU9aUFpfY0ZORnRaRjFQVHBiUll6UnFBOS16SEJZU080NDlnaGZNdzdick9nNzFmUmNjTzhBV3MwaG9XVUZzUS1JdEdBQnhReGY3eWNJUHFlZGg3OTQ0UmctYnBrNUdkaHRqa21Ed29hVnhTRU5NWXlmeE1TUUxWY2dQblNEQWprTkk0TVpocTJaeEE5dkJoR3hwRUFLWTVLWG9xRmdDVkNiTi1WcFROS3ZTV3BGc25vc0cyS29HOHdhZVZ0RG9IUmo0YjNxak10N1NTOWM4cFZMbmp4bURTaTlHd2tZT01sbmtQVnBWT1dlMHUzQ3c2U3lmTnY5YWlfejRhV1Qxdzk3UjRFaWY5Q1puaW5Sem5Db0pkQVdqOHBubk9SMlF2b1VPSXNxNGtHV1RIYU56SVNGSHlvQ3VwLWdud3RZREpnXy1JUnFyQy1BT0ZoZGNEanlMOUhFZkUzUndjWFliWGZaUXpwZUFUb0V0eTBQMFEybXE2d0w3ZlFkTmFVSnJfcGJ3UHBONVNVVzZvcWtlR1lSNS1ybW5fclBwcm5FUTczMl9VelNCclVwNVVpdnhfRkhzcTJCbHpFWGFPZE5uWHZWUURFbmVrZGdKbldtaGJvWC1nZkswRHpZVnIzV2MwZFd1cU16MloxWVd4WTY0RHNabFFibllvNEp1ek4wa0hkeXVlbzh5S2F0UzVMXzJlTDJsNXkxWTlkTHNIcFp1VnN2eFAybm45ZlUzdlBwQm5BIn0', '/panel-preview')
executor.render()
27/1:

import os
import pathlib
import sys

app = r'/Users/prabeshregmi/Downloads/Video-Classifier-Using-CNN-and-RNN-main/video_classifier_working.ipynb'
os.chdir(str(pathlib.Path(app).parent))
sys.path = [os.getcwd()] + sys.path[1:]

from panel.io.jupyter_executor import PanelExecutor
executor = PanelExecutor(app, 'eyJzZXNzaW9uX2lkIjogIkxIQUFqMGpHaFpETUFUbzA0WHdKVjdkblowM3RjVFJkaEg0RENGREZoOXFTIiwgInNlc3Npb25fZXhwaXJ5IjogMTcxNjQ1ODU3NCwgIl9fYmtfX3psaWJfIjogImVOck5VMTFQMnpBVV9TdFdub0ExclozVVRWTFVCN1l5S05CTVcyR01DUW01emszaUpiRkQ0cUlDNGI4dmNWdllKdkctUEVRLTk4djNubnY4YkxFcVdSVWdkVzJOMGZOTEQxa3BzQWdxQTYxVFZldjJZT1dLc3p4dHdkaHZQNnVOV2dDM1A0UG1xYjBRR3JvWXFTUjBuazlLWmNKWVZqVlVraFZndjZiYlhmcmsxbklhTXNZTndXUGlrZEdRLW83bk5vNDdmaWVoR1E3SDgySkd3LXZ3VnpqbE9Kd21KSnhtLU9mMVdSWTZaOW5ONVRHZFg5NE13LUlyRGs5bWs0Wnc1dkxZaXlLUE1oWTR3R0p3Z3NBbk9NWlJUR0k4aW9MQUdfa2VFSS1OX0NYMkthZkxrUjlnN0ZIZmpkbXRkWWp1MW5VVlQ1d21JalFtNEVIRG9vQUZoRVZ1N0JHSWwtQTZYdUQ2UG5GZFN2MWhGRFR0TEVQZmNRbWxHeHFrQks2RmtoMFZHVUJwczF3OEdJcXV5cVJxU2JabnNnYS1xc0QtQnZjcnFNME9MUEkzdlhNVmJlaGxEeUpoMnVRZmNRNmxXWXlHdFI2a3VzaDdyQ3h6d1ZsMzRXRGRXVDZzXzdVVy1lSDlCUGVEM3NIZ3dKek1KcTlhMHUyanBKVkFWM0N1bmtTZXN3SHRZN1EzWjF4SXJlcjBFTTJraGh5MUJ2UmxnWDRnZ3U4SXZmUDIwVkY3QVZ6RDhsem93UWpUUHVrVGl2Yk9UeV9uRnoyVWl3elFDZkJNN2FQdnJhYTZMb2pYZDlHQ3hhd1Nyd2x2TTlrWFRDWXJscGlSUWRvbkgzdnRfMnF4YlJ6a1c5OXZGRTFobzFJUlY2MTRfaWgyTExtS2hFdzZaX0lreWg2S0lNNDdFanVoY3lQVWpkRGZVWjd4d2JvVWxZbXp1dG9sMC1udXpGVlJiSW5yWUtRS0p1UU9GV3h0YnlmcDRHYlZPNVJxWFNxWlAtN3d3NGFlMS1DMmwzcjdzRXl6Um83X1NUc3ZMNzhCMmJaQ1FBIn0', '/panel-preview')
executor.render()
28/1:
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import os

dataset_path = os.listdir('dataset/train')

label_types = os.listdir('dataset/train')
print (label_types)
28/2:
rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/train' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))
    
# Build a dataframe        
train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(train_df.head())
print(train_df.tail())
28/3:
df = train_df.loc[:,['video_name','tag']]
df
df.to_csv('train.csv')
28/4:
dataset_path = os.listdir('dataset/test')
print(dataset_path)

room_types = os.listdir('dataset/test')
print("Types of activities found: ", len(dataset_path))

rooms = []

for item in dataset_path:
 # Get all the file names
 all_rooms = os.listdir('dataset/test' + '/' +item)

 # Add them to the list
 for room in all_rooms:
    rooms.append((item, str('dataset/test' + '/' +item) + '/' + room))
    
# Build a dataframe        
test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])
print(test_df.head())
print(test_df.tail())

df = test_df.loc[:,['video_name','tag']]
df
df.to_csv('test.csv')
28/5: # !pip install git+https://github.com/tensorflow/docs
28/6:
from tensorflow_docs.vis import embed
from tensorflow import keras
from imutils import paths

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os
28/7:
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])
  except RuntimeError as e:
    print(e)
28/8:
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")

print(f"Total videos for training: {len(train_df)}")
print(f"Total videos for testing: {len(test_df)}")


train_df.sample(10)
28/9:
# The following two methods are taken from this tutorial:
# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub
IMG_SIZE = 224


def crop_center_square(frame):
    y, x = frame.shape[0:2]
    min_dim = min(y, x)
    start_x = (x // 2) - (min_dim // 2)
    start_y = (y // 2) - (min_dim // 2)
    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]


def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):
    cap = cv2.VideoCapture(path)
    frames = []
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame = crop_center_square(frame)
            frame = cv2.resize(frame, resize)
            frame = frame[:, :, [2, 1, 0]]
            frames.append(frame)

            if len(frames) == max_frames:
                break
    finally:
        cap.release()
    return np.array(frames)
28/10:
def build_feature_extractor():
    feature_extractor = keras.applications.InceptionV3(
        weights="imagenet",
        include_top=False,
        pooling="avg",
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
    )
    preprocess_input = keras.applications.inception_v3.preprocess_input

    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))
    preprocessed = preprocess_input(inputs)

    outputs = feature_extractor(preprocessed)
    return keras.Model(inputs, outputs, name="feature_extractor")


feature_extractor = build_feature_extractor()
28/11:
label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df["tag"]))
print(label_processor.get_vocabulary())

labels = train_df["tag"].values
labels = label_processor(labels[..., None]).numpy()
labels
28/12:
print(train_data[0].shape)
train_data[0]
28/13:
def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["video_name"].values.tolist()
    
    ##take all classlabels from train_df column named 'tag' and store in labels
    labels = df["tag"].values
    
    #convert classlabels to label encoding
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool") # 145,20
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32") #145,20,2048

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_df, "")
test_data, test_labels = prepare_all_videos(test_df, "")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")



print(f"train_labels in train set: {train_labels.shape}")

print(f"test_labels in train set: {test_labels.shape}")

# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters
28/14:
print(train_data[0].shape)
train_data[0]
28/15:
#Define hyperparameters

IMG_SIZE = 224
BATCH_SIZE = 64
EPOCHS = 2

MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048
28/16:
def prepare_all_videos(df, root_dir):
    num_samples = len(df)
    video_paths = df["video_name"].values.tolist()
    
    ##take all classlabels from train_df column named 'tag' and store in labels
    labels = df["tag"].values
    
    #convert classlabels to label encoding
    labels = label_processor(labels[..., None]).numpy()

    # `frame_masks` and `frame_features` are what we will feed to our sequence model.
    # `frame_masks` will contain a bunch of booleans denoting if a timestep is
    # masked with padding or not.
    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype="bool") # 145,20
    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32") #145,20,2048

    # For each video.
    for idx, path in enumerate(video_paths):
        # Gather all its frames and add a batch dimension.
        frames = load_video(os.path.join(root_dir, path))
        frames = frames[None, ...]

        # Initialize placeholders to store the masks and features of the current video.
        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")
        temp_frame_features = np.zeros(
            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32"
        )

        # Extract features from the frames of the current video.
        for i, batch in enumerate(frames):
            video_length = batch.shape[0]
            length = min(MAX_SEQ_LENGTH, video_length)
            for j in range(length):
                temp_frame_features[i, j, :] = feature_extractor.predict(
                    batch[None, j, :]
                )
            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked

        frame_features[idx,] = temp_frame_features.squeeze()
        frame_masks[idx,] = temp_frame_mask.squeeze()

    return (frame_features, frame_masks), labels


train_data, train_labels = prepare_all_videos(train_df, "")
test_data, test_labels = prepare_all_videos(test_df, "")

print(f"Frame features in train set: {train_data[0].shape}")
print(f"Frame masks in train set: {train_data[1].shape}")



print(f"train_labels in train set: {train_labels.shape}")

print(f"test_labels in train set: {test_labels.shape}")

# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters
29/1:

import os
import pathlib
import sys

app = r'/Users/prabeshregmi/Downloads/Action-Recognition-in-the-Wild-master/Data Preparation/Frame and Optical Flow Generation.py'
os.chdir(str(pathlib.Path(app).parent))
sys.path = [os.getcwd()] + sys.path[1:]

from panel.io.jupyter_executor import PanelExecutor
executor = PanelExecutor(app, 'eyJzZXNzaW9uX2lkIjogIkRSWWg3bk5zVVBnSnJXVjFhbWJMVDRHTDlNRENSRTY3RnFpN2tNUllDNTdXIiwgInNlc3Npb25fZXhwaXJ5IjogMTcxNjQ2MjQzOSwgIl9fYmtfX3psaWJfIjogImVOck5WTnR5bXpBUV9SV0dwNlkxdGk0SWtETi1TTm8wZFZ2b05IYWF4cE9aakN3V3JCZ0V3VGh4WFBMdkZUaVh0dE84NTRYUnJuWlhaM2ZQNFpjdHFuU2RnNjVYOXRENmRkLXo3QVdJR0tyT3REOFZxOW9jN0t5UUlsc1lZeGdFQWJkTjFBU2s4eEZxdVhBbXFvWTJSaGNhMnB2M1JiRlVuV2U5Z2txTEhKeW5kS2ROSDEzWXBNRkQxR0EweEQ3MlhJOWdHalNFRGw5SWFGeDNHRTNIT0pvdTc4S3I3M1IyZFlTakQ3RTZKMk1Va1ZNMG14NWVSZVR6SXR5R2Q3T3o4YWpCYmdBSTVweGpDWVM2WEREa1NRRUI5d01PSHZjSklTNGpFZ2NNQ1E4WThqRUNrcmpZNXk2WU9INWg3MXZfeHhMOEJ6eWk5R1h3d1E2OHZJM3k3N2ZuMl9FMjJvWXNPZzVKZEh5RVpsZW5KSnllWkdFLVUtSFotRGFjcHFQR1N6enBFd2ItbkFhbURVRXdvamltYnN3NEp3RkJuRk9PeU53MDR4RktBTS1SZEFXYnV3bVktRGltTGZqTHphcEtScVNKTVVzdy1OQ0ltQXVPUlV3VEgwTXlCMHA4YnNJeHBZd0Zic3diMDRzYkVJb1oyLTFRYTVDMUtuUzd4eVZBNlloTTNYVDdQUzNUeWpERUdlc1Z5SFVGemdsY3IySFZFY2pHZjNNakxPSWROOFNOU2tYZDVSOUlDV1hIcWhvMjlXQlI1MWxQbEdXbXBHZ2ZIR3hhejd2TnY5NDgyNzhlb1Q3dnZSMjg3VTVCQjhZTTNUbElEWF9iZ21HeFZWa21CcXlQckRlaGtFclh4V3F4YjQxMURabGxITmEzaWZYVHd1Z1NzMHRfenpvd0Q4QVp6TC1vZXVBaDFzZDl6S3czWHo1Tnc2ODlLMU5Mc0k1Qkxvczk2NGNSUklzQy0zMXFUVVFpS3ZXVThOeVQ4MVhvZEMzU3JtWFF6dkZoejN4UEp3X0FRVF9qZmg3UkI5aEpUQ1dWSWM4ZnhZNjBMR0tsMF9ZeTNhcXlaOFdRWk8wUVc1WEtUbVU3bGI0Z20tNE9OcVdxdWppN3JWMktldkY0bGtXZVB3eXVOZU1pRjBvX1dybllPQS1kdE9adTFZX1dvcTdMUW1kM2pfYk5ianhQd1FiTDZ1R3YwSUY5UVJxdkIyQ25sMWNDNV83LU40NTRxQ0UifQ', '/panel-preview')
executor.render()
30/1:

import os
import pathlib
import sys

app = r'/Users/prabeshregmi/Downloads/Action-Recognition-in-the-Wild-master/Data Preparation/Frame and Optical Flow Generation.py'
os.chdir(str(pathlib.Path(app).parent))
sys.path = [os.getcwd()] + sys.path[1:]

from panel.io.jupyter_executor import PanelExecutor
executor = PanelExecutor(app, 'eyJzZXNzaW9uX2lkIjogIlppeHVvaDhENjVUT1RHVFZhVE5sUWVPaXVkVjlwcGNTNUVIQ1ZiZEVhUWlWIiwgInNlc3Npb25fZXhwaXJ5IjogMTcxNjQ2MjQ0MywgIl9fYmtfX3psaWJfIjogImVOck5WTnRTbzBBUV9SV0tKM1ZETWhmSVFLdzg2SHBMS2RuU3hIV1Rzc29haG9hTWdRRUpNZG1JXzc0RDhiSzdwZS0tVU5NOTNUMm51OF9oeWVSRnZFeEJsUXV6Wnp3OXR3eHpCanlFb2pITnMyeFI2b09aWklJbk0yMzBYTmYxVEIwMUFtR2RRQ2xtMWtpV1VNZW9URUY5OHozTDVyTHhMQmRRS0o2QzlaWnUxZW45VzVOVXVJY3FqSHFZNGE3ZEpkZ21GYUc5VHhJcTItNU4wcFBaTkQzRzBfdkRaSmo2WkhoME1wOGV4YXZoNXNDZWpBZGtjajlmVGU0SEdfX21zbDloU2tJN0FJWUZCbkFEeGhCbkljS01VWVFpbHdvSW5DNWpqZ2djT3lBZ09CRFFFQ2gxQXBjakRPR3R1Vzk4ak1YOUFEeWk5SFB3YmcxLU9CYXJZWHE1bW13R20tSEdkNGFudW9IVFl6Uzl2eWItLUNyeDA2bjBid1lyZnh6M3EyN1VGWXc0d0FMcXVvQTR3WWppa05xaDQzbkVKY2p6cUlkSTRIcXNTeWdCSENCaGN5ZXdJOUR4WVVocjhIZnJSUkgxU1JWaUo4TEFvT0toeHozTVF4b3hERkVBbERCUGgyUGRzLVBhb1ZmcFhteVhVT3c0MngwcUJhS1VtYXIzT0FmSUxaN0l4MmFfMTNsY2FJWllBN1VBc1N6QXVvS0hKU3dhQXBuNFgyNzRXYmpsQm4tVU1TLWJfQU1oSUc5WVZjSzY3TXpLTkdueFBFLWs0UFdEblhYdC1iYi0zNXNtLXc5OTFQWmFlNTI5NXVRMllQVFFyWU5ZODdjdTZHY2JtU1M4NDdTUnNlTnpJVldaTFdiN3hrQ1ZrQmphWWZ3WUdiOE1qTzZ3YzhkMmpRUDlBTnhBY0M3TFRoYzViZHpHanJGemZqYjJMMXBHSXVkZ25JS1laN3ZHVHkySUdnVm1iV3FNZU1RTC1aYnczcE4xd1ZXODVISFRNaWpyOUxDbHY5ZWpGLUNnM25HX2otZ0l0aEtUVWFISjgxZXhZeVd5VUtxNHZvdzNNbThaSVVSSlBjUmFwYUpSMlZhbG44aW11WU4xTG9zbXpxeHI1N3ljdlo1RmxxWXZnNnZOTUV1NVZLOVd5dGZXU3llMXVWMzFxelVyeXp4VHllOVgtM0U3bnJkZ2pXWHg4bGRvd0g0aWphOERzTkhMRjRIel9Qd0g3SnFxSEEifQ', '/panel-preview')
executor.render()
31/1:

import os
import pathlib
import sys

app = r'/Users/prabeshregmi/Downloads/Action-Recognition-in-the-Wild-master/Data Preparation/Frame and Optical Flow Generation.py'
os.chdir(str(pathlib.Path(app).parent))
sys.path = [os.getcwd()] + sys.path[1:]

from panel.io.jupyter_executor import PanelExecutor
executor = PanelExecutor(app, 'eyJzZXNzaW9uX2lkIjogIjQ1Uk12aUZYWkc3ZUxKWEljcjJXOVd3Q1NDWXBPOU95SGYycXJlaG5WNjVjIiwgInNlc3Npb25fZXhwaXJ5IjogMTcxNjQ2MjQ1MiwgIl9fYmtfX3psaWJfIjogImVOck5sTnRTbzBBUWhsLUY0a3Jka015QkNSQXJGLTU2U2lteDFpVHJtckxLR29hR2pJRUJDVEV4NHJ2dlFEenNidW05WEZEVFBkM0QxejM5ODJUeUlsNm1vTXFGMlRPZW5sdUdPUU1lUXRHWTVtbTJLUFhDVERMQms1azJlcTdyZXFhT0dvR3dqcUVVTTJza1M2aGpWS2FnM3ZtUlpYUFplSllMS0JSUHdYcEx0LXIwX28xSkt0eERGVVk5N09DdTNTV1k0WXJRM2ljSmxXMzNocHZMMlRBOVdsMWNEWkFfSHVEaDRiVTlQWXhYMHp1eDh0UEo0OFhoOFozMmtlbDQwcS02RGlCQVhnU3V6YkhIQURnT1dPRHlFS01naXV4dTZFVmRZUXNYSXM0RmNZQXlidlA2aVJ4aUUycmZtUHZHeHl6dUJfQ0kwc19oM1FaLUxGYkQ5T2ZxZWpQWUREYy1HNTc0WkhoeWhLWjNFLUtQTHhNX25Vcl9hckR5eDdHRzEyd09ZZUFFMUhVQmNZSVJ4U0cxUS1aNXhDWEk4NmlIU09CNlRwZFFBamhBd3VZc3NDUFE4V0ZJYV9qYjlhS0ktcVFLTVlzd09GRHgwT01lNWlHTkhBeFJBSlE0bmc3SGxETG0ycUZYNlZwc2wxRE0yUFlPbFFKUnlrelY5emdIeUMyZXlJZm1maWQ1WE9nSnNRWnFBV0paZ0hVSjkwdFlOQU5rNG45bnc4X0M3V3p3QnhuenNza19FQUx5WnFwS1dKZWRXWmttTFo3bmlSUzhfbUJuWFh1LXJmXzNwc24tZlItMXZkWmVaNjladVEyTWJycDFFT3Y1clFfMHM0MU1FdDVoYldUcy1GeElWV2FMMmI0eFVDVWtobllZRnlQanQ0SFJMV2EzenE1eG9EOEFWeENjeWJMVFJheU4yNWdaTzJlbllfLThaU1J5RHNZSmlIbTJhX3pTZ3FncHNOT214b2hIdkpCdkNlODFXZWRjeFVzZU55V0RzazYtdF9SN01ub0JCX1hPX2Q2aVE5aEtURWFGSHA2X0RqdFNJZ3VsaXV2TmVDUHpsaEZDbE5STnJGVXFHcFZ0VmZxSmJKbzlXT2V5YU9MTS11eWNsN1BYdGNqUzlLVnh0UmxtS1pmcTFVcjUybnFwcERhM1ZfMXF6Y295ejFUeS1Hb19iTnZ6RnF4WkZpOV9oUWIyRTJsOEhjQkdMMThFNV9uNUQtLXhxek0ifQ', '/panel-preview')
executor.render()
32/1:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os

#creating directory for storing the generated frames
p='RawData/*/*.mp4'
image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755

        os.mkdir(path, access_rights)

#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)

#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        
        
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next

cap.release()
cv2.destroyAllWindows()
32/2:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os

#creating directory for storing the generated frames
p='RawData/*/*.mp4'
image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755

        os.mkdir(path, access_rights)

#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)

#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        
        
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next

cap.release()
cv2.destroyAllWindows()
32/3:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os

#creating directory for storing the generated frames
p='RawData/*/*.mp4'
image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755

        os.mkdir(path, access_rights)

#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)

#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        
        
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next

cap.release()
cv2.destroyAllWindows()
32/4:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os

#creating directory for storing the generated frames
p='RawData/*/*.mp4'
p
image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755

        os.mkdir(path, access_rights)

#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)

#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        
        
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next

cap.release()
cv2.destroyAllWindows()
32/5:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os

#creating directory for storing the generated frames
p='RawData/*/*.mp4'

image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755

        os.mkdir(path, access_rights)

#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)

#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        
        
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next

cap.release()
cv2.destroyAllWindows()
32/6:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os
32/7:
#creating directory for storing the generated frames
p='RawData/*/*.mp4'

image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755

        os.mkdir(path, access_rights)
32/8: p
32/9:
#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)
32/10:
#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        
        
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next
32/11:
cap.release()
cv2.destroyAllWindows()
32/12:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os
32/13:
#creating directory for storing the generated frames
p='RawData/*/*.mp4'

image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755

        os.mkdir(path, access_rights)
32/14:
#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)
32/15:
#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        
        
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next
32/16: cap
32/17:
#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        
        
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next
32/18: image_base_checker
32/19:
#creating directory for storing the generated frames
p='../RawData/*/*.mp4'

image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755

        os.mkdir(path, access_rights)
32/20:
#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)
32/21:
#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        
        
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next
32/22: image_base_checker
32/23:
cap.release()
cv2.destroyAllWindows()
33/1:
#importing dependencies
from glob import glob
import tensorflow as tf 
from tensorflow.keras import Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, Average
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt
import cv2
from tensorflow.python.keras.callbacks import TensorBoard
from time import time
33/2:
#importing dependencies
from glob import glob
import tensorflow as tf 
from tensorflow.keras import Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, Average
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt
import cv2
from tensorflow.python.keras.callbacks import TensorBoard
from time import time
33/3:
#path to the training list
path='C:\\Users\\Tanya Joon\\Documents\\MM 803 Image\\split\\trainlist.txt'
f = open(path)
line = f.readline()
path1= 'C:/Users/Tanya Joon/Documents/MM 803 Image/UCF-101/'
33/4:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../RawData/'
33/5:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../RawData/'
33/6:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/7:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/8:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/9:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/10:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../RawData/'
33/11:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
32/24:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os
32/25:
#creating directory for storing the generated frames
p='../RawData/*/*.mp4'

image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755

        os.mkdir(path, access_rights)
32/26:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os
32/27:
#creating directory for storing the generated frames
p='../RawData/*/*.mp4'

image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755

        os.mkdir(path, access_rights)
33/12:
#importing dependencies
from glob import glob
import tensorflow as tf 
from tensorflow.keras import Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, Average
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt
import cv2
from tensorflow.python.keras.callbacks import TensorBoard
from time import time
33/13:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../RawData/'
33/14:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/15:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../RawData/'
33/16:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/17:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../RawData/'
33/18:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/19:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../RawData/'
33/20:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/21:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../RawData/'
33/22:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/23:
#importing dependencies
from glob import glob
import tensorflow as tf 
from tensorflow.keras import Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, Average
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt
import cv2
from tensorflow.python.keras.callbacks import TensorBoard
from time import time
33/24:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../RawData/'
33/25:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/26:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    print(filename_frames)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/27:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../RawData/'
33/28:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    print(filename_frames)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/29:
#importing dependencies
from glob import glob
import tensorflow as tf 
from tensorflow.keras import Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, Average
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt
import cv2
from tensorflow.python.keras.callbacks import TensorBoard
from time import time
33/30:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../RawData/'
33/31:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    print(filename_frames)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/32:
#storing the frames and optical flow names to a numpy array
files_frames = np.array(files_frames)
files_flow = np.array(files_flow)
33/33:
#storing the frames and optical flow names to a numpy array
files_frames = np.array(files_frames)
files_flow = np.array(files_flow)
file_flow
33/34:
#storing the frames and optical flow names to a numpy array
files_frames = np.array(files_frames)
files_flow = np.array(files_flow)
files_flow
33/35:
#reading all the 101 class labels from the file
dct = {}
for line in open("C:/Users/Tanya Joon/Documents/MM 803 Image/split/classInd.txt", "r").readlines():
    x, y = line.strip().split(' ')
    dct[y] = int(x)

BATCH_SIZE = 64
33/36:
#reading all the 101 class labels from the file
dct = {}
for line in open("../trainlist.txt", "r").readlines():
    x, y = line.strip().split(' ')
    dct[y] = int(x)

BATCH_SIZE = 64
33/37:
#defining a data generator to feed the frames and optical flow to the model
def datagen():
    
    while True:
        
        samples = np.random.randint(0, len(files_frames), size = BATCH_SIZE)
        yield [np.array([cv2.resize(cv2.imread(file), (224, 224)) for file in files_frames[samples]]), np.array([np.reshape(cv2.resize(cv2.imread(file, 0), (224, 224)), (224, 224, 1)) for file in files_flow[samples]])], to_categorical([dct[file.split('/')[6]]-1 for file in files_frames[samples]], 101)
        
gen = datagen()
33/38:
#Model


#input layer: taking generated frames as input
inp_frames = Input(shape=(224,224,3))
#Layer1
conv_1_frames = Conv2D(96, (7,7), strides= 2, activation='relu')(inp_frames)
batch_norm_1_frames= tf.nn.local_response_normalization(conv_1_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
pool_1_frames = MaxPooling2D((2,2)) (batch_norm_1_frames)
#Layer2
conv_2_frames = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_frames)
batch_norm_2_frames = tf.nn.local_response_normalization(conv_2_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
pool_2_frames = MaxPooling2D((2,2)) (batch_norm_2_frames)
#Layer3
conv_3_frames = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_frames)
#Layer4
conv_4_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_frames)
#Layer5
conv_5_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_frames)
pool_3_frames = MaxPooling2D((2,2))(conv_5_frames)
flat_frames = Flatten() (pool_3_frames)
#Layer6
fc_1_frames = Dense(4096,activation='relu')(flat_frames)
#Layer7
fc_2_frames = Dense(2048,activation='relu')(fc_1_frames)
#output layer
out_frames = Dense(101,activation='softmax')(fc_2_frames)


#input layer: taking generated optical flow as input
inp_flow = Input(shape=(224,224,1))
#Layer1
conv_1_flow = Conv2D(96, (7,7), strides= 2, activation='relu')(inp_flow)
batch_norm_1_flow= tf.nn.local_response_normalization(conv_1_flow, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
pool_1_flow = MaxPooling2D((2,2)) (batch_norm_1_flow)
#Layer2
conv_2_flow = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_flow)
pool_2_flow = MaxPooling2D((2,2)) (conv_2_flow)
#Layer3
conv_3_flow = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_flow)
#Layer4
conv_4_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_flow)
#Layer5
conv_5_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_flow)
pool_3_flow = MaxPooling2D((2,2))(conv_5_flow)
flat_flow = Flatten() (pool_3_flow)
#Layer6
fc_1_flow = Dense(4096,activation='relu')(flat_flow)
#Layer7
fc_2_flow = Dense(2048,activation='relu')(fc_1_flow)
#output layer
out_flow = Dense(101,activation='softmax')(fc_2_flow)
33/39:

#Taking the output of both the streams and combining them 
out = Average()([out_frames, out_flow])
model = Model(inputs=[inp_frames, inp_flow], outputs=out)
opti_flow = tf.keras.optimizers.Adam(learning_rate=1e-5)

#compiling the model by using categorical_crossentropy loss
model.compile(optimizer=opti_flow, loss = 'categorical_crossentropy', metrics=['mae','accuracy'])
model.summary()

#visualizing the model on tensorboard
tensorboard = TensorBoard(log_dir="logs\{}".format(time()),write_graph=True)

#calling the datagenerator and passing the inputs to our model for training
i=0
hist_frames=[]
for x, y in datagen():
    i=i+1 
    print(i)
    if(i == 15000): break
    history = model.fit(x,y, batch_size=64, epochs=1,callbacks=[tensorboard])
    hist_frames.append(history.history) 

#saving training history
print("\nhistory dict:",hist_frames)

#saving the model after training
model.save('C:\\Users\\Tanya Joon\\Documents\\MM 803 Image\\model.h5')

#saving the training loss in an numpy array
loss_array=[]
for i in hist_frames:
    for j in i['loss']:
        loss_array.append(j)
        
#saving the training accuracy in an numpy array
accuracy_array=[]
for i in hist_frames:
    for j in i['accuracy']:
        accuracy_array.append(j)   

#printing the average training loss and accuracy                
print("Average accuracy: ", np.average(accuracy_array))
print("Average test loss: ", np.average(loss_array))

#visualizing training accuracy
plt.plot(accuracy_array[0::200])
plt.title('model accuracy')
plt.ylabel('')
plt.xlabel('')
plt.legend(['accuracy'], loc='upper left')
plt.savefig('accuracy.png')

#visualizing training loss
plt.clf()  
plt.plot(loss_array[0::200])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('')
plt.savefig('loss.png')
33/40:
#importing dependencies
from glob import glob
import tensorflow as tf 
from tensorflow.keras import Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, Average
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt
import cv2
from tensorflow.python.keras.callbacks import TensorBoard
from time import time
33/41:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../RawData/'
33/42:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    print(filename_frames)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/43:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1  
    filename_flow=path1+line1
    print(filename_flow)
    print(filename_frames)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/44:
#storing the frames and optical flow names to a numpy array
files_frames = np.array(files_frames)
files_flow = np.array(files_flow)
files_flow
33/45:
#reading all the 101 class labels from the file
dct = {}
for line in open("../trainlist.txt", "r").readlines():
    x, y = line.strip().split(' ')
    dct[y] = int(x)

BATCH_SIZE = 64
33/46:
#importing dependencies
from glob import glob
import tensorflow as tf 
from tensorflow.keras import Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, Average
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt
import cv2
from tensorflow.python.keras.callbacks import TensorBoard
from time import time
33/47:
#path to the training list
path='../trainlist.txt'
f = open(path)
line = f.readline()
path1= '../UCF-101/'
33/48:
i = 0
w = [];
files_frames = []
files_flow = []
while line:
    #reading the training list line by line and appending the frames and optical flow names to the list
    line1=line[:-1]
    filename_frames= path1+line1+'_frames'  
    filename_flow=path1+line1+'_flow' 
    print(filename_flow)
    print(filename_frames)
    line = f.readline()
    imagePatches_frames = glob(filename_frames+'/*.png')
    imagePatches_flow = glob(filename_flow+'/*.png') 
    count_frames=0;
    count_flow=0;
    files_frames += imagePatches_frames
    files_flow += imagePatches_flow
    count_frames += len(imagePatches_frames)
    count_flow += len(imagePatches_flow)
print(count_frames)
print(count_flow)
f.close()
33/49:
#storing the frames and optical flow names to a numpy array
files_frames = np.array(files_frames)
files_flow = np.array(files_flow)
files_flow
33/50:
#reading all the 101 class labels from the file
dct = {}
for line in open("../trainlist.txt", "r").readlines():
    x, y = line.strip().split(' ')
    dct[y] = int(x)

BATCH_SIZE = 64
33/51:
#reading all the 101 class labels from the file
dct = {}
for line in open("../classInd.txt", "r").readlines():
    x, y = line.strip().split(' ')
    dct[y] = int(x)

BATCH_SIZE = 64
33/52:
#defining a data generator to feed the frames and optical flow to the model
def datagen():
    
    while True:
        
        samples = np.random.randint(0, len(files_frames), size = BATCH_SIZE)
        yield [np.array([cv2.resize(cv2.imread(file), (224, 224)) for file in files_frames[samples]]), np.array([np.reshape(cv2.resize(cv2.imread(file, 0), (224, 224)), (224, 224, 1)) for file in files_flow[samples]])], to_categorical([dct[file.split('/')[6]]-1 for file in files_frames[samples]], 101)
        
gen = datagen()
33/53:
#Model


#input layer: taking generated frames as input
inp_frames = Input(shape=(224,224,3))
#Layer1
conv_1_frames = Conv2D(96, (7,7), strides= 2, activation='relu')(inp_frames)
batch_norm_1_frames= tf.nn.local_response_normalization(conv_1_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
pool_1_frames = MaxPooling2D((2,2)) (batch_norm_1_frames)
#Layer2
conv_2_frames = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_frames)
batch_norm_2_frames = tf.nn.local_response_normalization(conv_2_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
pool_2_frames = MaxPooling2D((2,2)) (batch_norm_2_frames)
#Layer3
conv_3_frames = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_frames)
#Layer4
conv_4_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_frames)
#Layer5
conv_5_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_frames)
pool_3_frames = MaxPooling2D((2,2))(conv_5_frames)
flat_frames = Flatten() (pool_3_frames)
#Layer6
fc_1_frames = Dense(4096,activation='relu')(flat_frames)
#Layer7
fc_2_frames = Dense(2048,activation='relu')(fc_1_frames)
#output layer
out_frames = Dense(101,activation='softmax')(fc_2_frames)


#input layer: taking generated optical flow as input
inp_flow = Input(shape=(224,224,1))
#Layer1
conv_1_flow = Conv2D(96, (7,7), strides= 2, activation='relu')(inp_flow)
batch_norm_1_flow= tf.nn.local_response_normalization(conv_1_flow, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
pool_1_flow = MaxPooling2D((2,2)) (batch_norm_1_flow)
#Layer2
conv_2_flow = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_flow)
pool_2_flow = MaxPooling2D((2,2)) (conv_2_flow)
#Layer3
conv_3_flow = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_flow)
#Layer4
conv_4_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_flow)
#Layer5
conv_5_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_flow)
pool_3_flow = MaxPooling2D((2,2))(conv_5_flow)
flat_flow = Flatten() (pool_3_flow)
#Layer6
fc_1_flow = Dense(4096,activation='relu')(flat_flow)
#Layer7
fc_2_flow = Dense(2048,activation='relu')(fc_1_flow)
#output layer
out_flow = Dense(101,activation='softmax')(fc_2_flow)
34/1:
import numpy as np
import os
import cv2
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf
34/2:
# Define the path to your UCF-101 dataset
dataset_path = 'UCF-101'

# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

# Define a function to load the dataset and compute optical flow for each video
def load_dataset(dataset_path):
    data = []
    labels = []
    classes = os.listdir(dataset_path)
    class_to_label = {cls: i for i, cls in enumerate(classes)}
    for cls in classes:
        class_path = os.path.join(dataset_path, cls)
        for video_name in os.listdir(class_path):
            video_path = os.path.join(class_path, video_name)
            frames = extract_frames(video_path)
            optical_flows = compute_optical_flow(frames)
            data.append(optical_flows)
            labels.append(class_to_label[cls])
    return np.array(data), np.array(labels)

data, labels = load_dataset(dataset_path)

# Convert labels to categorical format
labels = to_categorical(labels, num_classes=len(os.listdir(dataset_path)))

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# Ensure data has the correct shape for Conv3D
X_train = np.expand_dims(X_train, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)
34/3:
# Define the path to your UCF-101 dataset
dataset_path = '/UCF-101'

# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

# Define a function to load the dataset and compute optical flow for each video
def load_dataset(dataset_path):
    data = []
    labels = []
    classes = os.listdir(dataset_path)
    class_to_label = {cls: i for i, cls in enumerate(classes)}
    for cls in classes:
        class_path = os.path.join(dataset_path, cls)
        for video_name in os.listdir(class_path):
            video_path = os.path.join(class_path, video_name)
            frames = extract_frames(video_path)
            optical_flows = compute_optical_flow(frames)
            data.append(optical_flows)
            labels.append(class_to_label[cls])
    return np.array(data), np.array(labels)

data, labels = load_dataset(dataset_path)

# Convert labels to categorical format
labels = to_categorical(labels, num_classes=len(os.listdir(dataset_path)))

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# Ensure data has the correct shape for Conv3D
X_train = np.expand_dims(X_train, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)
34/4:
# Define the path to your UCF-101 dataset
dataset_path = '../UCF-101'

# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

# Define a function to load the dataset and compute optical flow for each video
def load_dataset(dataset_path):
    data = []
    labels = []
    classes = os.listdir(dataset_path)
    class_to_label = {cls: i for i, cls in enumerate(classes)}
    for cls in classes:
        class_path = os.path.join(dataset_path, cls)
        for video_name in os.listdir(class_path):
            video_path = os.path.join(class_path, video_name)
            frames = extract_frames(video_path)
            optical_flows = compute_optical_flow(frames)
            data.append(optical_flows)
            labels.append(class_to_label[cls])
    return np.array(data), np.array(labels)

data, labels = load_dataset(dataset_path)

# Convert labels to categorical format
labels = to_categorical(labels, num_classes=len(os.listdir(dataset_path)))

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# Ensure data has the correct shape for Conv3D
X_train = np.expand_dims(X_train, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)
34/5:
# Define the path to your UCF-101 dataset
dataset_path = 'UCF-101'

# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

# Define a function to load the dataset and compute optical flow for each video
def load_dataset(dataset_path):
    data = []
    labels = []
    classes = os.listdir(dataset_path)
    class_to_label = {cls: i for i, cls in enumerate(classes)}
    for cls in classes:
        class_path = os.path.join(dataset_path, cls)
        for video_name in os.listdir(class_path):
            video_path = os.path.join(class_path, video_name)
            frames = extract_frames(video_path)
            optical_flows = compute_optical_flow(frames)
            data.append(optical_flows)
            labels.append(class_to_label[cls])
    return np.array(data), np.array(labels)

data, labels = load_dataset(dataset_path)

# Convert labels to categorical format
labels = to_categorical(labels, num_classes=len(os.listdir(dataset_path)))

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# Ensure data has the correct shape for Conv3D
X_train = np.expand_dims(X_train, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)
34/6:
# Define the path to your UCF-101 dataset
dataset_path = 'UCF-101/'

# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

# Define a function to load the dataset and compute optical flow for each video
def load_dataset(dataset_path):
    data = []
    labels = []
    classes = os.listdir(dataset_path)
    class_to_label = {cls: i for i, cls in enumerate(classes)}
    for cls in classes:
        class_path = os.path.join(dataset_path, cls)
        for video_name in os.listdir(class_path):
            video_path = os.path.join(class_path, video_name)
            frames = extract_frames(video_path)
            optical_flows = compute_optical_flow(frames)
            data.append(optical_flows)
            labels.append(class_to_label[cls])
    return np.array(data), np.array(labels)

data, labels = load_dataset(dataset_path)

# Convert labels to categorical format
labels = to_categorical(labels, num_classes=len(os.listdir(dataset_path)))

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# Ensure data has the correct shape for Conv3D
X_train = np.expand_dims(X_train, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)
34/7:
# Define the path to your UCF-101 dataset
dataset_path = 'UCF-101'

# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

# Define a function to load the dataset and compute optical flow for each video
def load_dataset(dataset_path):
    data = []
    labels = []
    classes = [cls for cls in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, cls))]
    class_to_label = {cls: i for i, cls in enumerate(classes)}
    for cls in classes:
        class_path = os.path.join(dataset_path, cls)
        for video_name in os.listdir(class_path):
            if video_name.endswith('.avi'):  # Ensure it's a video file
                video_path = os.path.join(class_path, video_name)
                frames = extract_frames(video_path)
                optical_flows = compute_optical_flow(frames)
                data.append(optical_flows)
                labels.append(class_to_label[cls])
    return np.array(data), np.array(labels)

data, labels = load_dataset(dataset_path)

# Convert labels to categorical format
labels = to_categorical(labels, num_classes=len(os.listdir(dataset_path)))

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# Ensure data has the correct shape for Conv3D
X_train = np.expand_dims(X_train, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)
34/8:
import numpy as np
import os
import cv2
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf
34/9:
# Define the path to your UCF-101 dataset
dataset_path = 'UCF-101'

# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

# Define a function to load the dataset and compute optical flow for each video
def load_dataset(dataset_path):
    data = []
    labels = []
    classes = [cls for cls in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, cls))]
    class_to_label = {cls: i for i, cls in enumerate(classes)}
    for cls in classes:
        class_path = os.path.join(dataset_path, cls)
        for video_name in os.listdir(class_path):
            if video_name.endswith('.avi'):  # Ensure it's a video file
                video_path = os.path.join(class_path, video_name)
                frames = extract_frames(video_path)
                optical_flows = compute_optical_flow(frames)
                data.append(optical_flows)
                labels.append(class_to_label[cls])
    return np.array(data), np.array(labels)

data, labels = load_dataset(dataset_path)

# Convert labels to categorical format
labels = to_categorical(labels, num_classes=len(os.listdir(dataset_path)))

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# Ensure data has the correct shape for Conv3D
X_train = np.expand_dims(X_train, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)
34/10:
# Define the path to your UCF-101 dataset
dataset_path = 'UCF-101'

# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

# Define a function to pad/truncate the sequences to a fixed length
def pad_truncate_sequence(sequence, max_len=40):
    if len(sequence) < max_len:
        # Pad the sequence with zeros if it's shorter than max_len
        padding = [np.zeros_like(sequence[0])] * (max_len - len(sequence))
        sequence.extend(padding)
    else:
        # Truncate the sequence if it's longer than max_len
        sequence = sequence[:max_len]
    return sequence

# Define a function to load the dataset and compute optical flow for each video
def load_dataset(dataset_path, max_len=40):
    data = []
    labels = []
    classes = [cls for cls in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, cls))]
    class_to_label = {cls: i for i, cls in enumerate(classes)}
    for cls in classes:
        class_path = os.path.join(dataset_path, cls)
        for video_name in os.listdir(class_path):
            if video_name.endswith('.avi'):  # Ensure it's a video file
                video_path = os.path.join(class_path, video_name)
                frames = extract_frames(video_path)
                optical_flows = compute_optical_flow(frames)
                optical_flows = pad_truncate_sequence(optical_flows, max_len)
                data.append(optical_flows)
                labels.append(class_to_label[cls])
    return np.array(data), np.array(labels)

max_len = 40  # Set the maximum length for sequences
data, labels = load_dataset(dataset_path, max_len=max_len)

# Convert labels to categorical format
labels = to_categorical(labels, num_classes=len(os.listdir(dataset_path)))

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# Ensure data has the correct shape for Conv3D
X_train = np.expand_dims(X_train, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)
34/11:
model = Sequential([
    Conv3D(32, (3, 3, 3), activation='relu', input_shape=(None, 224, 224, 2), padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Conv3D(64, (3, 3, 3), activation='relu', padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Conv3D(128, (3, 3, 3), activation='relu', padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(len(os.listdir(dataset_path)), activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()
34/12:
input_shape = (max_len, 224, 224, 2)  # Adjust input shape according to padded sequences

model = Sequential([
    Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape, padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Conv3D(64, (3, 3, 3), activation='relu', padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Conv3D(128, (3, 3, 3), activation='relu', padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()
34/13:
input_shape = (max_len, 224, 224, 2)  # Adjust input shape according to padded sequences

model = Sequential([
    Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape, padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Conv3D(64, (3, 3, 3), activation='relu', padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Conv3D(128, (3, 3, 3), activation='relu', padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()
34/14:
# Define the path to your UCF-101 dataset
dataset_path = 'UCF-101'

# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

# Define a function to pad/truncate the sequences to a fixed length
def pad_truncate_sequence(sequence, max_len=40):
    if len(sequence) < max_len:
        # Pad the sequence with zeros if it's shorter than max_len
        padding = [np.zeros_like(sequence[0])] * (max_len - len(sequence))
        sequence.extend(padding)
    else:
        # Truncate the sequence if it's longer than max_len
        sequence = sequence[:max_len]
    return sequence

# Define a function to load the dataset and compute optical flow for each video
def load_dataset(dataset_path, max_len=40):
    data = []
    labels = []
    classes = [cls for cls in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, cls))]
    class_to_label = {cls: i for i, cls in enumerate(classes)}
    for cls in classes:
        class_path = os.path.join(dataset_path, cls)
        for video_name in os.listdir(class_path):
            if video_name.endswith('.avi'):  # Ensure it's a video file
                video_path = os.path.join(class_path, video_name)
                frames = extract_frames(video_path)
                optical_flows = compute_optical_flow(frames)
                optical_flows = pad_truncate_sequence(optical_flows, max_len)
                data.append(optical_flows)
                labels.append(class_to_label[cls])
    return np.array(data), np.array(labels)

max_len = 40  # Set the maximum length for sequences
data, labels = load_dataset(dataset_path, max_len=max_len)

# Convert labels to categorical format
num_classes = len([cls for cls in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, cls))])
labels = to_categorical(labels, num_classes=num_classes)

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# Ensure data has the correct shape for Conv3D
X_train = np.expand_dims(X_train, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)
34/15:
input_shape = (max_len, 224, 224, 2)  # Adjust input shape according to padded sequences

model = Sequential([
    Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape, padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Conv3D(64, (3, 3, 3), activation='relu', padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Conv3D(128, (3, 3, 3), activation='relu', padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()
34/16:
# Define a callback to save the best model during training
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')

history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_val, y_val), callbacks=[checkpoint])
34/17:
from tensorflow.keras.callbacks import ModelCheckpoint

# Define a callback to save the best model during training
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')

history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_val, y_val), callbacks=[checkpoint])
34/18:
from tensorflow.keras.callbacks import ModelCheckpoint

# Define a callback to save the best model during training
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')

history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_val, y_val), callbacks=[checkpoint])
34/19:
from tensorflow.keras.callbacks import ModelCheckpoint

# Define a callback to save the best model during training
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')

history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_val, y_val), callbacks=[checkpoint])
34/20:
from tensorflow.keras.callbacks import ModelCheckpoint

# Define a callback to save the best model during training
# Change the filepath to end with '.keras' or '.weights.h5'
checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min')

history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_val, y_val), callbacks=[checkpoint])
34/21:
model.load_weights('best_model.h5')
loss, accuracy = model.evaluate(X_val, y_val)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')
35/1:
model.load_weights('best_model.keras')
loss, accuracy = model.evaluate(X_val, y_val)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')
35/2:
model.load_weights('best_model.keras')
loss, accuracy = model.evaluate(X_val, y_val)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')
35/3:
def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'path/to/test/video.mp4'
print(f'Predicted class: {predict(video_path)}')
35/4:
model.load_weights('best_model.keras')
loss, accuracy = model.evaluate(X_val, y_val)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')
35/5:
def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'path/to/test/video.mp4'
print(f'Predicted class: {predict(video_path)}')
35/6:
import numpy as np
import os
import cv2
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf
35/7:
def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'path/to/test/video.mp4'
print(f'Predicted class: {predict(video_path)}')
35/8:
# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'path/to/test/video.mp4'
print(f'Predicted class: {predict(video_path)}')
35/9:
# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'path/to/test/video.mp4'
print(f'Predicted class: {predict(video_path)}')
35/10:
# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'path/to/test/video.mp4'
print(f'Predicted class: {predict(video_path)}')
35/11:
# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'a.avi'
print(f'Predicted class: {predict(video_path)}')
35/12:
model.load_weights('best_model.keras')
loss, accuracy = model.evaluate(X_val, y_val)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')
35/13:
from tensorflow.keras.callbacks import ModelCheckpoint

# Define a callback to save the best model during training
# Change the filepath to end with '.keras' or '.weights.h5'
checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min')

history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_val, y_val), callbacks=[checkpoint])
35/14:
model.load_weights('best_model.keras')
loss, accuracy = model.evaluate(X_val, y_val)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')
35/15:
# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'a.avi'
print(f'Predicted class: {predict(video_path)}')
36/1:
import numpy as np
import os
import cv2
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf
36/2:
# Define the path to your UCF-101 dataset
dataset_path = 'UCF-101'

# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

# Define a function to pad/truncate the sequences to a fixed length
def pad_truncate_sequence(sequence, max_len=40):
    if len(sequence) < max_len:
        # Pad the sequence with zeros if it's shorter than max_len
        padding = [np.zeros_like(sequence[0])] * (max_len - len(sequence))
        sequence.extend(padding)
    else:
        # Truncate the sequence if it's longer than max_len
        sequence = sequence[:max_len]
    return sequence

# Define a function to load the dataset and compute optical flow for each video
def load_dataset(dataset_path, max_len=40):
    data = []
    labels = []
    classes = [cls for cls in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, cls))]
    class_to_label = {cls: i for i, cls in enumerate(classes)}
    for cls in classes:
        class_path = os.path.join(dataset_path, cls)
        for video_name in os.listdir(class_path):
            if video_name.endswith('.avi'):  # Ensure it's a video file
                video_path = os.path.join(class_path, video_name)
                frames = extract_frames(video_path)
                optical_flows = compute_optical_flow(frames)
                optical_flows = pad_truncate_sequence(optical_flows, max_len)
                data.append(optical_flows)
                labels.append(class_to_label[cls])
    return np.array(data), np.array(labels)

max_len = 40  # Set the maximum length for sequences
data, labels = load_dataset(dataset_path, max_len=max_len)

# Convert labels to categorical format
num_classes = len([cls for cls in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, cls))])
labels = to_categorical(labels, num_classes=num_classes)

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# Ensure data has the correct shape for Conv3D
X_train = np.expand_dims(X_train, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)
36/3:
input_shape = (max_len, 224, 224, 2)  # Adjust input shape according to padded sequences

model = Sequential([
    Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape, padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Conv3D(64, (3, 3, 3), activation='relu', padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Conv3D(128, (3, 3, 3), activation='relu', padding='same'),
    MaxPooling3D((2, 2, 2), padding='same'),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()
36/4:
from tensorflow.keras.callbacks import ModelCheckpoint

# Define a callback to save the best model during training
# Change the filepath to end with '.keras' or '.weights.h5'
checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min')

history = model.fit(X_train, y_train, epochs=2, batch_size=8, validation_data=(X_val, y_val), callbacks=[checkpoint])
36/5:
model.load_weights('best_model.keras')
loss, accuracy = model.evaluate(X_val, y_val)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')
36/6:
# Define a function to extract frames from a video file
def extract_frames(video_path):
    frames = []
    cap = cv2.VideoCapture(video_path)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(cv2.resize(frame, (224, 224)))
    cap.release()
    return frames

# Define a function to compute optical flow between frames
def compute_optical_flow(frames):
    optical_flows = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    for i in range(1, len(frames)):
        gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        optical_flows.append(flow)
        prev_gray = gray
    return optical_flows

def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'a.avi'
print(f'Predicted class: {predict(video_path)}')
36/7:
model.load_weights('best_model.keras')
loss, accuracy = model.evaluate(X_val, y_val)
print(f'Validation Accuracy: {accuracy * 100:.2f}%')
36/8:
def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'a.avi'
print(f'Predicted class: {predict(video_path)}')
36/9:
def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'a.avi'
print(f'Predicted class: {predict(video_path)}')
36/10:
def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'a.avi'
# print(f'Predicted class: {predict(video_path)}')
36/11:
def predict(video_path):
    frames = extract_frames(video_path)
    optical_flows = compute_optical_flow(frames)
    optical_flows = np.expand_dims(optical_flows, axis=0)
    optical_flows = np.expand_dims(optical_flows, axis=-1)
    predictions = model.predict(optical_flows)
    class_idx = np.argmax(predictions, axis=1)
    class_label = list(class_to_label.keys())[list(class_to_label.values()).index(class_idx)]
    return class_label

# Test the prediction function
video_path = 'a.avi'
print(f'Predicted class: {predict(video_path)}')
37/1:
# Import the required libraries.
import os
import cv2
# import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
37/2:
# Import the required libraries.
import os
import cv2
# import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

# from moviepy.editor import *
# %matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
37/3:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
37/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('dataset/UCF50')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'dataset/UCF50/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'dataset/UCF50/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
37/5:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('dataset/UCF50')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'dataset/UCF50/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'dataset/UCF50/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
37/6:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('dataset/UCF50')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 5)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'dataset/UCF50/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'dataset/UCF50/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
37/7:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF50"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["WalkingWithDog", "TaiChi", "Swing", "HorseRace"]
37/8:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
37/9:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
37/10:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
37/11:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF50"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["Archery", "BabyCrawling", "ApplyLipstick", "BandMarching"]
37/12:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
37/13:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
37/14:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
37/15:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
37/16:
# Split the Data into Train ( 75% ) and Test Set ( 25% ).
features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels,
                                                                            test_size = 0.25, shuffle = True,
                                                                            random_state = seed_constant)
37/17:
def create_convlstm_model():
    '''
    This function will construct the required convlstm model.
    Returns:
        model: It is the required constructed convlstm model.
    '''

    # We will use a Sequential model for model construction
    model = Sequential()

    # Define the Model Architecture.
    ########################################################################################################################
    
    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,
                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    #model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(Flatten()) 
    
    model.add(Dense(len(CLASSES_LIST), activation = "softmax"))
    
    ########################################################################################################################
     
    # Display the models summary.
    model.summary()
    
    # Return the constructed convlstm model.
    return model
37/18:
# Construct the required convlstm model.
convlstm_model = create_convlstm_model()

# Display the success message. 
print("Model Created Successfully!")
37/19:
# Construct the required convlstm model.
convlstm_model = create_convlstm_model()

# Display the success message. 
print("Model Created Successfully!")
37/20:
# Plot the structure of the contructed model.
plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)
37/21:
# Create an Instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)

# Compile the model and specify loss function, optimizer and metrics values to the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 50, batch_size = 4,
                                                     shuffle = True, validation_split = 0.2, 
                                                     callbacks = [early_stopping_callback])
37/22:
# Evaluate the trained model.
model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)
37/23:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save your Model.
convlstm_model.save(model_file_name)
37/24:
def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):
    '''
    This function will plot the metrics passed to it in a graph.
    Args:
        model_training_history: A history object containing a record of training and validation 
                                loss values and metrics values at successive epochs
        metric_name_1:          The name of the first metric that needs to be plotted in the graph.
        metric_name_2:          The name of the second metric that needs to be plotted in the graph.
        plot_name:              The title of the graph.
    '''
    
    # Get metric values using metric names as identifiers.
    metric_value_1 = model_training_history.history[metric_name_1]
    metric_value_2 = model_training_history.history[metric_name_2]
    
    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.
    epochs = range(len(metric_value_1))

    # Plot the Graph.
    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)

    # Add title to the plot.
    plt.title(str(plot_name))

    # Add legend to the plot.
    plt.legend()
37/25:
# Visualize the training and validation loss metrices.
plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
37/26:
# Visualize the training and validation accuracy metrices.
plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
37/27:
def create_LRCN_model():
    '''
    This function will construct the required LRCN model.
    Returns:
        model: It is the required constructed LRCN model.
    '''

    # We will use a Sequential model for model construction.
    model = Sequential()
    
    # Define the Model Architecture.
    ########################################################################################################################
    
    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),
                              input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))
    
    model.add(TimeDistributed(MaxPooling2D((4, 4)))) 
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((4, 4))))
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    #model.add(TimeDistributed(Dropout(0.25)))
                                      
    model.add(TimeDistributed(Flatten()))
                                      
    model.add(LSTM(32))
                                      
    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))

    ########################################################################################################################

    # Display the models summary.
    model.summary()
    
    # Return the constructed LRCN model.
    return model
37/28:
# Construct the required LRCN model.
LRCN_model = create_LRCN_model()

# Display the success message.
print("Model Created Successfully!")
37/29:
# Plot the structure of the contructed LRCN model.
plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)
37/30:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LRCN_model_training_history = LRCN_model.fit(x = features_train, y = labels_train, epochs = 70, batch_size = 4 ,
                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])
37/31:
# Evaluate the trained model.
model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)
37/32:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save your Model.
convlstm_model.save(model_file_name)
37/33:
def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):
    '''
    This function will plot the metrics passed to it in a graph.
    Args:
        model_training_history: A history object containing a record of training and validation 
                                loss values and metrics values at successive epochs
        metric_name_1:          The name of the first metric that needs to be plotted in the graph.
        metric_name_2:          The name of the second metric that needs to be plotted in the graph.
        plot_name:              The title of the graph.
    '''
    
    # Get metric values using metric names as identifiers.
    metric_value_1 = model_training_history.history[metric_name_1]
    metric_value_2 = model_training_history.history[metric_name_2]
    
    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.
    epochs = range(len(metric_value_1))

    # Plot the Graph.
    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)

    # Add title to the plot.
    plt.title(str(plot_name))

    # Add legend to the plot.
    plt.legend()
37/34:
# Visualize the training and validation loss metrices.
plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
37/35:
# Visualize the training and validation accuracy metrices.
plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
37/36:
# Evaluate the trained model.
model_evaluation_history = LRCN_model.evaluate(features_test, labels_test)
37/37:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save the Model.
LRCN_model.save(model_file_name)
37/38:
# Visualize the training and validation loss metrices.
plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
37/39:
# Visualize the training and validation accuracy metrices.
plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
37/40:
def download_youtube_videos(youtube_video_url, output_directory):
     '''
    This function downloads the youtube video whose URL is passed to it as an argument.
    Args:
        youtube_video_url: URL of the video that is required to be downloaded.
        output_directory:  The directory path to which the video needs to be stored after downloading.
    Returns:
        title: The title of the downloaded youtube video.
    '''
 
     # Create a video object which contains useful information about the video.
     video = pafy.new(youtube_video_url)
 
     # Retrieve the title of the video.
     title = video.title
 
     # Get the best available quality object for the video.
     video_best = video.getbest()
 
     # Construct the output file path.
     output_file_path = f'{output_directory}/{title}.mp4'
 
     # Download the youtube video at the best available quality and store it to the contructed path.
     video_best.download(filepath = output_file_path, quiet = True)
 
     # Return the video title.
     return title
37/41:
# Make the Output directory if it does not exist
test_videos_directory = 'test_videos'
os.makedirs(test_videos_directory, exist_ok = True)

# Download a YouTube Video.
video_title = download_youtube_videos('https://www.youtube.com/watch?v=8u0qjmHIOcE', test_videos_directory)

# Get the YouTube Video's path we just downloaded.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
37/42:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
37/43:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
37/44:
# # Make the Output directory if it does not exist
# test_videos_directory = 'test_videos'
# os.makedirs(test_videos_directory, exist_ok = True)

# # Download a YouTube Video.
# video_title = download_youtube_videos('https://www.youtube.com/watch?v=8u0qjmHIOcE', test_videos_directory)

# Get the YouTube Video's path we just downloaded.
input_video_file_path = 'test_videos/v-applyeyemakeup-g20-c06_Yobm6Rq9.mp4'
37/45:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
37/46:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
37/47:
# # Make the Output directory if it does not exist
# test_videos_directory = 'test_videos'
# os.makedirs(test_videos_directory, exist_ok = True)

# # Download a YouTube Video.
# video_title = download_youtube_videos('https://www.youtube.com/watch?v=8u0qjmHIOcE', test_videos_directory)
video_title='v-applyeyemakeup-g20-c06_Yobm6Rq9';
# Get the YouTube Video's path we just downloaded.
input_video_file_path = 'test_videos/v-applyeyemakeup-g20-c06_Yobm6Rq9.mp4'
37/48:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
37/49:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
37/50:
def predict_single_action(video_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform single action recognition prediction on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Declare a list to store video frames we will extract.
    frames_list = []
    
    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Get the number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH),1)

    # Iterating the number of times equal to the fixed length of sequence.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Read a frame.
        success, frame = video_reader.read() 

        # Check if frame is not read properly then break the loop.
        if not success:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        
        # Appending the pre-processed frame into the frames list
        frames_list.append(normalized_frame)

    # Passing the  pre-processed frames to the model and get the predicted probabilities.
    predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_list, axis = 0))[0]

    # Get the index of class with highest probability.
    predicted_label = np.argmax(predicted_labels_probabilities)

    # Get the class name using the retrieved index.
    predicted_class_name = CLASSES_LIST[predicted_label]
    
    # Display the predicted action along with the prediction confidence.
    print(f'Action Predicted: {predicted_class_name}\nConfidence: {predicted_labels_probabilities[predicted_label]}')
        
    # Release the VideoCapture object. 
    video_reader.release()
37/51:
# Download the youtube video.
video_title = download_youtube_videos('https://youtu.be/fc3w827kwyA', test_videos_directory)

# Construct tihe nput youtube video path
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'

# Perform Single Prediction on the Test Video.
predict_single_action(input_video_file_path, SEQUENCE_LENGTH)

# Display the input video.
VideoFileClip(input_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
37/52: !pip install tensorflow opencv-contrib-python youtube-dl moviepy pydot
37/53:
from moviepy.editor import *
%matplotlib inline
37/54:
from moviepy.config import *
change_settings({"FFMPEG_BINARY": "/usr/bin/ffmpeg"})
%matplotlib inline
37/55:
from moviepy.config import change_settings
change_settings({"FFMPEG_BINARY": "/usr/bin/ffmpeg"})
37/56:
import os
os.environ["IMAGEIO_FFMPEG_EXE"] = "/usr/bin/ffmpeg"
from moviepy.editor import *
%matplotlib inline
37/57:

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
38/1:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os
38/2:
#creating directory for storing the generated frames
p='../UCF-101/*/*.avi'
image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755
        os.mkdir(path, access_rights)
38/3:
#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)
38/4:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os
38/5:
#creating directory for storing the generated frames
p='../UCF-101/*/*.avi'
image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755
        os.mkdir(path, access_rights)
38/6:
#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)
38/7:
#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)

#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next
38/8:
cap.release()
cv2.destroyAllWindows()
38/9:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os
38/10:
#creating directory for storing the generated frames
p='../Dataset/*/*.avi'
image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755
        os.mkdir(path, access_rights)
38/11:
#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)
38/12:
#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)

#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next
38/13:
cap.release()
cv2.destroyAllWindows()
38/14:
#importing dependencies
import cv2
import numpy as np
from glob import glob
import os
38/15:
#creating directory for storing the generated frames
p='../Dataset/*/*.avi'
image_base_checker = glob(p)
for filenames in image_base_checker:
        print(filenames)
        path=str(filenames)+'_frames'
        print(path)        
        access_rights = 0o755
        os.mkdir(path, access_rights)
38/16:
#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)
38/17:
#creating directory for storing the generated optical flow
image_base_checker = glob(p)
for filenames1 in image_base_checker:
        print(filenames1)
        path1=str(filenames1)+'_flow'
        print(path1)       
        access_rights = 0o755
        os.mkdir(path1, access_rights)

#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next
38/18:
#listing all the videos using glob
image_base_checker = glob(p)
for filenames3 in image_base_checker:
        print(filenames3)
        #extracting the frames from video
        cap = cv2.VideoCapture(filenames3)
        ret, frame1 = cap.read()
        #changing the frame to grayscale
        prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)
        hsv = np.zeros_like(frame1)
        hsv[...,1] = 255
        ret, frame2 = cap.read()
        next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
        i=0
        #defining paths to store frames and optical low        
        path=str(filenames3)+'_frames'
        path1=str(filenames3)+'_flow'
        while(1):
            ret, frame2 = cap.read()
            j=str(i)
            if(ret == False):
                break
            next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)
            #generating the optical flow
            flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
            hsv[...,0] = ang*180/np.pi/2
            hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
            #changing the flow to grayscale
            rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)
            gray = cv2.cvtColor(rgb,cv2.COLOR_BGR2GRAY)


            #saving the generated frames and flow
            file_name = j + '.png'
            ret = cv2.imwrite(os.path.join(path, file_name), frame2)
            print(ret, os.path.join(path, file_name))     
            ret1 = cv2.imwrite(os.path.join(path1, file_name), gray)
            
            #continuing the loop till end of video
            i = i+1
            prvs = next
39/1:
# Discard the output of this cell.
#%%capture

# Install the required libraries.
!pip install tensorflow opencv-contrib-python youtube-dl moviepy pydot
!pip install git+https://github.com/TahaAnwar/pafy.git#egg=pafy
39/2:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
39/3:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
40/1:
# Discard the output of this cell.
#%%capture

# Install the required libraries.
!pip install tensorflow opencv-contrib-python youtube-dl moviepy pydot
!pip install git+https://github.com/TahaAnwar/pafy.git#egg=pafy
40/2:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
40/3:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
40/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('dataset/UCF50')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'dataset/UCF50/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'dataset/UCF50/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
40/5:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('dataset/UCF50')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'dataset/UCF50/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'dataset/UCF50/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
40/6:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('dataset/UCF50')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'dataset/UCF50/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'dataset/UCF50/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
40/7:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('dataset/UCF50')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'dataset/UCF50/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'dataset/UCF50/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
40/8:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF50"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
40/9:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
40/10:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
40/11:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
40/12:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
40/13:
# Split the Data into Train ( 75% ) and Test Set ( 25% ).
features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels,
                                                                            test_size = 0.25, shuffle = True,
                                                                            random_state = seed_constant)
40/14:
def create_convlstm_model():
    '''
    This function will construct the required convlstm model.
    Returns:
        model: It is the required constructed convlstm model.
    '''

    # We will use a Sequential model for model construction
    model = Sequential()

    # Define the Model Architecture.
    ########################################################################################################################
    
    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,
                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    #model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(Flatten()) 
    
    model.add(Dense(len(CLASSES_LIST), activation = "softmax"))
    
    ########################################################################################################################
     
    # Display the models summary.
    model.summary()
    
    # Return the constructed convlstm model.
    return model
40/15:
# Construct the required convlstm model.
convlstm_model = create_convlstm_model()

# Display the success message. 
print("Model Created Successfully!")
40/16:
# Plot the structure of the contructed model.
plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)
40/17:
# Create an Instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)

# Compile the model and specify loss function, optimizer and metrics values to the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 50, batch_size = 4,
                                                     shuffle = True, validation_split = 0.2, 
                                                     callbacks = [early_stopping_callback])
40/18:
# Create an Instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)

# Compile the model and specify loss function, optimizer and metrics values to the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 4,
                                                     shuffle = True, validation_split = 0.2, 
                                                     callbacks = [early_stopping_callback])
40/19:
# Evaluate the trained model.
model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)
40/20:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save your Model.
convlstm_model.save(model_file_name)
40/21:
def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):
    '''
    This function will plot the metrics passed to it in a graph.
    Args:
        model_training_history: A history object containing a record of training and validation 
                                loss values and metrics values at successive epochs
        metric_name_1:          The name of the first metric that needs to be plotted in the graph.
        metric_name_2:          The name of the second metric that needs to be plotted in the graph.
        plot_name:              The title of the graph.
    '''
    
    # Get metric values using metric names as identifiers.
    metric_value_1 = model_training_history.history[metric_name_1]
    metric_value_2 = model_training_history.history[metric_name_2]
    
    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.
    epochs = range(len(metric_value_1))

    # Plot the Graph.
    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)

    # Add title to the plot.
    plt.title(str(plot_name))

    # Add legend to the plot.
    plt.legend()
40/22:
# Visualize the training and validation loss metrices.
plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
40/23:
# Visualize the training and validation accuracy metrices.
plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
40/24:
def create_LRCN_model():
    '''
    This function will construct the required LRCN model.
    Returns:
        model: It is the required constructed LRCN model.
    '''

    # We will use a Sequential model for model construction.
    model = Sequential()
    
    # Define the Model Architecture.
    ########################################################################################################################
    
    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),
                              input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))
    
    model.add(TimeDistributed(MaxPooling2D((4, 4)))) 
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((4, 4))))
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    #model.add(TimeDistributed(Dropout(0.25)))
                                      
    model.add(TimeDistributed(Flatten()))
                                      
    model.add(LSTM(32))
                                      
    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))

    ########################################################################################################################

    # Display the models summary.
    model.summary()
    
    # Return the constructed LRCN model.
    return model
40/25:
# Construct the required LRCN model.
LRCN_model = create_LRCN_model()

# Display the success message.
print("Model Created Successfully!")
40/26:
# Plot the structure of the contructed LRCN model.
plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)
40/27:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LRCN_model_training_history = LRCN_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 4 ,
                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])
40/28:
# Evaluate the trained model.
model_evaluation_history = LRCN_model.evaluate(features_test, labels_test)
40/29:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save the Model.
LRCN_model.save(model_file_name)
40/30:
# Visualize the training and validation loss metrices.
plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
40/31:
# Visualize the training and validation accuracy metrices.
plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
40/32:
def download_youtube_videos(youtube_video_url, output_directory):
     '''
    This function downloads the youtube video whose URL is passed to it as an argument.
    Args:
        youtube_video_url: URL of the video that is required to be downloaded.
        output_directory:  The directory path to which the video needs to be stored after downloading.
    Returns:
        title: The title of the downloaded youtube video.
    '''
 
     # Create a video object which contains useful information about the video.
     video = pafy.new(youtube_video_url)
 
     # Retrieve the title of the video.
     title = video.title
 
     # Get the best available quality object for the video.
     video_best = video.getbest()
 
     # Construct the output file path.
     output_file_path = f'{output_directory}/{title}.mp4'
 
     # Download the youtube video at the best available quality and store it to the contructed path.
     video_best.download(filepath = output_file_path, quiet = True)
 
     # Return the video title.
     return title
40/33:
# Make the Output directory if it does not exist
test_videos_directory = 'test_videos'
os.makedirs(test_videos_directory, exist_ok = True)

# Download a YouTube Video.
video_title = download_youtube_videos('https://www.youtube.com/watch?v=8u0qjmHIOcE', test_videos_directory)

# Get the YouTube Video's path we just downloaded.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
40/34:
# Make the Output directory if it does not exist
test_videos_directory = 'test_videos'
os.makedirs(test_videos_directory, exist_ok = True)

# Download a YouTube Video.
video_title = download_youtube_videos('https://www.youtube.com/watch?v=8u0qjmHIOcE', test_videos_directory)

# Get the YouTube Video's path we just downloaded.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
40/35:
# Make the Output directory if it does not exist
test_videos_directory = 'test_videos'
os.makedirs(test_videos_directory, exist_ok = True)

# Download a YouTube Video.
# video_title = download_youtube_videos('https://www.youtube.com/watch?v=8u0qjmHIOcE', test_videos_directory)
video_title = 'v-applyeyemakeup-g20-c06_Yobm6Rq9'
# Get the YouTube Video's path we just downloaded.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
40/36:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
40/37:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
41/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
41/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
41/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('dataset/UCF50')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'dataset/UCF50/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'dataset/UCF50/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
41/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('dataset/UCF50')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
41/5:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('dataset/UCF50')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
41/6:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
41/7:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
41/8:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('dataset/UCF50')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
41/9:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('dataset/UCF50')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
41/10:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
41/11:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
41/12:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF50"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
41/13:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
41/14:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
41/15:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
41/16:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
41/17:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
41/18:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
41/19:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
41/20:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
41/21:
# Split the Data into Train ( 75% ) and Test Set ( 25% ).
features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels,
                                                                            test_size = 0.25, shuffle = True,
                                                                            random_state = seed_constant)
41/22:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
41/23:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
41/24:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
41/25:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
41/26:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
41/27:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
41/28:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
41/29:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
41/30:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
41/31:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
41/32:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
41/33:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
41/34:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
41/35:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
41/36:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
41/37:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
41/38:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
41/39:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
41/40:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
41/41:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
41/42:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
41/43:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
41/44:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
41/45:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
41/46:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
41/47:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
41/48:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
41/49:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
42/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
42/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
42/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
42/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
42/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
42/6:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
42/7:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
42/8:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
42/9:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
43/1:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Pad or resize optical flow vectors to a fixed length
            padded_optical_flow = pad_or_resize_optical_flow(optical_flow, SEQUENCE_LENGTH)

            # Append data to lists
            features.append(padded_optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths

def pad_or_resize_optical_flow(optical_flow, sequence_length):
    '''
    This function pads or resizes optical flow vectors to a fixed length.
    Args:
        optical_flow:     A list containing the computed optical flow vectors.
        sequence_length:  The fixed length to pad or resize the optical flow vectors.
    Returns:
        padded_optical_flow: A numpy array containing padded or resized optical flow vectors.
    '''
    padded_optical_flow = []

    # Pad or resize optical flow vectors to sequence_length
    for flow in optical_flow:
        # Resize optical flow vector to sequence_length
        if len(flow) < sequence_length:
            # Resize optical flow vector using interpolation
            resized_flow = cv2.resize(flow, (sequence_length, flow.shape[1]), interpolation=cv2.INTER_LINEAR)
            padded_optical_flow.append(resized_flow)
        # Trim optical flow vector if longer than sequence_length
        elif len(flow) > sequence_length:
            trimmed_flow = flow[:sequence_length]
            padded_optical_flow.append(trimmed_flow)
        else:
            padded_optical_flow.append(flow)

    return np.asarray(padded_optical_flow)
43/2:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
43/3:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
43/4:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
43/5:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
43/6:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Pad or resize optical flow vectors to a fixed length
            padded_optical_flow = pad_or_resize_optical_flow(optical_flow, SEQUENCE_LENGTH)

            # Append data to lists
            features.append(padded_optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths

def pad_or_resize_optical_flow(optical_flow, sequence_length):
    '''
    This function pads or resizes optical flow vectors to a fixed length.
    Args:
        optical_flow:     A list containing the computed optical flow vectors.
        sequence_length:  The fixed length to pad or resize the optical flow vectors.
    Returns:
        padded_optical_flow: A numpy array containing padded or resized optical flow vectors.
    '''
    padded_optical_flow = []

    # Pad or resize optical flow vectors to sequence_length
    for flow in optical_flow:
        # Resize optical flow vector to sequence_length
        if len(flow) < sequence_length:
            # Resize optical flow vector using interpolation
            resized_flow = cv2.resize(flow, (sequence_length, flow.shape[1]), interpolation=cv2.INTER_LINEAR)
            padded_optical_flow.append(resized_flow)
        # Trim optical flow vector if longer than sequence_length
        elif len(flow) > sequence_length:
            trimmed_flow = flow[:sequence_length]
            padded_optical_flow.append(trimmed_flow)
        else:
            padded_optical_flow.append(flow)

    return np.asarray(padded_optical_flow)
43/7:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
43/8:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
43/9:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
43/10:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
43/11:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
43/12:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
43/13:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
43/14:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
44/1:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
44/2:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
44/3:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
44/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
44/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
44/6:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
44/7:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
44/8:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
45/1:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
45/2:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
45/3:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
45/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
45/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
45/6:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
45/7:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
45/8:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
45/9:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
45/10:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
45/11:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
45/12:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
45/13:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
45/14:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
45/15:
def create_dataset_with_optical_flow(batch_size=10):
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Args:
        batch_size: The number of videos to process in each batch.
    Returns:
        features: A list containing the extracted frames of the videos.
        labels: A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all the classes mentioned in the classes list.
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through the files in batches.
        for i in range(0, len(files_list), batch_size):
            batch_files = files_list[i:i+batch_size]
            batch_features = []
            batch_labels = []
            
            # Iterate through the files in the current batch.
            for file_name in batch_files:
                
                # Get the complete video path.
                video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

                # Extract the frames of the video file.
                frames = frames_extraction_with_optical_flow(video_file_path)

                # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
                # So ignore the videos having frames less than the SEQUENCE_LENGTH.
                if len(frames) == SEQUENCE_LENGTH:

                    # Append the data to their respective lists.
                    batch_features.append(frames)
                    batch_labels.append(class_index)
                    video_files_paths.append(video_file_path)
            
            # Convert lists to numpy arrays
            batch_features = np.asarray(batch_features)
            batch_labels = np.array(batch_labels)
            
            # Append the batch features and labels to the main lists
            features.append(batch_features)
            labels.append(batch_labels)
    
    # Concatenate lists to create final features and labels arrays
    features = np.concatenate(features, axis=0)
    labels = np.concatenate(labels, axis=0)
    
    return features, labels, video_files_paths

def frames_extraction_with_optical_flow(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    cap = cv2.VideoCapture(video_path)
    ret, prev_frame = cap.read()
    
    # Iterate through the Video Frames.
    while ret:
        ret, next_frame = cap.read()
        if not ret:
            break
            
        # Resize the frames to a fixed height and width.
        prev_frame = cv2.resize(prev_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        next_frame = cv2.resize(next_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Convert frames to grayscale
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
        next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Append optical flow to frames list
        frames_list.append(flow)
        
        # Update previous frame
        prev_frame = next_frame
        
    # Release the VideoCapture object. 
    cap.release()

    # Return the frames list.
    return frames_list
45/16:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
45/17:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
45/18:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
45/19:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
45/20:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
45/21:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
45/22:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
45/23:
def create_dataset_with_optical_flow(batch_size=10):
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Args:
        batch_size: The number of videos to process in each batch.
    Returns:
        features: A list containing the extracted frames of the videos.
        labels: A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all the classes mentioned in the classes list.
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')  # Added print statement
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through the files in batches.
        for i in range(0, len(files_list), batch_size):
            batch_files = files_list[i:i+batch_size]
            batch_features = []
            batch_labels = []
            print(f'Processing batch {i//batch_size + 1}/{len(files_list)//batch_size + 1}')  # Added print statement
            
            # Iterate through the files in the current batch.
            for file_name in batch_files:
                
                # Get the complete video path.
                video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

                # Extract the frames of the video file.
                frames = frames_extraction_with_optical_flow(video_file_path)

                # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
                # So ignore the videos having frames less than the SEQUENCE_LENGTH.
                if len(frames) == SEQUENCE_LENGTH:

                    # Append the data to their respective lists.
                    batch_features.append(frames)
                    batch_labels.append(class_index)
                    video_files_paths.append(video_file_path)
            
            # Convert lists to numpy arrays
            batch_features = np.asarray(batch_features)
            batch_labels = np.array(batch_labels)
            
            # Append the batch features and labels to the main lists
            features.append(batch_features)
            labels.append(batch_labels)
    
    # Concatenate lists to create final features and labels arrays
    features = np.concatenate(features, axis=0)
    labels = np.concatenate(labels, axis=0)
    
    return features, labels, video_files_paths
45/24:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
45/25:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
45/26:
def create_dataset_with_optical_flow(batch_size=10):
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Args:
        batch_size: The number of videos to process in each batch.
    Returns:
        features: A list containing the extracted frames of the videos.
        labels: A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all the classes mentioned in the classes list.
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')  # Added print statement
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through the files in batches.
        for i in range(0, len(files_list), batch_size):
            batch_files = files_list[i:i+batch_size]
            batch_features = []
            batch_labels = []
            print(f'Processing batch {i//batch_size + 1}/{len(files_list)//batch_size + 1}')  # Added print statement
            
            # Iterate through the files in the current batch.
            for file_name in batch_files:
                
                # Get the complete video path.
                video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

                # Extract the frames of the video file.
                frames = frames_extraction_with_optical_flow(video_file_path)

                # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
                # So ignore the videos having frames less than the SEQUENCE_LENGTH.
                if len(frames) == SEQUENCE_LENGTH:

                    # Append the data to their respective lists.
                    batch_features.append(frames)
                    batch_labels.append(class_index)
                    video_files_paths.append(video_file_path)
            
            # Convert lists to numpy arrays
            batch_features = np.asarray(batch_features)
            batch_labels = np.array(batch_labels)
            print(f'Batch Features shape: {batch_features.shape}')  # Added print statement
            print(f'Batch Labels shape: {batch_labels.shape}')  # Added print statement
            
            # Append the batch features and labels to the main lists
            features.append(batch_features)
            labels.append(batch_labels)
    
    # Concatenate lists to create final features and labels arrays
    features = np.concatenate(features, axis=0)
    labels = np.concatenate(labels, axis=0)
    print(f'Final Features shape: {features.shape}')  # Added print statement
    print(f'Final Labels shape: {labels.shape}')  # Added print statement
    
    return features, labels, video_files_paths
45/27:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
45/28:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
45/29:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
47/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
47/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
47/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
47/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
47/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
47/6:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
47/7:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
47/8:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
49/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
49/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
49/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
49/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
49/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyLipstick", "Archery", "BabyCrawling", "BalanceBeam"]
49/6:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
49/7:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
49/8:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
50/1:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
50/2:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
50/3:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
50/4:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
50/5:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
50/6:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
50/7:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
50/8:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
50/9:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
50/10:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
50/11:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
50/12:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
50/13:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
51/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
51/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
51/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
51/4:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
51/5:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
51/6:
def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through video files
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Compute optical flow
            optical_flow = compute_optical_flow(video_file_path)

            # Check if the optical flow list is empty
            if len(optical_flow) == 0:
                continue
            
            # Append data to lists
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
51/7:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
51/8:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
51/9:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
51/10:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
51/11:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
51/12:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
51/13:
def create_dataset_with_optical_flow(batch_size=10):
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Args:
        batch_size: The number of videos to process in each batch.
    Returns:
        features: A list containing the extracted frames of the videos.
        labels: A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all the classes mentioned in the classes list.
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')  # Added print statement
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through the files in batches.
        for i in range(0, len(files_list), batch_size):
            batch_files = files_list[i:i+batch_size]
            batch_features = []
            batch_labels = []
            print(f'Processing batch {i//batch_size + 1}/{len(files_list)//batch_size + 1}')  # Added print statement
            
            # Iterate through the files in the current batch.
            for file_name in batch_files:
                
                # Get the complete video path.
                video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

                # Extract the frames of the video file.
                frames = frames_extraction_with_optical_flow(video_file_path)

                # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
                # So ignore the videos having frames less than the SEQUENCE_LENGTH.
                if len(frames) == SEQUENCE_LENGTH:

                    # Append the data to their respective lists.
                    batch_features.append(frames)
                    batch_labels.append(class_index)
                    video_files_paths.append(video_file_path)
            
            # Convert lists to numpy arrays
            batch_features = np.asarray(batch_features)
            batch_labels = np.array(batch_labels)
            print(f'Batch Features shape: {batch_features.shape}')  # Added print statement
            print(f'Batch Labels shape: {batch_labels.shape}')  # Added print statement
            
            # Append the batch features and labels to the main lists
            features.append(batch_features)
            labels.append(batch_labels)
    
    # Concatenate lists to create final features and labels arrays
    features = np.concatenate(features, axis=0)
    labels = np.concatenate(labels, axis=0)
    print(f'Final Features shape: {features.shape}')  # Added print statement
    print(f'Final Labels shape: {labels.shape}')  # Added print statement
    
    return features, labels, video_files_paths
51/14:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
51/15:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
51/16:
def create_dataset_with_optical_flow(batch_size=10):
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Args:
        batch_size: The number of videos to process in each batch.
    Returns:
        features: A list containing the extracted frames of the videos.
        labels: A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all the classes mentioned in the classes list.
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')  # Added print statement
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through the files in batches.
        for i in range(0, len(files_list), batch_size):
            batch_files = files_list[i:i+batch_size]
            batch_features = []
            batch_labels = []
            print(f'Processing batch {i//batch_size + 1}/{len(files_list)//batch_size + 1}')  # Added print statement
            
            # Iterate through the files in the current batch.
            for file_name in batch_files:
                
                # Get the complete video path.
                video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

                # Extract the frames of the video file.
                frames = frames_extraction_with_optical_flow(video_file_path)

                # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
                # So ignore the videos having frames less than the SEQUENCE_LENGTH.
                if len(frames) == SEQUENCE_LENGTH:

                    # Append the data to their respective lists.
                    batch_features.append(frames)
                    batch_labels.append(class_index)
                    video_files_paths.append(video_file_path)
            
            # Convert lists to numpy arrays
            batch_features = np.asarray(batch_features)
            batch_labels = np.array(batch_labels)
            print(f'Batch Features shape: {batch_features.shape}')  # Added print statement
            print(f'Batch Labels shape: {batch_labels.shape}')  # Added print statement
            
            # Append the batch features and labels to the main lists
            features.append(batch_features)
            labels.append(batch_labels)
    
    # Concatenate lists to create final features and labels arrays
    features = np.concatenate(features, axis=0)
    labels = np.concatenate(labels, axis=0)
    print(f'Final Features shape: {features.shape}')  # Added print statement
    print(f'Final Labels shape: {labels.shape}')  # Added print statement
    
    return features, labels, video_files_paths

def frames_extraction_with_optical_flow(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    cap = cv2.VideoCapture(video_path)
    ret, prev_frame = cap.read()
    
    # Iterate through the Video Frames.
    while ret:
        ret, next_frame = cap.read()
        if not ret:
            break
            
        # Resize the frames to a fixed height and width.
        prev_frame = cv2.resize(prev_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        next_frame = cv2.resize(next_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Convert frames to grayscale
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
        next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Append optical flow to frames list
        frames_list.append(flow)
        
        # Update previous frame
        prev_frame = next_frame
        
    # Release the VideoCapture object. 
    cap.release()

    # Return the frames list.
    return frames_list
51/17:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
51/18:
def create_dataset_with_optical_flow(batch_size=10):
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Args:
        batch_size: The number of videos to process in each batch.
    Returns:
        features: A list containing the extracted frames of the videos.
        labels: A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all the classes mentioned in the classes list.
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')  # Added print statement
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through the files in batches.
        for i in range(0, len(files_list), batch_size):
            batch_files = files_list[i:i+batch_size]
            batch_features = []
            batch_labels = []
            print(f'Processing batch {i//batch_size + 1}/{len(files_list)//batch_size + 1}')  # Added print statement
            
            # Iterate through the files in the current batch.
            for file_name in batch_files:
                
                # Get the complete video path.
                video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

                # Extract the frames of the video file.
                frames = frames_extraction_with_optical_flow(video_file_path)

                # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
                # So ignore the videos having frames less than the SEQUENCE_LENGTH.
                if len(frames) == SEQUENCE_LENGTH:

                    # Append the data to their respective lists.
                    batch_features.append(frames)
                    batch_labels.append(class_index)
                    video_files_paths.append(video_file_path)
            
            if batch_features and batch_labels:  # Check if the batch contains valid data
                # Convert lists to numpy arrays
                batch_features = np.asarray(batch_features)
                batch_labels = np.array(batch_labels)
                print(f'Batch Features shape: {batch_features.shape}')  # Added print statement
                print(f'Batch Labels shape: {batch_labels.shape}')  # Added print statement

                # Append the batch features and labels to the main lists
                features.append(batch_features)
                labels.append(batch_labels)
    
    # Concatenate lists to create final features and labels arrays
    features = np.concatenate(features, axis=0)
    labels = np.concatenate(labels, axis=0)
    print(f'Final Features shape: {features.shape}')  # Added print statement
    print(f'Final Labels shape: {labels.shape}')  # Added print statement
    
    return features, labels, video_files_paths


def frames_extraction_with_optical_flow(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    cap = cv2.VideoCapture(video_path)
    ret, prev_frame = cap.read()
    
    # Iterate through the Video Frames.
    while ret:
        ret, next_frame = cap.read()
        if not ret:
            break
            
        # Resize the frames to a fixed height and width.
        prev_frame = cv2.resize(prev_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        next_frame = cv2.resize(next_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Convert frames to grayscale
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
        next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Append optical flow to frames list
        frames_list.append(flow)
        
        # Update previous frame
        prev_frame = next_frame
        
    # Release the VideoCapture object. 
    cap.release()

    # Return the frames list.
    return frames_list
51/19:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
51/20:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
51/21:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
51/22:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
51/23:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Read the first frame
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    # Iterate through the frames
    while True:
        # Read the next frame
        _, frame = video_reader.read()
        if frame is None:
            break

        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
51/24:
import cv2
import numpy as np
import os
from glob import glob

def create_dataset_with_optical_flow():
    '''
    This function creates the dataset using optical flow between consecutive frames.
    Returns:
        features:          A list containing the computed optical flow vectors.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declare Empty Lists
    features = []
    labels = []
    video_files_paths = []
    
    # Iterate through all classes
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        
        # Get video files in class directory
        files_list = glob(os.path.join(DATASET_DIR, class_name, '*.avi'))
        
        # Iterate through video files
        for video_file_path in files_list:
            print(f'Processing video: {video_file_path}')

            # Extract frames directory path
            frames_dir_path = video_file_path.replace('.avi', '_frames')

            # Create frames directory if it doesn't exist
            os.makedirs(frames_dir_path, exist_ok=True)

            # Extract optical flow directory path
            flow_dir_path = video_file_path.replace('.avi', '_flow')

            # Create optical flow directory if it doesn't exist
            os.makedirs(flow_dir_path, exist_ok=True)

            # Initialize VideoCapture object
            video_reader = cv2.VideoCapture(video_file_path)

            # Read the first frame
            ret, prev_frame = video_reader.read()
            prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

            # Frame counter
            frame_count = 0

            # Iterate through the frames
            while ret:
                # Convert frame to grayscale
                gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

                # Compute optical flow
                flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

                # Normalize optical flow vectors
                flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)

                # Save the optical flow frame
                frame_path = os.path.join(flow_dir_path, f'{frame_count}.png')
                cv2.imwrite(frame_path, flow_norm)

                # Save the original frame
                frame_path = os.path.join(frames_dir_path, f'{frame_count}.png')
                cv2.imwrite(frame_path, prev_frame)

                # Increment frame counter
                frame_count += 1

                # Read the next frame
                ret, prev_frame = video_reader.read()
                prev_gray = gray

            # Release VideoCapture object
            video_reader.release()

            # Append data to lists
            features.append(flow_dir_path)  # Append optical flow directory path
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    # Convert lists to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
51/25:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
51/26:
import cv2
import numpy as np
import os
from glob import glob

def create_dataset_with_optical_flow():
    features = []
    labels = []
    video_files_paths = []
    
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)
            optical_flow = compute_optical_flow(video_file_path)
            if len(optical_flow) == 0:
                continue
            
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
51/27:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
51/28:
def compute_optical_flow(video_path):
    optical_flow_list = []
    video_reader = cv2.VideoCapture(video_path)
    _, prev_frame = video_reader.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

    while True:
        _, frame = video_reader.read()
        if frame is None:
            break
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        mag, _ = cv2.cartToPolar(flow[...,0], flow[...,1])
        flow_norm = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)
        flow_norm = flow_norm.astype(np.uint8)  # Convert to uint8
        optical_flow_list.append(flow_norm)
        prev_gray = gray

    video_reader.release()
    return optical_flow_list
51/29:
import cv2
import numpy as np
import os
from glob import glob

def create_dataset_with_optical_flow():
    features = []
    labels = []
    video_files_paths = []
    
    for class_index, class_name in enumerate(CLASSES_LIST):
        print(f'Extracting Data of Class: {class_name}')
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        for file_name in files_list:
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)
            optical_flow = compute_optical_flow(video_file_path)
            if len(optical_flow) == 0:
                continue
            
            features.append(optical_flow)
            labels.append(class_index)
            video_files_paths.append(video_file_path)

    features = np.asarray(features)
    labels = np.array(labels)  

    return features, labels, video_files_paths
51/30:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
51/31:
def compute_optical_flow(video_path):
    '''
    This function computes the optical flow between consecutive frames of a video.
    Args:
        video_path: The path of the video file.
    Returns:
        optical_flow_list: A list containing the optical flow vectors between consecutive frames.
    '''

    # Initialize optical flow list
    optical_flow_list = []

    # Initialize VideoCapture object
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)
    
    # Iterate through the frames
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Read the next frame
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Convert frame to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Compute optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)

        # Normalize optical flow vectors
        flow_norm = cv2.normalize(flow, None, 0, 255, cv2.NORM_MINMAX)
        optical_flow_list.append(flow_norm)

        # Update previous frame
        prev_gray = gray

    # Release VideoCapture object
    video_reader.release()

    return optical_flow_list
51/32:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
51/33:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
51/34:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
51/35:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after calculating optical flow, resizing, and normalizing them.
    Args:
        video_path: The path of the video on the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(1, SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
51/36:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset using optical flow.
    Returns:
        features:          A list containing the extracted optical flow frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos on the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the optical flow frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the videos having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their respective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
51/37:
# Create the dataset.
features, labels, video_files_paths = create_dataset_with_optical_flow()
51/38:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
51/39:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
51/40:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
51/41:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
51/42:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
51/43:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
52/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
52/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
52/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
52/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
52/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
52/6:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
52/7:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
52/8:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
53/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
53/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
53/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
53/4:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
53/5:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
53/6:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
53/7:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
53/8:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
53/9:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
53/10:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
53/11:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
53/12:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
54/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
54/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
54/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
54/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
54/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
54/6:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
54/7:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
54/8:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
54/9:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
54/10:
# Split the Data into Train ( 75% ) and Test Set ( 25% ).
features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels,
                                                                            test_size = 0.25, shuffle = True,
                                                                            random_state = seed_constant)
54/11:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after calculating optical flow, resizing, and normalizing them.
    Args:
        video_path: The path of the video on the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''
    print(f"Processing video: {video_path}")
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(1, SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
54/12:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset using optical flow.
    Returns:
        features:          A list containing the extracted optical flow frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos on the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        class_dir = os.path.join(DATASET_DIR, class_name)
        if not os.path.exists(class_dir):
            print(f"Error: Directory {class_dir} does not exist")
            continue
        
        files_list = os.listdir(class_dir)
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(class_dir, file_name)

            # Extract the optical flow frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the videos having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:
                # Append the data to their respective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)
            else:
                print(f"Warning: Video {video_file_path} does not have enough frames ({len(frames)} frames extracted)")

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
54/13:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
54/14:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
54/15:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
54/16:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
54/17:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
54/18:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 15

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
54/19:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 19

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
54/20:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after calculating optical flow, resizing, and normalizing them.
    Args:
        video_path: The path of the video on the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''
    print(f"Processing video: {video_path}")
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(1, SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
54/21:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset using optical flow.
    Returns:
        features:          A list containing the extracted optical flow frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos on the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        class_dir = os.path.join(DATASET_DIR, class_name)
        if not os.path.exists(class_dir):
            print(f"Error: Directory {class_dir} does not exist")
            continue
        
        files_list = os.listdir(class_dir)
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(class_dir, file_name)

            # Extract the optical flow frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the videos having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:
                # Append the data to their respective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)
            else:
                print(f"Warning: Video {video_file_path} does not have enough frames ({len(frames)} frames extracted)")

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
54/22:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
54/23:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
54/24:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after calculating optical flow, resizing, and normalizing them.
    Args:
        video_path: The path of the video on the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''
    print(f"Processing video: {video_path}")
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(1, SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
54/25:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset using optical flow.
    Returns:
        features:          A list containing the extracted optical flow frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos on the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        class_dir = os.path.join(DATASET_DIR, class_name)
        if not os.path.exists(class_dir):
            print(f"Error: Directory {class_dir} does not exist")
            continue
        
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the optical flow frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the videos having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:
                # Append the data to their respective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)
            else:
                print(f"Warning: Video {video_file_path} does not have enough frames ({len(frames)} frames extracted)")

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
54/26:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
54/27:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset using optical flow.
    Returns:
        features:          A list containing the extracted optical flow frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos on the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        class_dir = os.path.join(DATASET_DIR, class_name)
        if not os.path.exists(class_dir):
            print(f"Error: Directory {class_dir} does not exist")
            continue
        
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the optical flow frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the videos having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:
                # Append the data to their respective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
54/28:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
54/29:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
54/30:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after calculating optical flow, resizing, and normalizing them.
    Args:
        video_path: The path of the video on the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''
    print(f"Processing video: {video_path}")
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
54/31:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset using optical flow.
    Returns:
        features:          A list containing the extracted optical flow frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos on the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        class_dir = os.path.join(DATASET_DIR, class_name)
        if not os.path.exists(class_dir):
            print(f"Error: Directory {class_dir} does not exist")
            continue
        
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the optical flow frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the videos having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:
                # Append the data to their respective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)
            else:
                print(f"Warning: Video {video_file_path} does not have enough frames ({len(frames)} frames extracted)")

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
54/32:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
54/33:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
54/34:
# Split the Data into Train ( 75% ) and Test Set ( 25% ).
features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels,
                                                                            test_size = 0.25, shuffle = True,
                                                                            random_state = seed_constant)
54/35:
def create_convlstm_model():
    '''
    This function will construct the required convlstm model.
    Returns:
        model: It is the required constructed convlstm model.
    '''

    # We will use a Sequential model for model construction
    model = Sequential()

    # Define the Model Architecture.
    ########################################################################################################################
    
    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,
                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    #model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(Flatten()) 
    
    model.add(Dense(len(CLASSES_LIST), activation = "softmax"))
    
    ########################################################################################################################
     
    # Display the models summary.
    model.summary()
    
    # Return the constructed convlstm model.
    return model
54/36:
# Construct the required convlstm model.
convlstm_model = create_convlstm_model()

# Display the success message. 
print("Model Created Successfully!")
54/37:
# Plot the structure of the contructed model.
plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)
54/38:
# Create an Instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)

# Compile the model and specify loss function, optimizer and metrics values to the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 4,
                                                     shuffle = True, validation_split = 0.2, 
                                                     callbacks = [early_stopping_callback])
54/39:
# Create an Instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)

# Compile the model and specify loss function, optimizer and metrics values to the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 4,
                                                     shuffle = True, validation_split = 0.2, 
                                                     callbacks = [early_stopping_callback])
54/40:
# Plot the structure of the contructed model.
plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)
54/41:
# Plot the structure of the contructed model.
plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)
54/42:
# Plot the structure of the contructed model.
# plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)
54/43:
# Create an Instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)

# Compile the model and specify loss function, optimizer and metrics values to the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 4,
                                                     shuffle = True, validation_split = 0.2, 
                                                     callbacks = [early_stopping_callback])
54/44:
def create_convlstm_model():
    '''
    This function will construct the required convlstm model.
    Returns:
        model: It is the required constructed convlstm model.
    '''

    # We will use a Sequential model for model construction
    model = Sequential()

    # Define the Model Architecture.
    ########################################################################################################################
    
    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,
                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    #model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(Flatten()) 
    
    model.add(Dense(len(CLASSES_LIST), activation = "softmax"))
    
    ########################################################################################################################
     
    # Display the models summary.
    model.summary()
    
    # Return the constructed convlstm model.
    return model
54/45:
# Construct the required convlstm model.
convlstm_model = create_convlstm_model()

# Display the success message. 
print("Model Created Successfully!")
54/46:
# Plot the structure of the contructed model.
# plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)
54/47:
# Create an Instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)

# Compile the model and specify loss function, optimizer and metrics values to the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 4,
                                                     shuffle = True, validation_split = 0.2, 
                                                     callbacks = [early_stopping_callback])
54/48:
def create_convlstm_model():
    '''
    This function will construct the required convlstm model.
    Returns:
        model: It is the required constructed convlstm model.
    '''

    # We will use a Sequential model for model construction
    model = Sequential()

    # Define the Model Architecture.
    ########################################################################################################################
    
    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,
                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 3)))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    #model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(Flatten()) 
    
    model.add(Dense(len(CLASSES_LIST), activation = "softmax"))
    
    ########################################################################################################################
     
    # Display the models summary.
    model.summary()
    
    # Return the constructed convlstm model.
    return model
54/49:
# Construct the required convlstm model.
convlstm_model = create_convlstm_model()

# Display the success message. 
print("Model Created Successfully!")
54/50:
# Plot the structure of the contructed model.
# plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)
54/51:
# Create an Instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)

# Compile the model and specify loss function, optimizer and metrics values to the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 4,
                                                     shuffle = True, validation_split = 0.2, 
                                                     callbacks = [early_stopping_callback])
54/52:
def create_convlstm_model():
    '''
    This function will construct the required convlstm model.
    Returns:
        model: It is the required constructed convlstm model.
    '''

    # We will use a Sequential model for model construction
    model = Sequential()

    # Define the Model Architecture.
    ########################################################################################################################
    
    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,
                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 1)))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    #model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(Flatten()) 
    
    model.add(Dense(len(CLASSES_LIST), activation = "softmax"))
    
    ########################################################################################################################
     
    # Display the models summary.
    model.summary()
    
    # Return the constructed convlstm model.
    return model
54/53:
# Construct the required convlstm model.
convlstm_model = create_convlstm_model()

# Display the success message. 
print("Model Created Successfully!")
54/54:
# Plot the structure of the contructed model.
# plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)
54/55:
# Create an Instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)

# Compile the model and specify loss function, optimizer and metrics values to the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 4,
                                                     shuffle = True, validation_split = 0.2, 
                                                     callbacks = [early_stopping_callback])
54/56:
# Evaluate the trained model.
model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)
54/57:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save your Model.
convlstm_model.save(model_file_name)
54/58:
# Visualize the training and validation loss metrices.
plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
54/59:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save your Model.
convlstm_model.save(model_file_name)
54/60:
# Visualize the training and validation loss metrices.
plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
54/61:
def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):
    '''
    This function will plot the metrics passed to it in a graph.
    Args:
        model_training_history: A history object containing a record of training and validation 
                                loss values and metrics values at successive epochs
        metric_name_1:          The name of the first metric that needs to be plotted in the graph.
        metric_name_2:          The name of the second metric that needs to be plotted in the graph.
        plot_name:              The title of the graph.
    '''
    
    # Get metric values using metric names as identifiers.
    metric_value_1 = model_training_history.history[metric_name_1]
    metric_value_2 = model_training_history.history[metric_name_2]
    
    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.
    epochs = range(len(metric_value_1))

    # Plot the Graph.
    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)

    # Add title to the plot.
    plt.title(str(plot_name))

    # Add legend to the plot.
    plt.legend()
54/62:
# Visualize the training and validation loss metrices.
plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
54/63:
# Visualize the training and validation accuracy metrices.
plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
54/64:
def create_LRCN_model():
    '''
    This function will construct the required LRCN model.
    Returns:
        model: It is the required constructed LRCN model.
    '''

    # We will use a Sequential model for model construction.
    model = Sequential()
    
    # Define the Model Architecture.
    ########################################################################################################################
    
    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),
                              input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))
    
    model.add(TimeDistributed(MaxPooling2D((4, 4)))) 
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((4, 4))))
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    #model.add(TimeDistributed(Dropout(0.25)))
                                      
    model.add(TimeDistributed(Flatten()))
                                      
    model.add(LSTM(32))
                                      
    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))

    ########################################################################################################################

    # Display the models summary.
    model.summary()
    
    # Return the constructed LRCN model.
    return model
54/65:
# Construct the required LRCN model.
LRCN_model = create_LRCN_model()

# Display the success message.
print("Model Created Successfully!")
54/66:
# Plot the structure of the contructed LRCN model.
plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)
54/67:
# Plot the structure of the contructed LRCN model.
# plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)
54/68:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LRCN_model_training_history = LRCN_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 4 ,
                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])
54/69:
def create_LRCN_model():
    '''
    This function will construct the required LRCN model.
    Returns:
        model: It is the required constructed LRCN model.
    '''

    # We will use a Sequential model for model construction.
    model = Sequential()
    
    # Define the Model Architecture.
    ########################################################################################################################
    
    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),
                              input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1)))
    
    model.add(TimeDistributed(MaxPooling2D((4, 4)))) 
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((4, 4))))
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    #model.add(TimeDistributed(Dropout(0.25)))
                                      
    model.add(TimeDistributed(Flatten()))
                                      
    model.add(LSTM(32))
                                      
    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))

    ########################################################################################################################

    # Display the models summary.
    model.summary()
    
    # Return the constructed LRCN model.
    return model
54/70:
# Construct the required LRCN model.
LRCN_model = create_LRCN_model()

# Display the success message.
print("Model Created Successfully!")
54/71:
# Plot the structure of the contructed LRCN model.
# plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)
54/72:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LRCN_model_training_history = LRCN_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 4 ,
                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])
54/73:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save the Model.
LRCN_model.save(model_file_name)
54/74:
# Visualize the training and validation loss metrices.
plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
54/75:
# Visualize the training and validation accuracy metrices.
plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
54/76:
def download_youtube_videos(youtube_video_url, output_directory):
     '''
    This function downloads the youtube video whose URL is passed to it as an argument.
    Args:
        youtube_video_url: URL of the video that is required to be downloaded.
        output_directory:  The directory path to which the video needs to be stored after downloading.
    Returns:
        title: The title of the downloaded youtube video.
    '''
 
     # Create a video object which contains useful information about the video.
     video = pafy.new(youtube_video_url)
 
     # Retrieve the title of the video.
     title = video.title
 
     # Get the best available quality object for the video.
     video_best = video.getbest()
 
     # Construct the output file path.
     output_file_path = f'{output_directory}/{title}.mp4'
 
     # Download the youtube video at the best available quality and store it to the contructed path.
     video_best.download(filepath = output_file_path, quiet = True)
 
     # Return the video title.
     return title
54/77:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g20-c06_Yobm6Rq9'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
54/78:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/79:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
40/38:
from collections import deque

def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the ConvLSTM model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Calculate optical flow
        flow_frames = frames_extraction(video_file_path)
        
        # Appending the pre-processed frame into the frames list.
        frames_queue.append(flow_frames)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
40/39:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
40/40:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
40/41:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/80:
from collections import deque

def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the ConvLSTM model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Calculate optical flow
        flow_frames = frames_extraction(video_file_path)
        
        # Appending the pre-processed frame into the frames list.
        frames_queue.append(flow_frames)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/81:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/82:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the ConvLSTM model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Calculate optical flow
        flow_frames = frames_extraction(video_file_path)
        
        # Resize the optical flow frames to the desired dimensions
        resized_flow_frames = [cv2.resize(flow_frame, (IMAGE_HEIGHT, IMAGE_WIDTH)) for flow_frame in flow_frames]
        
        # Convert to numpy array and add channel dimension
        flow_frames_array = np.array(resized_flow_frames)[..., np.newaxis]
        
        # Appending the pre-processed frame into the frames list.
        frames_queue.append(flow_frames_array)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/83:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/84:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the ConvLSTM model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Calculate optical flow
        flow_frames = frames_extraction(video_file_path)
        
        # Resize the optical flow frames to the desired dimensions
        resized_flow_frames = [cv2.resize(flow_frame, (IMAGE_HEIGHT, IMAGE_WIDTH)) for flow_frame in flow_frames]
        
        # Convert to numpy array and add channel dimension
        flow_frames_array = np.array(resized_flow_frames)[..., np.newaxis]
        
        # Appending the pre-processed frame into the frames list.
        frames_queue.append(flow_frames_array)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/85:
def preprocess_optical_flow(flow_frames):
    # Resize optical flow frames to desired dimensions
    resized_flow_frames = [cv2.resize(flow_frame, (IMAGE_HEIGHT, IMAGE_WIDTH)) for flow_frame in flow_frames]
    # Convert to numpy array and add channel dimension
    flow_frames_array = np.array(resized_flow_frames)[..., np.newaxis]
    return flow_frames_array

def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the ConvLSTM model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the output video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Calculate optical flow
        flow_frames = frames_extraction(video_file_path)
        
        # Preprocess optical flow frames
        preprocessed_flow_frames = preprocess_optical_flow(flow_frames)

        # Append the pre-processed frame into the frames queue.
        frames_queue.append(preprocessed_flow_frames)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Concatenate frames along the sequence length dimension
            input_frames = np.concatenate(frames_queue, axis=0)

            # Reshape input frames to match the expected input shape of the ConvLSTM model
            input_frames = np.expand_dims(input_frames, axis=0)  # Add batch dimension

            # Pass the input frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict(input_frames)[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write the frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/86:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/87:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g20-c06_Yobm6Rq9'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
54/88:
def preprocess_optical_flow(flow_frames):
    # Resize optical flow frames to desired dimensions
    resized_flow_frames = [cv2.resize(flow_frame, (IMAGE_HEIGHT, IMAGE_WIDTH)) for flow_frame in flow_frames]
    # Convert to numpy array and add channel dimension
    flow_frames_array = np.array(resized_flow_frames)[..., np.newaxis]
    return flow_frames_array

def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the ConvLSTM model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the output video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Calculate optical flow
        flow_frames = frames_extraction(video_file_path)
        
        # Preprocess optical flow frames
        preprocessed_flow_frames = preprocess_optical_flow(flow_frames)

        # Append the pre-processed frame into the frames queue.
        frames_queue.append(preprocessed_flow_frames)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Concatenate frames along the sequence length dimension
            input_frames = np.concatenate(frames_queue, axis=0)

            # Reshape input frames to match the expected input shape of the ConvLSTM model
            input_frames = np.expand_dims(input_frames, axis=0)  # Add batch dimension

            # Pass the input frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict(input_frames)[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write the frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/89:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/90:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/91:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/92:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/93:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/94:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/95:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/96:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/97:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/98:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/99:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/100:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/101:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/102:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/103:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/104:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/105:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/106:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/107:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/108:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/109:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/110:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
        skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/111:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/112:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
        skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/113:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/114:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g20-c06_Yobm6Rq9'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
54/115:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/116:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2
                    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/117:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/118:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
        skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
    
    
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
              # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/119:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
        skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
    
    
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/120:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
        skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
    
    
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/121:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
        skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
    
    
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/122:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
        skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
    
    
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/123:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        success, previous_frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not success:
            break

        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
        skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)

        # Convert to grayscale
        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

        # Iterate through the Video Frames to calculate optical flow.
        for frame_counter in range(SEQUENCE_LENGTH):

            # Set the current frame position of the video.
            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

            # Reading the frame from the video. 
            success, current_frame = video_reader.read() 

            # Check if Video frame is not successfully read then break the loop
            if not success:
                print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
                break

            # Convert to grayscale
            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            # Calculate optical flow between the previous frame and the current frame
            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                                0.5, 3, 15, 3, 5, 1.2, 0)

            # Normalize the optical flow
            flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
            flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
            flow_magnitude = np.uint8(flow_magnitude)

            # Resize the Frame to fixed height and width.
            resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

            # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
            normalized_flow = resized_flow / 255
        
            # Append the normalized optical flow frame into the frames list
            frames_queue.append(normalized_flow)
        
            # Set the current frame to the previous frame for the next iteration
            previous_frame_gray = current_frame_gray

    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/124:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''


    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/125:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g20-c06_Yobm6Rq9'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
54/126:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''


    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/127:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/128:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after calculating optical flow, resizing, and normalizing them.
    Args:
        video_path: The path of the video on the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''
    print(f"Processing video: {video_path}")
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # # Calculate the interval after which frames will be added to the list.
    # skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    # print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
54/129:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset using optical flow.
    Returns:
        features:          A list containing the extracted optical flow frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos on the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        class_dir = os.path.join(DATASET_DIR, class_name)
        if not os.path.exists(class_dir):
            print(f"Error: Directory {class_dir} does not exist")
            continue
        
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the optical flow frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the videos having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:
                # Append the data to their respective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)
            else:
                print(f"Warning: Video {video_file_path} does not have enough frames ({len(frames)} frames extracted)")

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
54/130:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
54/131:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''


    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(current_frame_gray, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame_gray)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/132:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/133:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''


    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(normalized_flow, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(normalized_flow)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/134:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/135:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''


    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(normalized_flow, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(previous_frame)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/136:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/137:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''


    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(normalized_flow, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(previous_frame)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/138:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/139:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''


    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(normalized_flow, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(previous_frame)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/140:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/141:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4v'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/142:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/143:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/144:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/145:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/146:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/147:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/148:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/149:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Convert the queue of frames into a numpy array
            frames_array = np.array(frames_queue)
            
            # Reshape the frames array to match the input shape expected by the model
            frames_array = frames_array.reshape((1, SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
            
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(frames_array)[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/150:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/151:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Initialize predicted_class_name
        predicted_class_name = ''

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Convert the queue of frames into a numpy array
            frames_array = np.array(frames_queue)
            
            # Reshape the frames array to match the input shape expected by the model
            frames_array = frames_array.reshape((1, SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
            
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(frames_array)[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/152:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/153:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the output video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video on the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if the frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the preprocessed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue is equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Convert the queue of frames into a numpy array.
            frames_array = np.array(frames_queue)
            
            # Reshape the frames array to match the input shape expected by the model.
            frames_array = np.expand_dims(frames_array, axis=0)  # Add batch dimension
            frames_array = np.expand_dims(frames_array, axis=-1)  # Add channel dimension
            
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(frames_array)[0]

            # Get the index of the class with the highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        else:
            # If the number of frames is less than the sequence length, continue reading the next frame.
            continue

        # Write the predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write the frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/154:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the output video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video on the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if the frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the preprocessed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue is equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Convert the queue of frames into a numpy array.
            frames_array = np.array(frames_queue)
            
            # Reshape the frames array to match the input shape expected by the model.
            frames_array = np.expand_dims(frames_array, axis=0)  # Add batch dimension
            frames_array = np.expand_dims(frames_array, axis=-1)  # Add channel dimension
            
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(frames_array)[0]

            # Get the index of the class with the highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        else:
            # If the number of frames is less than the sequence length, continue reading the next frame.
            continue

        # Write the predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write the frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/155:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the output video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video on the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if the frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the preprocessed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue is equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Convert the queue of frames into a numpy array.
            frames_array = np.array(frames_queue)
            
            # Reshape the frames array to match the input shape expected by the model.
            frames_array = np.expand_dims(frames_array, axis=0)  # Add batch dimension
            frames_array = np.expand_dims(frames_array, axis=-1)  # Add channel dimension
            
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(frames_array)[0]

            # Get the index of the class with the highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        else:
            # If the number of frames is less than the sequence length, continue reading the next frame.
            continue

        # Write the predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write the frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/156:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the output video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video on the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if the frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the preprocessed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue is equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Convert the queue of frames into a numpy array.
            frames_array = np.array(frames_queue)
            
            # Reshape the frames array to match the input shape expected by the model.
            frames_array = np.expand_dims(frames_array, axis=0)  # Add batch dimension
            frames_array = np.expand_dims(frames_array, axis=-1)  # Add channel dimension
            
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(frames_array)[0]

            # Get the index of the class with the highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        else:
            # If the number of frames is less than the sequence length, continue reading the next frame.
            continue

        # Write the predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write the frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/157:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the output video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video on the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if the frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the preprocessed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue is equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Convert the queue of frames into a numpy array.
            frames_array = np.array(frames_queue)
            
            # Reshape the frames array to match the input shape expected by the model.
            frames_array = np.expand_dims(frames_array, axis=0)  # Add batch dimension
            frames_array = np.expand_dims(frames_array, axis=-1)  # Add channel dimension
            
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(frames_array)[0]

            # Get the index of the class with the highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        else:
            # If the number of frames is less than the sequence length, continue reading the next frame.
            continue

        # Write the predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write the frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/158:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/159:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the output video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video on the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if the frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the preprocessed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue is equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Convert the queue of frames into a numpy array.
            frames_array = np.array(frames_queue)
            
            # Reshape the frames array to match the input shape expected by the model.
            frames_array = np.expand_dims(frames_array, axis=0)  # Add batch dimension
            frames_array = np.expand_dims(frames_array, axis=-1)  # Add channel dimension
            
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(frames_array)[0]

            # Get the index of the class with the highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        else:
            # If the number of frames is less than the sequence length, continue reading the next frame.
            continue

        # Write the predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write the frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/160:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/161:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Convert the queue of frames into a numpy array
            frames_array = np.array(frames_queue)
            
            # Reshape the frames array to match the input shape expected by the model
            frames_array = frames_array.reshape((1, SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
            
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(frames_array)[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/162:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/163:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g20-c06_Yobm6Rq9'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
54/164:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Convert the queue of frames into a numpy array
            frames_array = np.array(frames_queue)
            
            # Reshape the frames array to match the input shape expected by the model
            frames_array = frames_array.reshape((1, SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
            
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(frames_array)[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/165:
import logging

def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the output video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Check if the video file is opened successfully.
    if not video_reader.isOpened():
        logging.error(f"Error opening video file: {video_file_path}")
        return

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video on the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Check if the video writer is initialized successfully.
    if not video_writer.isOpened():
        logging.error(f"Error initializing video writer for output file: {output_file_path}")
        video_reader.release()
        return

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if the frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the preprocessed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue is equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:
            # Convert the queue of frames into a numpy array.
            frames_array = np.array(frames_queue)
            
            # Reshape the frames array to match the input shape expected by the model.
            frames_array = np.expand_dims(frames_array, axis=0)  # Add batch dimension
            frames_array = np.expand_dims(frames_array, axis=-1)  # Add channel dimension
            
            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(frames_array)[0]

            # Get the index of the class with the highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        else:
            # If the number of frames is less than the sequence length, continue reading the next frame.
            continue

        # Write the predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write the frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()

# Set up logging configuration
logging.basicConfig(filename='action_recognition.log', level=logging.ERROR)

# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/166:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''


    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(previous_frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(previous_frame)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/167:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/168:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g20-c06_Yobm6Rq9'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
54/169:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''


    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(previous_frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(previous_frame)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/170:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/171:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''


    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_file_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(previous_frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(previous_frame)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/172:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/173:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/174:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''


    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_file_path}")
        return frames_queue

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_queue
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_queue.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
        
        # Write predicted class name on top of the frame.
        cv2.putText(current_frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(current_frame)
    
    
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/175:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/176:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/177:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/178:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/179:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/180:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
54/181:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
54/182:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
54/183:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
55/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
55/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
55/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
55/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
55/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
55/6:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after calculating optical flow, resizing, and normalizing them.
    Args:
        video_path: The path of the video on the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''
    print(f"Processing video: {video_path}")
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
55/7:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset using optical flow.
    Returns:
        features:          A list containing the extracted optical flow frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos on the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        class_dir = os.path.join(DATASET_DIR, class_name)
        if not os.path.exists(class_dir):
            print(f"Error: Directory {class_dir} does not exist")
            continue
        
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the optical flow frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the videos having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:
                # Append the data to their respective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
55/8:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
55/9:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
55/10:
# Split the Data into Train ( 75% ) and Test Set ( 25% ).
features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels,
                                                                            test_size = 0.25, shuffle = True,
                                                                            random_state = seed_constant)
55/11:
def create_convlstm_model():
    '''
    This function will construct the required convlstm model.
    Returns:
        model: It is the required constructed convlstm model.
    '''

    # We will use a Sequential model for model construction
    model = Sequential()

    # Define the Model Architecture.
    ########################################################################################################################
    
    model.add(ConvLSTM2D(filters = 4, kernel_size = (3, 3), activation = 'tanh',data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True, input_shape = (SEQUENCE_LENGTH,
                                                                                      IMAGE_HEIGHT, IMAGE_WIDTH, 1)))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 8, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 14, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(ConvLSTM2D(filters = 16, kernel_size = (3, 3), activation = 'tanh', data_format = "channels_last",
                         recurrent_dropout=0.2, return_sequences=True))
    
    model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same', data_format='channels_last'))
    #model.add(TimeDistributed(Dropout(0.2)))
    
    model.add(Flatten()) 
    
    model.add(Dense(len(CLASSES_LIST), activation = "softmax"))
    
    ########################################################################################################################
     
    # Display the models summary.
    model.summary()
    
    # Return the constructed convlstm model.
    return model
55/12:
# Construct the required convlstm model.
convlstm_model = create_convlstm_model()

# Display the success message. 
print("Model Created Successfully!")
55/13:
# Plot the structure of the contructed model.
# plot_model(convlstm_model, to_file = 'convlstm_model_structure_plot.png', show_shapes = True, show_layer_names = True)
55/14:
# Create an Instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)

# Compile the model and specify loss function, optimizer and metrics values to the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 5, batch_size = 4,
                                                     shuffle = True, validation_split = 0.2, 
                                                     callbacks = [early_stopping_callback])
55/15:
# Create an Instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)

# Compile the model and specify loss function, optimizer and metrics values to the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit(x = features_train, y = labels_train, epochs = 10, batch_size = 4,
                                                     shuffle = True, validation_split = 0.2, 
                                                     callbacks = [early_stopping_callback])
55/16:
# Evaluate the trained model.
model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)
55/17:
# Evaluate the trained model.
model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)
55/18:
# Evaluate the trained model.
model_evaluation_history = convlstm_model.evaluate(features_test, labels_test)
55/19:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save your Model.
convlstm_model.save(model_file_name)
55/20:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save your Model.
convlstm_model.save(model_file_name)
55/21:
def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):
    '''
    This function will plot the metrics passed to it in a graph.
    Args:
        model_training_history: A history object containing a record of training and validation 
                                loss values and metrics values at successive epochs
        metric_name_1:          The name of the first metric that needs to be plotted in the graph.
        metric_name_2:          The name of the second metric that needs to be plotted in the graph.
        plot_name:              The title of the graph.
    '''
    
    # Get metric values using metric names as identifiers.
    metric_value_1 = model_training_history.history[metric_name_1]
    metric_value_2 = model_training_history.history[metric_name_2]
    
    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.
    epochs = range(len(metric_value_1))

    # Plot the Graph.
    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)

    # Add title to the plot.
    plt.title(str(plot_name))

    # Add legend to the plot.
    plt.legend()
55/22:
# Visualize the training and validation loss metrices.
plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
55/23:
# Visualize the training and validation accuracy metrices.
plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
55/24:
def create_LRCN_model():
    '''
    This function will construct the required LRCN model.
    Returns:
        model: It is the required constructed LRCN model.
    '''

    # We will use a Sequential model for model construction.
    model = Sequential()
    
    # Define the Model Architecture.
    ########################################################################################################################
    
    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),
                              input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1)))
    
    model.add(TimeDistributed(MaxPooling2D((4, 4)))) 
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((4, 4))))
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    model.add(TimeDistributed(Dropout(0.25)))
    
    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    #model.add(TimeDistributed(Dropout(0.25)))
                                      
    model.add(TimeDistributed(Flatten()))
                                      
    model.add(LSTM(32))
                                      
    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))

    ########################################################################################################################

    # Display the models summary.
    model.summary()
    
    # Return the constructed LRCN model.
    return model
55/25:
# Construct the required LRCN model.
LRCN_model = create_LRCN_model()

# Display the success message.
print("Model Created Successfully!")
55/26:
# Plot the structure of the contructed LRCN model.
# plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)
55/27:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LRCN_model_training_history = LRCN_model.fit(x = features_train, y = labels_train, epochs = 20, batch_size = 4 ,
                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])
55/28:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save the Model.
LRCN_model.save(model_file_name)
55/29:
# Visualize the training and validation loss metrices.
plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
55/30:
# Visualize the training and validation accuracy metrices.
plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
55/31:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g20-c06_Yobm6Rq9'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
55/32:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
55/33:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
55/34:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
55/35:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
55/36:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer1 = cv2.VideoWriter('abc.mp4', cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))
    
    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        video_writer1.write(flow)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
55/37:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
55/38:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer1 = cv2.VideoWriter('abc.mp4', cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))
    
    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        video_writer1.write(normalized_frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
55/39:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
55/40:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
55/41:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
55/42:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
55/43:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
55/44:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
55/45:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
55/46:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'X264'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
55/47:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'X264'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
55/48:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
55/49:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'avc1'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
55/50:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
55/51:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))
    
    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
55/52:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
56/1:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g01-c01_wuNUmB5y'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
56/2:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))
    
    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
56/3:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
56/4:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g01-c01_wuNUmB5y'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
55/53:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g01-c01_wuNUmB5y'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
55/54:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))
    
    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
55/55:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
55/56:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
57/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
57/2:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
57/3:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
57/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize=(20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
57/5:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize=(20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
57/6:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
57/7:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
57/8:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize=(20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
57/9:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after calculating optical flow, resizing, and normalizing them.
    Args:
        video_path: The path of the video on the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''
    print(f"Processing video: {video_path}")
    
    # Declare lists to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return rgb_frames_list, flow_frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frames to fixed height and width.
        resized_rgb_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frames by dividing them with 255 so that each pixel value lies between 0 and 1
        normalized_rgb_frame = resized_rgb_frame / 255.0
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frames into the frames list
        rgb_frames_list.append(normalized_rgb_frame)
        flow_frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
57/10:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset using both RGB and optical flow.
    Returns:
        rgb_features:      A list containing the extracted RGB frames of the videos.
        flow_features:     A list containing the extracted optical flow frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos on the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        class_dir = os.path.join(DATASET_DIR, class_name)
        if not os.path.exists(class_dir):
            print(f"Error: Directory {class_dir} does not exist")
            continue
        
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the RGB and optical flow frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the videos having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:
                # Append the data to their respective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)

    return rgb_features, flow_features, labels, video_files_paths

# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()

# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)

# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
57/11:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
57/12:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after calculating optical flow, resizing, and normalizing them.
    Args:
        video_path: The path of the video on the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''
    print(f"Processing video: {video_path}")
    
    # Declare lists to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return rgb_frames_list, flow_frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frames to fixed height and width.
        resized_rgb_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frames by dividing them with 255 so that each pixel value lies between 0 and 1
        normalized_rgb_frame = resized_rgb_frame / 255.0
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frames into the frames list
        rgb_frames_list.append(normalized_rgb_frame)
        flow_frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
57/13:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset using both RGB and optical flow.
    Returns:
        rgb_features:      A list containing the extracted RGB frames of the videos.
        flow_features:     A list containing the extracted optical flow frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos on the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        class_dir = os.path.join(DATASET_DIR, class_name)
        if not os.path.exists(class_dir):
            print(f"Error: Directory {class_dir} does not exist")
            continue
        
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the RGB and optical flow frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the videos having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:
                # Append the data to their respective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)

    return rgb_features, flow_features, labels, video_files_paths

# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()

# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)

# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
57/14:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
57/15:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
57/16:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
57/17:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
57/18:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after calculating optical flow, resizing, and normalizing them.
    Args:
        video_path: The path of the video on the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''
    print(f"Processing video: {video_path}")
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
57/19:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset using both RGB and optical flow.
    Returns:
        rgb_features:      A list containing the extracted RGB frames of the videos.
        flow_features:     A list containing the extracted optical flow frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos on the disk.
    '''

    # Declared Empty Lists to store the features, labels, and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        class_dir = os.path.join(DATASET_DIR, class_name)
        if not os.path.exists(class_dir):
            print(f"Error: Directory {class_dir} does not exist")
            continue
        
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the RGB and optical flow frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the videos having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:
                # Append the data to their respective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)

    return rgb_features, flow_features, labels, video_files_paths

# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()

# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)

# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
58/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
58/2:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
58/3:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
58/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
58/5:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
58/6:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
58/7:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
58/8:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break

        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
58/9:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                features.append(frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    features = np.asarray(features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return features, labels, video_files_paths
58/10:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
58/11:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list, flow_frames_list
58/12:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
58/13:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
58/14:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
58/15:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
58/16:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
58/17:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    return model
58/18:
# Create the combined CNN-LSTM model.
model = create_combined_cnn_lstm_model()
58/19:
# Create the combined CNN-LSTM model.
model = create_combined_cnn_lstm_model()
58/20:
# Create the combined CNN-LSTM model.
model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
58/21:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
58/22:
# Create the combined CNN-LSTM model.
model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
58/23:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
58/24:
# Create the combined CNN-LSTM model.
model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
58/25:
# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model.
model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
58/26:
# Specify the number of epochs for training.
EPOCHS = 50

# Specify the batch size.
BATCH_SIZE = 32

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Train the model.
model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
58/27:
# Specify the number of epochs for training.
EPOCHS = 50

# Specify the batch size.
BATCH_SIZE = 32

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Train the model.
model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
58/28:
# Specify the number of epochs for training.
EPOCHS = 10

# Specify the batch size.
BATCH_SIZE = 32

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Display the model architecture.
model.summary()

# Train the model.
model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
58/29:
# Evaluate the trained model.
model_evaluation_history = model.evaluate([rgb_features_test, flow_features_test], labels_test)
58/30:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save your Model.
convlstm_model.save(model_file_name)
58/31:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save your Model.
model.save(model_file_name)
58/32:
def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):
    '''
    This function will plot the metrics passed to it in a graph.
    Args:
        model_training_history: A history object containing a record of training and validation 
                                loss values and metrics values at successive epochs
        metric_name_1:          The name of the first metric that needs to be plotted in the graph.
        metric_name_2:          The name of the second metric that needs to be plotted in the graph.
        plot_name:              The title of the graph.
    '''
    
    # Get metric values using metric names as identifiers.
    metric_value_1 = model_training_history.history[metric_name_1]
    metric_value_2 = model_training_history.history[metric_name_2]
    
    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.
    epochs = range(len(metric_value_1))

    # Plot the Graph.
    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)

    # Add title to the plot.
    plt.title(str(plot_name))

    # Add legend to the plot.
    plt.legend()
58/33:
# Visualize the training and validation loss metrices.
plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
58/34:
# Visualize the training and validation loss metrices.
plot_metric(model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
58/35:
# Visualize the training and validation loss metrices.
plot_metric(model, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
58/36:
# Visualize the training and validation loss metrices.
plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
58/37:
# Visualize the training and validation accuracy metrices.
plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
58/38:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))
    
    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/39:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/40:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))
    
    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LTSM_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/41:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/42:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g01-c01_wuNUmB5y'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
58/43:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g01-c01_wuNUmB5y'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
58/44:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))
    
    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LTSM_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/45:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/46:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))
    
    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/47:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/48:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/49:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/50:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    frames_queue = deque(maxlen = SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255

        # Appending the pre-processed frame into the frames list.
        frames_queue.append(normalized_frame)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = model.predict(np.expand_dims(frames_queue, axis = 0))[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/51:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/52:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/53:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/54:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/55:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/56:
# Plot the structure of the contructed LRCN model.
# plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)
58/57:
from keras.models import Model
from keras.layers import Input, TimeDistributed, Conv2D, MaxPooling2D, Dropout, Flatten, LSTM, Dense, concatenate

def create_LRCN_model():
    '''
    This function will construct the required two-stream LRCN model.
    Returns:
        model: It is the required constructed LRCN model.
    '''
    
    # Define the Model Architecture for RGB stream
    ########################################################################################################################
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(rgb_input)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Flatten())(x)
    
    x = TimeDistributed(Dense(128, activation='relu'))(x)
    x = Dropout(0.5)(x)

    x = LSTM(32, return_sequences=True)(x)
    x = LSTM(32)(x)
    
    ########################################################################################################################

    # Define the Model Architecture for Optical Flow stream
    ########################################################################################################################
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    y = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(flow_input)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Flatten())(y)
    
    y = TimeDistributed(Dense(128, activation='relu'))(y)
    y = Dropout(0.5)(y)

    y = LSTM(32, return_sequences=True)(y)
    y = LSTM(32)(y)
    
    ########################################################################################################################

    # Merge the two streams
    merged = concatenate([x, y])

    # Add final Dense layer for classification
    output = Dense(len(CLASSES_LIST), activation='softmax')(merged)

    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output)

    # Display the model's summary
    model.summary()
    
    # Return the constructed LRCN model.
    return model
58/58:
# Construct the required LRCN model.
LRCN_model = create_LRCN_model()

# Display the success message.
print("Model Created Successfully!")
58/59:
# Plot the structure of the contructed LRCN model.
# plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)
58/60:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LRCN_model_training_history = LRCN_model.fit(x = features_train, y = labels_train, epochs = 20, batch_size = 4 ,
                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])
58/61:
from keras.models import Model
from keras.layers import Input, TimeDistributed, Conv2D, MaxPooling2D, Dropout, Flatten, LSTM, Dense, concatenate

def create_LRCN_model():
    '''
    This function will construct the required two-stream LRCN model.
    Returns:
        model: It is the required constructed LRCN model.
    '''
    
    # Define the Model Architecture for RGB stream
    ########################################################################################################################
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(rgb_input)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Flatten())(x)
    
    x = TimeDistributed(Dense(128, activation='relu'))(x)
    x = Dropout(0.5)(x)

    x = LSTM(32, return_sequences=True)(x)
    x = LSTM(32)(x)
    
    ########################################################################################################################

    # Define the Model Architecture for Optical Flow stream
    ########################################################################################################################
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    y = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(flow_input)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Flatten())(y)
    
    y = TimeDistributed(Dense(128, activation='relu'))(y)
    y = Dropout(0.5)(y)

    y = LSTM(32, return_sequences=True)(y)
    y = LSTM(32)(y)
    
    ########################################################################################################################

    # Merge the two streams
    merged = concatenate([x, y])

    # Add final Dense layer for classification
    output = Dense(len(CLASSES_LIST), activation='softmax')(merged)

    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output)

    # Display the model's summary
    model.summary()
    
    # Return the constructed LRCN model.
    return model
58/62:
# Construct the required LRCN model.
LRCN_model = create_LRCN_model()

# Display the success message.
print("Model Created Successfully!")
58/63:
# Plot the structure of the contructed LRCN model.
# plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)
58/64:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LRCN_model_training_history = LRCN_model.fit(x = features_train, y = labels_train, epochs = 20, batch_size = 4 ,
                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])
58/65:
def create_LRCN_model():
    '''
    This function will construct the required two-stream LRCN model.
    Returns:
        model: It is the required constructed LRCN model.
    '''
    
    # Define the Model Architecture for RGB stream
    ########################################################################################################################
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(rgb_input)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Flatten())(x)
    
    x = TimeDistributed(Dense(128, activation='relu'))(x)
    x = Dropout(0.5)(x)

    x = LSTM(32, return_sequences=True)(x)
    x = LSTM(32)(x)
    
    ########################################################################################################################

    # Define the Model Architecture for Optical Flow stream
    ########################################################################################################################
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    y = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(flow_input)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Flatten())(y)
    
    y = TimeDistributed(Dense(128, activation='relu'))(y)
    y = Dropout(0.5)(y)

    y = LSTM(32, return_sequences=True)(y)
    y = LSTM(32)(y)
    
    ########################################################################################################################

    # Merge the two streams
    merged = concatenate([x, y])

    # Add final Dense layer for classification
    output = Dense(len(CLASSES_LIST), activation='softmax')(merged)

    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output)

    # Display the model's summary
    model.summary()
    
    # Return the constructed LRCN model.
    return model
58/66:
# Construct the required LRCN model.
LRCN_model = create_LRCN_model()

# Display the success message.
print("Model Created Successfully!")
58/67:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LRCN_model_training_history = LRCN_model.fit(
    x=[rgb_features_train, flow_features_train], 
    y=labels_train, 
    epochs=20, 
    batch_size=4,
    shuffle=True, 
    validation_split=0.2, 
    callbacks=[early_stopping_callback]
)
58/68:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save the Model.
LRCN_model.save(model_file_name)
58/69:
# Visualize the training and validation loss metrices.
plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
58/70:
# Visualize the training and validation accuracy metrices.
plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
58/71:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g01-c01_wuNUmB5y'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
58/72:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/73:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/74:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save the Model.
LRCN_model.save(model_file_name)
58/75:
# Visualize the training and validation loss metrices.
plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
58/76:
# Visualize the training and validation accuracy metrices.
plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
58/77:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g01-c01_wuNUmB5y'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
58/78:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/79:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/80:
def create_LRCN_model():
    '''
    This function will construct the required two-stream LRCN model.
    Returns:
        model: It is the required constructed LRCN model.
    '''
    
    # Define the Model Architecture for RGB stream
    ########################################################################################################################
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(rgb_input)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Flatten())(x)
    
    x = TimeDistributed(Dense(128, activation='relu'))(x)
    x = Dropout(0.5)(x)

    x = LSTM(32, return_sequences=True)(x)
    x = LSTM(32)(x)
    
    ########################################################################################################################

    # Define the Model Architecture for Optical Flow stream
    ########################################################################################################################
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    y = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(flow_input)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Flatten())(y)
    
    y = TimeDistributed(Dense(128, activation='relu'))(y)
    y = Dropout(0.5)(y)

    y = LSTM(32, return_sequences=True)(y)
    y = LSTM(32)(y)
    
    ########################################################################################################################

    # Merge the two streams
    merged = concatenate([x, y])

    # Add final Dense layer for classification
    output = Dense(len(CLASSES_LIST), activation='softmax')(merged)

    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output)

    # Display the model's summary
    model.summary()
    
    # Return the constructed LRCN model.
    return model
58/81:
# Construct the required LRCN model.
LRCN_model = create_LRCN_model()

# Display the success message.
print("Model Created Successfully!")
58/82:
# Plot the structure of the contructed LRCN model.
# plot_model(LRCN_model, to_file = 'LRCN_model_structure_plot.png', show_shapes = True, show_layer_names = True)
58/83:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LRCN_model_training_history = LRCN_model.fit(
    x=[rgb_features_train, flow_features_train], 
    y=labels_train, 
    epochs=70, 
    batch_size=4,
    shuffle=True, 
    validation_split=0.2, 
    callbacks=[early_stopping_callback]
)
58/84:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save the Model.
LRCN_model.save(model_file_name)
58/85:
# Visualize the training and validation loss metrices.
plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
58/86:
# Visualize the training and validation accuracy metrices.
plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
58/87:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g01-c01_wuNUmB5y'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
58/88:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/89:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/90:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LRCN_model_training_history = LRCN_model.fit(
    x=[rgb_features_train, flow_features_train], 
    y=labels_train, 
    epochs=70, 
    batch_size=4,
    shuffle=True, 
    validation_split=0.2, 
    
)
58/91:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save the Model.
LRCN_model.save(model_file_name)
58/92:
# Visualize the training and validation loss metrices.
plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
58/93:
# Visualize the training and validation accuracy metrices.
plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
58/94:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g01-c01_wuNUmB5y'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
58/95:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/96:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/97:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save the Model.
LRCN_model.save(model_file_name)
58/98:
# Visualize the training and validation loss metrices.
plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
58/99:
# Visualize the training and validation accuracy metrices.
plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
58/100:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g01-c01_wuNUmB5y'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
58/101:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
58/102:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
58/103:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
59/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
59/2:
seed_constant = 30
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
59/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (30, 30))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 50)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
59/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (2, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
59/5:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
59/6:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 50

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
59/7:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
59/8:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
59/9:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
57/20:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after calculating optical flow, resizing, and normalizing them.
    Args:
        video_path: The path of the video on the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''
    print(f"Processing video: {video_path}")
    
    # Declare a list to store video frames.
    frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Check if the video was successfully opened
    if not video_reader.isOpened():
        print(f"Error: Could not open video {video_path}")
        return frames_list

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {video_frames_count}")

    # Calculate the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)
    print(f"Frames skip window: {skip_frames_window}")

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return frames_list
    
    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames to calculate optical flow.
    for frame_counter in range(SEQUENCE_LENGTH):
        
        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            print(f"Warning: Could not read frame {frame_counter * skip_frames_window}")
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed height and width.
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))
        
        # Normalize the resized frame by dividing it with 255 so that each pixel value lies between 0 and 1
        normalized_flow = resized_flow / 255
        
        # Append the normalized optical flow frame into the frames list
        frames_list.append(normalized_flow)
        
        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray
    
    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return frames_list
57/21:
# Create the dataset.
features, labels, video_files_paths = create_dataset()
57/22:
# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
one_hot_encoded_labels = to_categorical(labels)
59/10:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
59/11:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
59/12:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
59/13:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
59/14:
# Specify the number of epochs for training.
EPOCHS = 10

# Specify the batch size.
BATCH_SIZE = 32

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
60/1:
# Specify the number of epochs for training.
EPOCHS = 50

# Specify the batch size.
BATCH_SIZE = 64

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
61/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
61/2:
seed_constant = 30
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
61/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
61/4:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 50

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
61/5:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
61/6:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
61/7:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
61/8:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
61/9:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
61/10:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
61/11:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
61/12:
# Specify the number of epochs for training.
EPOCHS = 50

# Specify the batch size.
BATCH_SIZE = 64

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
62/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
62/2:
seed_constant = 30
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
62/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
62/4:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 50

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
62/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 30

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
62/6:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
62/7:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
62/8:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
62/9:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
62/10:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
62/11:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
62/12:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
62/13:
# Specify the number of epochs for training.
EPOCHS = 50

# Specify the batch size.
BATCH_SIZE = 64

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
63/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
63/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
63/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
63/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
63/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 30

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
63/6:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
63/7:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
63/8:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
63/9:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
63/10:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
63/11:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
63/12:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
63/13:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
63/14:
# Specify the number of epochs for training.
EPOCHS = 50

# Specify the batch size.
BATCH_SIZE = 64

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
64/1:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
64/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
64/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
64/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
64/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
64/6:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        frames_list: A list containing the resized and normalized frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
64/7:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
64/8:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
64/9:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
64/10:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
64/11:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
64/12:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
64/13:
# Specify the number of epochs for training.
EPOCHS = 50

# Specify the batch size.
BATCH_SIZE = 64

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
64/14:
# Import the required libraries.
import os
import cv2
import pafy
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
64/15:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
64/16: #Two Stream Video Classification For Human Activity Recognition
64/17:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
65/1:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
65/2: ## Import Libraries
65/3:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
65/4:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
65/5:
## Step 1: Visualize the Data with its Labels</font>**

In the first step, we will visualize the data along with labels to get an idea about what we will be dealing with. We will be using the [UCF50 - Action Recognition Dataset](https://www.crcv.ucf.edu/data/UCF50.php), consisting of realistic videos taken from youtube which differentiates this data set from most of the other available action recognition data sets as they are not realistic and are staged by actors. The Dataset contains:

*   **`50`** Action Categories

*   **`25`** Groups of Videos per Action Category

*   **`133`** Average Videos per Action Category

*   **`199`** Average Number of Frames per Video

*   **`320`** Average Frames Width per Video

*   **`240`** Average Frames Height per Video

*   **`26`** Average Frames Per Seconds per Video

For visualization, we will pick `20` random categories from the dataset and a random video from each selected category and will visualize the first frame of the selected videos with their associated labels written. This way well be able to visualize a subset ( `20` random videos ) of the dataset.
65/6:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/7:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
65/8:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/9:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/10:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/11:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/12:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
65/13:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
65/14: Now we will utilize the function **`create_dataset()`** created above to extract the data of the selected classes and create the required dataset.
65/15:
## **<font style="color:rgb(134,19,348)">Step 4: Implement the ConvLSTM Approach</font>**

In this step, we will implement the first approach by using a combination of ConvLSTM cells. A ConvLSTM cell is a variant of an LSTM network that contains convolutions operations in the network. it is an LSTM with convolution embedded in the architecture, which makes it capable of identifying spatial features of the data while keeping into account the temporal relation. 

<center>
<img src="https://drive.google.com/uc?export=view&id=1KHN_JFWJoJi1xQj_bRdxy2QgevGOH1qP" width= 500px>
</center>


For video classification, this approach effectively captures the spatial relation in the individual frames and the temporal relation across the different frames. As a result of this convolution structure, the ConvLSTM is capable of taking in 3-dimensional input `(width, height, num_of_channels)` whereas a simple LSTM only takes in 1-dimensional input hence an LSTM is incompatible for modeling Spatio-temporal data on its own.

You can read the paper [**Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting**](https://arxiv.org/abs/1506.04214v1) by **Xingjian Shi** (NIPS 2015), to learn more about this architecture.
65/16:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
65/17:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
65/18:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/19:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/20:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
65/21:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 120, 120

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
65/22:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them, and compute optical flow between consecutive frames.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
65/23:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
65/24:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
65/25: Now we will utilize the function **`create_convlstm_model()`** created above, to construct the required `convlstm` model.
65/26:
#### Evaluate the Trained Model

After training, we will evaluate the model on the test set.
65/27:
### Step 4.3: Plot Models Loss & Accuracy Curves


Now we will create a function **`plot_metric()`** to visualize the training and validation metrics. We already have separate metrics from our training and validation steps so now we just have to visualize them.
65/28:
##  Implement the LRCN Approach

In this step, we will implement the LRCN Approach by combining Convolution and LSTM layers in a single model. Another similar approach can be to use a CNN model and LSTM model trained separately. The CNN model can be used to extract spatial features from the frames in the video, and for this purpose, a pre-trained model can be used, that can be fine-tuned for the problem. And the LSTM model can then use the features extracted by CNN, to predict the action being performed in the video. 


But here, we will implement another approach known as the Long-term Recurrent Convolutional Network (LRCN), which combines CNN and LSTM layers in a single model. The Convolutional layers are used for spatial feature extraction from the frames, and the extracted spatial features are fed to LSTM layer(s) at each time-steps for temporal sequence modeling. This way the network learns spatiotemporal features directly in an end-to-end training, resulting in a robust model.

<center>
<img src='https://drive.google.com/uc?export=download&id=1I-q5yLsIoNh2chfzT7JYvra17FsXvdme'>
</center>


You can read the paper [Long-term Recurrent Convolutional Networks for Visual Recognition and Description](https://arxiv.org/abs/1411.4389?source=post_page---------------------------) by Jeff Donahue (CVPR 2015), to learn more about this architecture.

We will also use [**`TimeDistributed`**](https://keras.io/api/layers/recurrent_layers/time_distributed/) wrapper layer, which allows applying the same layer to every frame of the video independently. So it makes a layer (around which it is wrapped) capable of taking input of shape `(no_of_frames, width, height, num_of_channels)` if originally the layer's input shape was `(width, height, num_of_channels)` which is very beneficial as it allows to input the whole video into the model in a single shot. 

<center>
<img src='https://drive.google.com/uc?export=download&id=1CbauSm5XTY7ypHYBHH7rDSnJ5LO9CUWX' width=400>
</center>
65/29:
### Step 5.1: Construct the Model

To implement our LRCN architecture, we will use time-distributed **`Conv2D`** layers which will be followed by **`MaxPooling2D`** and **`Dropout`** layers. The feature extracted from the **`Conv2D`** layers will be then flattened using the  **`Flatten`** layer and will be fed to a **`LSTM`** layer. The **`Dense`** layer with softmax activation will then use the output from the **`LSTM`** layer to predict the action being performed.
65/30:
# Evaluate the trained model.
model_evaluation_history = LRCN_model.evaluate(features_test, labels_test)
65/31:
## Step 6: Test the Best Performing Model on Sample videos

From the results, it seems that the LRCN model performed significantly well for a small number of classes. so in this step, we will put the `LRCN` model to test on some youtube videos.
65/32: Create a function to save frames inside folders
65/33:
def save_frames(class_name, video_name, rgb_frames, flow_frames):
    '''
    This function saves the RGB and optical flow frames to their respective class directories.
    Args:
        class_name: Name of the class to which the frames belong.
        video_name: Name of the video file being processed.
        rgb_frames: List of RGB frames.
        flow_frames: List of optical flow frames.
    '''
    # Create directories for RGB and optical flow frames if they don't exist.
    rgb_output_dir = os.path.join('processed_frames', 'rgb', class_name, video_name)
    flow_output_dir = os.path.join('processed_frames', 'flow', class_name, video_name)
    os.makedirs(rgb_output_dir, exist_ok=True)
    os.makedirs(flow_output_dir, exist_ok=True)

    # Save each frame as an image file.
    for i, (rgb_frame, flow_frame) in enumerate(zip(rgb_frames, flow_frames)):
        rgb_frame_path = os.path.join(rgb_output_dir, f'frame_{i+1:04d}.jpg')
        flow_frame_path = os.path.join(flow_output_dir, f'frame_{i+1:04d}.jpg')
        cv2.imwrite(rgb_frame_path, (rgb_frame * 255).astype(np.uint8))
        cv2.imwrite(flow_frame_path, (flow_frame * 255).astype(np.uint8))
65/34:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them, and compute optical flow between consecutive frames.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
65/35:
def save_frames(class_name, video_name, rgb_frames, flow_frames):
    '''
    This function saves the RGB and optical flow frames to their respective class directories.
    Args:
        class_name: Name of the class to which the frames belong.
        video_name: Name of the video file being processed.
        rgb_frames: List of RGB frames.
        flow_frames: List of optical flow frames.
    '''
    # Create directories for RGB and optical flow frames if they don't exist.
    rgb_output_dir = os.path.join('processed_frames', 'rgb', class_name, video_name)
    flow_output_dir = os.path.join('processed_frames', 'flow', class_name, video_name)
    os.makedirs(rgb_output_dir, exist_ok=True)
    os.makedirs(flow_output_dir, exist_ok=True)

    # Save each frame as an image file.
    for i, (rgb_frame, flow_frame) in enumerate(zip(rgb_frames, flow_frames)):
        rgb_frame_path = os.path.join(rgb_output_dir, f'frame_{i+1:04d}.jpg')
        flow_frame_path = os.path.join(flow_output_dir, f'frame_{i+1:04d}.jpg')
        cv2.imwrite(rgb_frame_path, (rgb_frame * 255).astype(np.uint8))
        cv2.imwrite(flow_frame_path, (flow_frame * 255).astype(np.uint8))
65/36:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

                # Save the frames to the disk.
                video_name = os.path.splitext(file_name)[0]
                save_frames(class_name, video_name, rgb_frames, flow_frames)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
65/37:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
65/38:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
65/39:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
65/40:
def save_frames(class_name, video_name, rgb_frames, flow_frames):
    '''
    This function saves the RGB and optical flow frames to their respective class directories.
    Args:
        class_name: Name of the class to which the frames belong.
        video_name: Name of the video file being processed.
        rgb_frames: List of RGB frames.
        flow_frames: List of optical flow frames.
    '''
    # Create directories for RGB and optical flow frames if they don't exist.
    rgb_output_dir = os.path.join('processed_frames', 'rgb', class_name, video_name)
    flow_output_dir = os.path.join('processed_frames', 'flow', class_name, video_name)
    os.makedirs(rgb_output_dir, exist_ok=True)
    os.makedirs(flow_output_dir, exist_ok=True)

    # Save each frame as an image file.
    for i, (rgb_frame, flow_frame) in enumerate(zip(rgb_frames, flow_frames)):
        rgb_frame_path = os.path.join(rgb_output_dir, f'frame_{i+1:04d}.jpg')
        flow_frame_path = os.path.join(flow_output_dir, f'frame_{i+1:04d}.jpg')
        cv2.imwrite(rgb_frame_path, (rgb_frame * 255).astype(np.uint8))
        cv2.imwrite(flow_frame_path, (flow_frame * 255).astype(np.uint8))
65/41:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

                # Save the frames to the disk.
                video_name = os.path.splitext(file_name)[0]
                save_frames(class_name, video_name, rgb_frames, flow_frames)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
65/42:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
65/43:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
65/44:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
65/45:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
65/46:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/47:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 9)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/48:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
65/49:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 9)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/50:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 9)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/51:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/52:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 120, 120

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
65/53:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them, and compute optical flow between consecutive frames.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
65/54:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
65/55:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
65/56:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/57:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/58:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
57/23:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 20 random values. The values will be between 0-50, 
# where 50 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 20)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/59:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
65/60:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 120, 120

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
65/61:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them, and compute optical flow between consecutive frames.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
65/62:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    #Layer1
    conv_1_frames = Conv2D(96, (7,7), strides= 2, activation='relu')(rgb_input)
    batch_norm_1_frames= tf.nn.local_response_normalization(conv_1_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_1_frames = MaxPooling2D((2,2)) (batch_norm_1_frames)
    #Layer2
    conv_2_frames = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_frames)
    batch_norm_2_frames = tf.nn.local_response_normalization(conv_2_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_2_frames = MaxPooling2D((2,2)) (batch_norm_2_frames)
    #Layer3
    conv_3_frames = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_frames)
    #Layer4
    conv_4_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_frames)
    #Layer5
    conv_5_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_frames)
    pool_3_frames = MaxPooling2D((2,2))(conv_5_frames)
    flat_frames = Flatten() (pool_3_frames)
    #Layer6
    fc_1_frames = Dense(4096,activation='relu')(flat_frames)
    #Layer7
    fc_2_frames = Dense(2048,activation='relu')(fc_1_frames)
    #output layer
    out_frames = Dense(101,activation='softmax')(fc_2_frames)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    #Layer1
    conv_1_flow = Conv2D(96, (7,7), strides= 2, activation='relu')(inp_flow)
    batch_norm_1_flow= tf.nn.local_response_normalization(conv_1_flow, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_1_flow = MaxPooling2D((2,2)) (batch_norm_1_flow)
    #Layer2
    conv_2_flow = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_flow)
    pool_2_flow = MaxPooling2D((2,2)) (conv_2_flow)
    #Layer3
    conv_3_flow = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_flow)
    #Layer4
    conv_4_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_flow)
    #Layer5
    conv_5_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_flow)
    pool_3_flow = MaxPooling2D((2,2))(conv_5_flow)
    flat_flow = Flatten() (pool_3_flow)
    #Layer6
    fc_1_flow = Dense(4096,activation='relu')(flat_flow)
    #Layer7
    fc_2_flow = Dense(2048,activation='relu')(fc_1_flow)
    #output layer
    out_flow = Dense(101,activation='softmax')(fc_2_flow)

    
    #Taking the output of both the streams and combining them 
    out = Average()([out_frames, out_flow])
    model = Model(inputs=[inp_frames, inp_flow], outputs=out)
    opti_flow = tf.keras.optimizers.Adam(learning_rate=1e-5)

    #compiling the model by using categorical_crossentropy loss
    model.compile(optimizer=opti_flow, loss = 'categorical_crossentropy', metrics=['mae','accuracy'])
    model.summary()
    
    model.summary()
    
    return model
65/63:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
65/64:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    #Layer1
    conv_1_frames = Conv2D(96, (7,7), strides= 2, activation='relu')(rgb_input)
    batch_norm_1_frames= tf.nn.local_response_normalization(conv_1_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_1_frames = MaxPooling2D((2,2)) (batch_norm_1_frames)
    #Layer2
    conv_2_frames = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_frames)
    batch_norm_2_frames = tf.nn.local_response_normalization(conv_2_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_2_frames = MaxPooling2D((2,2)) (batch_norm_2_frames)
    #Layer3
    conv_3_frames = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_frames)
    #Layer4
    conv_4_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_frames)
    #Layer5
    conv_5_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_frames)
    pool_3_frames = MaxPooling2D((2,2))(conv_5_frames)
    flat_frames = Flatten() (pool_3_frames)
    #Layer6
    fc_1_frames = Dense(4096,activation='relu')(flat_frames)
    #Layer7
    fc_2_frames = Dense(2048,activation='relu')(fc_1_frames)
    #output layer
    out_frames = Dense(101,activation='softmax')(fc_2_frames)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    #Layer1
    conv_1_flow = Conv2D(96, (7,7), strides= 2, activation='relu')(flow_input)
    batch_norm_1_flow= tf.nn.local_response_normalization(conv_1_flow, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_1_flow = MaxPooling2D((2,2)) (batch_norm_1_flow)
    #Layer2
    conv_2_flow = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_flow)
    pool_2_flow = MaxPooling2D((2,2)) (conv_2_flow)
    #Layer3
    conv_3_flow = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_flow)
    #Layer4
    conv_4_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_flow)
    #Layer5
    conv_5_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_flow)
    pool_3_flow = MaxPooling2D((2,2))(conv_5_flow)
    flat_flow = Flatten() (pool_3_flow)
    #Layer6
    fc_1_flow = Dense(4096,activation='relu')(flat_flow)
    #Layer7
    fc_2_flow = Dense(2048,activation='relu')(fc_1_flow)
    #output layer
    out_flow = Dense(101,activation='softmax')(fc_2_flow)

    
    #Taking the output of both the streams and combining them 
    out = Average()([out_frames, out_flow])
    model = Model(inputs=[inp_frames, inp_flow], outputs=out)
    opti_flow = tf.keras.optimizers.Adam(learning_rate=1e-5)

    #compiling the model by using categorical_crossentropy loss
    model.compile(optimizer=opti_flow, loss = 'categorical_crossentropy', metrics=['mae','accuracy'])
    model.summary()
    
    model.summary()
    
    return model
65/65:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
65/66:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    #Layer1
    conv_1_frames = Conv2D(96, (7,7), strides= 2, activation='relu')(rgb_input)
    batch_norm_1_frames= tf.nn.local_response_normalization(conv_1_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_1_frames = MaxPooling2D((2,2)) (batch_norm_1_frames)
    #Layer2
    conv_2_frames = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_frames)
    batch_norm_2_frames = tf.nn.local_response_normalization(conv_2_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_2_frames = MaxPooling2D((2,2)) (batch_norm_2_frames)
    #Layer3
    conv_3_frames = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_frames)
    #Layer4
    conv_4_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_frames)
    #Layer5
    conv_5_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_frames)
    pool_3_frames = MaxPooling2D((2,2))(conv_5_frames)
    flat_frames = Flatten() (pool_3_frames)
    #Layer6
    fc_1_frames = Dense(4096,activation='relu')(flat_frames)
    #Layer7
    fc_2_frames = Dense(2048,activation='relu')(fc_1_frames)
    #output layer
    out_frames = Dense(101,activation='softmax')(fc_2_frames)
    
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    #Layer1
    conv_1_flow = Conv2D(96, (7,7), strides= 2, activation='relu')(flow_input)
    batch_norm_1_flow= tf.nn.local_response_normalization(conv_1_flow, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_1_flow = MaxPooling2D((2,2)) (batch_norm_1_flow)
    #Layer2
    conv_2_flow = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_flow)
    pool_2_flow = MaxPooling2D((2,2)) (conv_2_flow)
    #Layer3
    conv_3_flow = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_flow)
    #Layer4
    conv_4_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_flow)
    #Layer5
    conv_5_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_flow)
    pool_3_flow = MaxPooling2D((2,2))(conv_5_flow)
    flat_flow = Flatten() (pool_3_flow)
    #Layer6
    fc_1_flow = Dense(4096,activation='relu')(flat_flow)
    #Layer7
    fc_2_flow = Dense(2048,activation='relu')(fc_1_flow)
    #output layer
    out_flow = Dense(101,activation='softmax')(fc_2_flow)

    
    #Taking the output of both the streams and combining them 
    out = Average()([out_frames, out_flow])
    model = Model(inputs=[inp_frames, inp_flow], outputs=out)
    opti_flow = tf.keras.optimizers.Adam(learning_rate=1e-5)

    #compiling the model by using categorical_crossentropy loss
    model.compile(optimizer=opti_flow, loss = 'categorical_crossentropy', metrics=['mae','accuracy'])
    model.summary()
    
    model.summary()
    
    return model
65/67:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
65/68:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    #Layer1
    conv_1_frames = Conv2D(96, (7,7), strides= 2, activation='relu')(rgb_input)
    batch_norm_1_frames= tf.nn.local_response_normalization(conv_1_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_1_frames = MaxPooling2D((2,2)) (batch_norm_1_frames)
    #Layer2
    conv_2_frames = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_frames)
    batch_norm_2_frames = tf.nn.local_response_normalization(conv_2_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_2_frames = MaxPooling2D((2,2)) (batch_norm_2_frames)
    #Layer3
    conv_3_frames = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_frames)
    #Layer4
    conv_4_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_frames)
    #Layer5
    conv_5_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_frames)
    pool_3_frames = MaxPooling2D((2,2))(conv_5_frames)
    flat_frames = Flatten() (pool_3_frames)
    #Layer6
    fc_1_frames = Dense(4096,activation='relu')(flat_frames)
    #Layer7
    fc_2_frames = Dense(2048,activation='relu')(fc_1_frames)
    #output layer
    out_frames = Dense(101,activation='softmax')(fc_2_frames)
    
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    #Layer1
    conv_1_flow = Conv2D(96, (7,7), strides= 2, activation='relu')(flow_input)
    batch_norm_1_flow= tf.nn.local_response_normalization(conv_1_flow, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_1_flow = MaxPooling2D((2,2)) (batch_norm_1_flow)
    #Layer2
    conv_2_flow = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_flow)
    pool_2_flow = MaxPooling2D((2,2)) (conv_2_flow)
    #Layer3
    conv_3_flow = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_flow)
    #Layer4
    conv_4_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_flow)
    #Layer5
    conv_5_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_flow)
    pool_3_flow = MaxPooling2D((2,2))(conv_5_flow)
    flat_flow = Flatten() (pool_3_flow)
    #Layer6
    fc_1_flow = Dense(4096,activation='relu')(flat_flow)
    #Layer7
    fc_2_flow = Dense(2048,activation='relu')(fc_1_flow)
    #output layer
    out_flow = Dense(101,activation='softmax')(fc_2_flow)

    
    #Taking the output of both the streams and combining them 
    out = Average()([out_frames, out_flow])
    model = Model(inputs=[inp_frames, inp_flow], outputs=out)
    opti_flow = tf.keras.optimizers.Adam(learning_rate=1e-5)

    #compiling the model by using categorical_crossentropy loss
    model.compile(optimizer=opti_flow, loss = 'categorical_crossentropy', metrics=['mae','accuracy'])
    model.summary()
    
    model.summary()
    
    return model
65/69:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
65/70:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    #Layer1
    conv_1_frames = Conv2D(96, (7,7), strides= 2, activation='relu')(rgb_input)
    batch_norm_1_frames= tf.nn.local_response_normalization(conv_1_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_1_frames = MaxPooling2D((2,2)) (batch_norm_1_frames)
    #Layer2
    conv_2_frames = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_frames)
    batch_norm_2_frames = tf.nn.local_response_normalization(conv_2_frames, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_2_frames = MaxPooling2D((2,2)) (batch_norm_2_frames)
    #Layer3
    conv_3_frames = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_frames)
    #Layer4
    conv_4_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_frames)
    #Layer5
    conv_5_frames = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_frames)
    pool_3_frames = MaxPooling2D((2,2))(conv_5_frames)
    flat_frames = Flatten() (pool_3_frames)
    #Layer6
    fc_1_frames = Dense(4096,activation='relu')(flat_frames)
    #Layer7
    fc_2_frames = Dense(2048,activation='relu')(fc_1_frames)
    #output layer
    out_frames = Dense(101,activation='softmax')(fc_2_frames)
    
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    #Layer1
    conv_1_flow = Conv2D(96, (7,7), strides= 2, activation='relu')(flow_input)
    batch_norm_1_flow= tf.nn.local_response_normalization(conv_1_flow, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)
    pool_1_flow = MaxPooling2D((2,2)) (batch_norm_1_flow)
    #Layer2
    conv_2_flow = Conv2D(256, (5,5), strides= 2, activation='relu')(pool_1_flow)
    pool_2_flow = MaxPooling2D((2,2)) (conv_2_flow)
    #Layer3
    conv_3_flow = Conv2D(512,(3,3),strides=1,activation='relu')(pool_2_flow)
    #Layer4
    conv_4_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_3_flow)
    #Layer5
    conv_5_flow = Conv2D(512,(3,3),strides=1,activation='relu')(conv_4_flow)
    pool_3_flow = MaxPooling2D((2,2))(conv_5_flow)
    flat_flow = Flatten() (pool_3_flow)
    #Layer6
    fc_1_flow = Dense(4096,activation='relu')(flat_flow)
    #Layer7
    fc_2_flow = Dense(2048,activation='relu')(fc_1_flow)
    #output layer
    out_flow = Dense(101,activation='softmax')(fc_2_flow)

    
    #Taking the output of both the streams and combining them 
    out = Average()([out_frames, out_flow])
    model = Model(inputs=[inp_frames, inp_flow], outputs=out)
    opti_flow = tf.keras.optimizers.Adam(learning_rate=1e-5)

    #compiling the model by using categorical_crossentropy loss
    model.compile(optimizer=opti_flow, loss = 'categorical_crossentropy', metrics=['mae','accuracy'])
    model.summary()
    
    model.summary()
    
    return model
65/71:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
65/72:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
65/73:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
65/74:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
65/75:
# Specify the number of epochs for training.
EPOCHS = 50

# Specify the batch size.
BATCH_SIZE = 64

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
66/1:
def create_combined_cnn_lstm_model(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, CLASSES_LIST):
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Result 1 from LSTM based RGB stream
    result1 = Dense(128, activation='relu')(rgb_lstm_layer)
    result1 = Dropout(0.5)(result1)
    result1 = Dense(len(CLASSES_LIST), activation='softmax', name='rgb_result')(result1)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Result 2 from Dense network based Optical Flow stream
    result2 = Dense(128, activation='relu')(flow_cnn_layer)
    result2 = Dropout(0.5)(result2)
    result2 = Dense(len(CLASSES_LIST), activation='softmax', name='flow_result')(result2)
    
    # Fusion of Result 1 and Result 2
    combined = concatenate([result1, result2])
    
    # Define the dense layers for final result
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax', name='final_result')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
66/2:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
66/3:
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Result 1 from LSTM based RGB stream
    result1 = Dense(128, activation='relu')(rgb_lstm_layer)
    result1 = Dropout(0.5)(result1)
    result1 = Dense(len(CLASSES_LIST), activation='softmax', name='rgb_result')(result1)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Result 2 from Dense network based Optical Flow stream
    result2 = Dense(128, activation='relu')(flow_cnn_layer)
    result2 = Dropout(0.5)(result2)
    result2 = Dense(len(CLASSES_LIST), activation='softmax', name='flow_result')(result2)
    
    # Fusion of Result 1 and Result 2
    combined = concatenate([result1, result2])
    
    # Define the dense layers for final result
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax', name='final_result')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
66/4:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
66/5:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
66/6: And will set `Numpy`, `Python`, and `Tensorflow` seeds to get consistent results on every execution.
66/7:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 120, 120

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
66/8:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them, and compute optical flow between consecutive frames.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
66/9:
def save_frames(class_name, video_name, rgb_frames, flow_frames):
    '''
    This function saves the RGB and optical flow frames to their respective class directories.
    Args:
        class_name: Name of the class to which the frames belong.
        video_name: Name of the video file being processed.
        rgb_frames: List of RGB frames.
        flow_frames: List of optical flow frames.
    '''
    # Create directories for RGB and optical flow frames if they don't exist.
    rgb_output_dir = os.path.join('processed_frames', 'rgb', class_name, video_name)
    flow_output_dir = os.path.join('processed_frames', 'flow', class_name, video_name)
    os.makedirs(rgb_output_dir, exist_ok=True)
    os.makedirs(flow_output_dir, exist_ok=True)

    # Save each frame as an image file.
    for i, (rgb_frame, flow_frame) in enumerate(zip(rgb_frames, flow_frames)):
        rgb_frame_path = os.path.join(rgb_output_dir, f'frame_{i+1:04d}.jpg')
        flow_frame_path = os.path.join(flow_output_dir, f'frame_{i+1:04d}.jpg')
        cv2.imwrite(rgb_frame_path, (rgb_frame * 255).astype(np.uint8))
        cv2.imwrite(flow_frame_path, (flow_frame * 255).astype(np.uint8))
66/10:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

                # Save the frames to the disk.
                video_name = os.path.splitext(file_name)[0]
                save_frames(class_name, video_name, rgb_frames, flow_frames)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
66/11:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
66/12:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
66/13: And will set `Numpy`, `Python`, and `Tensorflow` seeds to get consistent results on every execution.
66/14:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
66/15:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
66/16:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
66/17:
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Result 1 from LSTM based RGB stream
    result1 = Dense(128, activation='relu')(rgb_lstm_layer)
    result1 = Dropout(0.5)(result1)
    result1 = Dense(len(CLASSES_LIST), activation='softmax', name='rgb_result')(result1)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Result 2 from Dense network based Optical Flow stream
    result2 = Dense(128, activation='relu')(flow_cnn_layer)
    result2 = Dropout(0.5)(result2)
    result2 = Dense(len(CLASSES_LIST), activation='softmax', name='flow_result')(result2)
    
    # Fusion of Result 1 and Result 2
    combined = concatenate([result1, result2])
    
    # Define the dense layers for final result
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax', name='final_result')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
66/18:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
66/19:
from keras.layers import Input, TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout, concatenate, Reshape
from keras.models import Model

def create_combined_cnn_lstm_model(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, CLASSES_LIST):
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Result 1 from LSTM based RGB stream
    result1 = Dense(128, activation='relu')(rgb_lstm_layer)
    result1 = Dropout(0.5)(result1)
    result1 = Dense(len(CLASSES_LIST), activation='softmax', name='rgb_result')(result1)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Flatten the flow_cnn_layer and reshape to match the shape of rgb_lstm_layer
    flow_cnn_layer_flat = TimeDistributed(Flatten())(flow_cnn_layer)
    flow_cnn_layer_reshaped = Reshape((SEQUENCE_LENGTH * 256,))(flow_cnn_layer_flat)
    
    # Result 2 from Dense network based Optical Flow stream
    result2 = Dense(128, activation='relu')(flow_cnn_layer_reshaped)
    result2 = Dropout(0.5)(result2)
    result2 = Dense(len(CLASSES_LIST), activation='softmax', name='flow_result')(result2)
    
    # Fusion of Result 1 and Result 2
    combined = concatenate([result1, result2])
    
    # Define the dense layers for final result
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax', name='final_result')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
66/20:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
66/21:
from keras.layers import Input, TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout, concatenate, Reshape
from keras.models import Model

def create_combined_cnn_lstm_model(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, CLASSES_LIST):
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Result 1 from LSTM based RGB stream
    result1 = Dense(128, activation='relu')(rgb_lstm_layer)
    result1 = Dropout(0.5)(result1)
    result1 = Dense(len(CLASSES_LIST), activation='softmax', name='rgb_result')(result1)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Flatten the flow_cnn_layer and reshape to match the shape of rgb_lstm_layer
    flow_cnn_layer_flat = TimeDistributed(Flatten())(flow_cnn_layer)
    flow_cnn_layer_reshaped = Reshape((SEQUENCE_LENGTH * 256,))(flow_cnn_layer_flat)
    
    # Result 2 from Dense network based Optical Flow stream
    result2 = Dense(128, activation='relu')(flow_cnn_layer_reshaped)
    result2 = Dropout(0.5)(result2)
    result2 = Dense(len(CLASSES_LIST), activation='softmax', name='flow_result')(result2)
    
    # Fusion of Result 1 and Result 2
    combined = concatenate([result1, result2])
    
    # Define the dense layers for final result
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax', name='final_result')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
66/22:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
66/23:
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Result 1 from LSTM based RGB stream
    result1 = Dense(128, activation='relu')(rgb_lstm_layer)
    result1 = Dropout(0.5)(result1)
    result1 = Dense(len(CLASSES_LIST), activation='softmax', name='rgb_result')(result1)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Flatten the flow_cnn_layer and reshape to match the shape of rgb_lstm_layer
    flow_cnn_layer_flat = TimeDistributed(Flatten())(flow_cnn_layer)
    flow_cnn_layer_reshaped = Reshape((SEQUENCE_LENGTH * 256,))(flow_cnn_layer_flat)
    
    # Result 2 from Dense network based Optical Flow stream
    result2 = Dense(128, activation='relu')(flow_cnn_layer_reshaped)
    result2 = Dropout(0.5)(result2)
    result2 = Dense(len(CLASSES_LIST), activation='softmax', name='flow_result')(result2)
    
    # Fusion of Result 1 and Result 2
    combined = concatenate([result1, result2])
    
    # Define the dense layers for final result
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax', name='final_result')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
66/24:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
66/25:
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(GlobalAveragePooling2D())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Result 1 from LSTM based RGB stream
    result1 = Dense(128, activation='relu')(rgb_lstm_layer)
    result1 = Dropout(0.5)(result1)
    result1 = Dense(len(CLASSES_LIST), activation='softmax', name='rgb_result')(result1)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(GlobalAveragePooling2D())(flow_cnn_layer)
    
    # Flatten the flow_cnn_layer
    flow_flattened = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Result 2 from Dense network based Optical Flow stream
    result2 = Dense(128, activation='relu')(flow_flattened)
    result2 = Dropout(0.5)(result2)
    result2 = Dense(len(CLASSES_LIST), activation='softmax', name='flow_result')(result2)
    
    # Fusion of Result 1 and Result 2
    combined = concatenate([rgb_lstm_layer, flow_flattened])
    
    # Define the dense layers for final result
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax', name='final_result')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
66/26:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
66/27:
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Result 1 from LSTM based RGB stream
    result1 = Dense(128, activation='relu')(rgb_lstm_layer)
    result1 = Dropout(0.5)(result1)
    result1 = Dense(len(CLASSES_LIST), activation='softmax', name='rgb_result')(result1)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the Dense layers for optical flow stream
    flow_flattened = TimeDistributed(Flatten())(flow_cnn_layer)
    flow_dense = TimeDistributed(Dense(64, activation='relu'))(flow_flattened)
    
    # Global pooling to match shape
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_dense)
    
    # Result 2 from Dense network based Optical Flow stream
    result2 = Dense(128, activation='relu')(flow_lstm_layer)
    result2 = Dropout(0.5)(result2)
    result2 = Dense(len(CLASSES_LIST), activation='softmax', name='flow_result')(result2)
    
    # Fusion of Result 1 and Result 2
    combined = concatenate([result1, result2])
    
    # Define the dense layers for final result
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax', name='final_result')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model

# Replace with appropriate values for your use case
SEQUENCE_LENGTH = 20
IMAGE_HEIGHT = 64
IMAGE_WIDTH = 64
CLASSES_LIST = ['class1', 'class2', 'class3']

# Create the model
model = create_combined_cnn_lstm_model(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, CLASSES_LIST)
66/28:
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Result 1 from LSTM based RGB stream
    result1 = Dense(128, activation='relu')(rgb_lstm_layer)
    result1 = Dropout(0.5)(result1)
    result1 = Dense(len(CLASSES_LIST), activation='softmax', name='rgb_result')(result1)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the Dense layers for optical flow stream
    flow_flattened = TimeDistributed(Flatten())(flow_cnn_layer)
    flow_dense = TimeDistributed(Dense(64, activation='relu'))(flow_flattened)
    
    # Global pooling to match shape
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_dense)
    
    # Result 2 from Dense network based Optical Flow stream
    result2 = Dense(128, activation='relu')(flow_lstm_layer)
    result2 = Dropout(0.5)(result2)
    result2 = Dense(len(CLASSES_LIST), activation='softmax', name='flow_result')(result2)
    
    # Fusion of Result 1 and Result 2
    combined = concatenate([result1, result2])
    
    # Define the dense layers for final result
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax', name='final_result')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
66/29:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
66/30:
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Result 1 from LSTM based RGB stream
    result1 = Dense(128, activation='relu')(rgb_lstm_layer)
    result1 = Dropout(0.5)(result1)
    result1 = Dense(len(CLASSES_LIST), activation='softmax', name='rgb_result')(result1)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the Dense layers for optical flow stream
    flow_flattened = TimeDistributed(Flatten())(flow_cnn_layer)
    flow_dense = TimeDistributed(Dense(64, activation='relu'))(flow_flattened)
    
    # Global pooling to match shape
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_dense)
    
    # Result 2 from Dense network based Optical Flow stream
    result2 = Dense(128, activation='relu')(flow_lstm_layer)
    result2 = Dropout(0.5)(result2)
    result2 = Dense(len(CLASSES_LIST), activation='softmax', name='flow_result')(result2)
    
    # Fusion of Result 1 and Result 2
    combined = concatenate([result1, result2])
    
    # Define the dense layers for final result
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax', name='final_result')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
66/31:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
66/32:
# Specify the number of epochs for training.
EPOCHS = 10

# Specify the batch size.
BATCH_SIZE = 64

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
66/33:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
66/34:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
66/35:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
67/1:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
67/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
67/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
67/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
67/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 120, 120

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
67/6:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them, and compute optical flow between consecutive frames.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
67/7:
def save_frames(class_name, video_name, rgb_frames, flow_frames):
    '''
    This function saves the RGB and optical flow frames to their respective class directories.
    Args:
        class_name: Name of the class to which the frames belong.
        video_name: Name of the video file being processed.
        rgb_frames: List of RGB frames.
        flow_frames: List of optical flow frames.
    '''
    # Create directories for RGB and optical flow frames if they don't exist.
    rgb_output_dir = os.path.join('processed_frames', 'rgb', class_name, video_name)
    flow_output_dir = os.path.join('processed_frames', 'flow', class_name, video_name)
    os.makedirs(rgb_output_dir, exist_ok=True)
    os.makedirs(flow_output_dir, exist_ok=True)

    # Save each frame as an image file.
    for i, (rgb_frame, flow_frame) in enumerate(zip(rgb_frames, flow_frames)):
        rgb_frame_path = os.path.join(rgb_output_dir, f'frame_{i+1:04d}.jpg')
        flow_frame_path = os.path.join(flow_output_dir, f'frame_{i+1:04d}.jpg')
        cv2.imwrite(rgb_frame_path, (rgb_frame * 255).astype(np.uint8))
        cv2.imwrite(flow_frame_path, (flow_frame * 255).astype(np.uint8))
67/8:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

                # Save the frames to the disk.
                video_name = os.path.splitext(file_name)[0]
                save_frames(class_name, video_name, rgb_frames, flow_frames)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
67/9:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
67/10:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
67/11:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
67/12:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
67/13:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
67/14:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
67/15:
# Specify the number of epochs for training.
EPOCHS = 50

# Specify the batch size.
BATCH_SIZE = 64

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
68/1:
# Specify the number of epochs for training.
EPOCHS = 10

# Specify the batch size.
BATCH_SIZE = 64

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
68/2:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
68/3:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
68/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
68/5:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
68/6:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 120, 120

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
68/7:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them, and compute optical flow between consecutive frames.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
68/8:
def save_frames(class_name, video_name, rgb_frames, flow_frames):
    '''
    This function saves the RGB and optical flow frames to their respective class directories.
    Args:
        class_name: Name of the class to which the frames belong.
        video_name: Name of the video file being processed.
        rgb_frames: List of RGB frames.
        flow_frames: List of optical flow frames.
    '''
    # Create directories for RGB and optical flow frames if they don't exist.
    rgb_output_dir = os.path.join('processed_frames', 'rgb', class_name, video_name)
    flow_output_dir = os.path.join('processed_frames', 'flow', class_name, video_name)
    os.makedirs(rgb_output_dir, exist_ok=True)
    os.makedirs(flow_output_dir, exist_ok=True)

    # Save each frame as an image file.
    for i, (rgb_frame, flow_frame) in enumerate(zip(rgb_frames, flow_frames)):
        rgb_frame_path = os.path.join(rgb_output_dir, f'frame_{i+1:04d}.jpg')
        flow_frame_path = os.path.join(flow_output_dir, f'frame_{i+1:04d}.jpg')
        cv2.imwrite(rgb_frame_path, (rgb_frame * 255).astype(np.uint8))
        cv2.imwrite(flow_frame_path, (flow_frame * 255).astype(np.uint8))
68/9:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

                # Save the frames to the disk.
                video_name = os.path.splitext(file_name)[0]
                save_frames(class_name, video_name, rgb_frames, flow_frames)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
68/10:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
68/11:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
68/12:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
68/13:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
68/14:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
68/15:
# Specify the number of epochs for training.
EPOCHS = 10

# Specify the batch size.
BATCH_SIZE = 64

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
69/1:
# Specify the number of epochs for training.
EPOCHS = 10

# Specify the batch size.
BATCH_SIZE = 4

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
69/2:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
69/3:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
69/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
70/1:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
70/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
70/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
70/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
70/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 120, 120

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
70/6:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them, and compute optical flow between consecutive frames.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
70/7:
def save_frames(class_name, video_name, rgb_frames, flow_frames):
    '''
    This function saves the RGB and optical flow frames to their respective class directories.
    Args:
        class_name: Name of the class to which the frames belong.
        video_name: Name of the video file being processed.
        rgb_frames: List of RGB frames.
        flow_frames: List of optical flow frames.
    '''
    # Create directories for RGB and optical flow frames if they don't exist.
    rgb_output_dir = os.path.join('processed_frames', 'rgb', class_name, video_name)
    flow_output_dir = os.path.join('processed_frames', 'flow', class_name, video_name)
    os.makedirs(rgb_output_dir, exist_ok=True)
    os.makedirs(flow_output_dir, exist_ok=True)

    # Save each frame as an image file.
    for i, (rgb_frame, flow_frame) in enumerate(zip(rgb_frames, flow_frames)):
        rgb_frame_path = os.path.join(rgb_output_dir, f'frame_{i+1:04d}.jpg')
        flow_frame_path = os.path.join(flow_output_dir, f'frame_{i+1:04d}.jpg')
        cv2.imwrite(rgb_frame_path, (rgb_frame * 255).astype(np.uint8))
        cv2.imwrite(flow_frame_path, (flow_frame * 255).astype(np.uint8))
70/8:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

                # Save the frames to the disk.
                video_name = os.path.splitext(file_name)[0]
                save_frames(class_name, video_name, rgb_frames, flow_frames)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
70/9:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
70/10:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
70/11:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
70/12:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
70/13:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
70/14:
# Specify the number of epochs for training.
EPOCHS = 10

# Specify the batch size.
BATCH_SIZE = 4

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
70/15:
# Specify the number of epochs for training.
EPOCHS = 50

# Specify the batch size.
BATCH_SIZE = 4

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
70/16:
def create_two_stream_model():
    # Spatial stream (RGB)
    rgb_input = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    x_rgb = Conv2D(96, (7, 7), strides=2, activation='relu')(rgb_input)
    x_rgb = MaxPooling2D((2, 2))(x_rgb)
    x_rgb = Conv2D(256, (5, 5), strides=2, activation='relu')(x_rgb)
    x_rgb = MaxPooling2D((2, 2))(x_rgb)
    x_rgb = Conv2D(512, (3, 3), strides=1, activation='relu')(x_rgb)
    x_rgb = Conv2D(512, (3, 3), strides=1, activation='relu')(x_rgb)
    x_rgb = Conv2D(512, (3, 3), strides=1, activation='relu')(x_rgb)
    x_rgb = MaxPooling2D((2, 2))(x_rgb)
    x_rgb = Flatten()(x_rgb)
    x_rgb = Dense(4096, activation='relu')(x_rgb)
    x_rgb = Dropout(0.5)(x_rgb)
    x_rgb = Dense(2048, activation='relu')(x_rgb)
    x_rgb = Dropout(0.5)(x_rgb)
    rgb_output = Dense(len(CLASSES_LIST), activation='softmax')(x_rgb)

    # Temporal stream (Optical Flow)
    flow_input = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 20))  # Assuming 20-frame optical flow stack
    x_flow = Conv2D(96, (7, 7), strides=2, activation='relu')(flow_input)
    x_flow = MaxPooling2D((2, 2))(x_flow)
    x_flow = Conv2D(256, (5, 5), strides=2, activation='relu')(x_flow)
    x_flow = MaxPooling2D((2, 2))(x_flow)
    x_flow = Conv2D(512, (3, 3), strides=1, activation='relu')(x_flow)
    x_flow = Conv2D(512, (3, 3), strides=1, activation='relu')(x_flow)
    x_flow = Conv2D(512, (3, 3), strides=1, activation='relu')(x_flow)
    x_flow = MaxPooling2D((2, 2))(x_flow)
    x_flow = Flatten()(x_flow)
    x_flow = Dense(4096, activation='relu')(x_flow)
    x_flow = Dropout(0.5)(x_flow)
    x_flow = Dense(2048, activation='relu')(x_flow)
    x_flow = Dropout(0.5)(x_flow)
    flow_output = Dense(len(CLASSES_LIST), activation='softmax')(x_flow)

    # Combine class scores
    combined = concatenate([rgb_output, flow_output])
    final_output = Dense(len(CLASSES_LIST), activation='softmax')(combined)

    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=final_output)

    model.summary()

    return model
70/17:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
70/18:
def create_two_stream_model():
    # Spatial stream (RGB)
    rgb_input = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    x_rgb = Conv2D(96, (7, 7), strides=2, activation='relu')(rgb_input)
    x_rgb = MaxPooling2D((2, 2))(x_rgb)
    x_rgb = Conv2D(256, (5, 5), strides=2, activation='relu')(x_rgb)
    x_rgb = MaxPooling2D((2, 2))(x_rgb)
    x_rgb = Conv2D(512, (3, 3), strides=1, activation='relu')(x_rgb)
    x_rgb = Conv2D(512, (3, 3), strides=1, activation='relu')(x_rgb)
    x_rgb = Conv2D(512, (3, 3), strides=1, activation='relu')(x_rgb)
    x_rgb = MaxPooling2D((2, 2))(x_rgb)
    x_rgb = Flatten()(x_rgb)
    x_rgb = Dense(4096, activation='relu')(x_rgb)
    x_rgb = Dropout(0.5)(x_rgb)
    x_rgb = Dense(2048, activation='relu')(x_rgb)
    x_rgb = Dropout(0.5)(x_rgb)
    rgb_output = Dense(len(CLASSES_LIST), activation='softmax')(x_rgb)

    # Temporal stream (Optical Flow)
    flow_input = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 20))  # Assuming 20-frame optical flow stack
    x_flow = Conv2D(96, (7, 7), strides=2, activation='relu')(flow_input)
    x_flow = MaxPooling2D((2, 2))(x_flow)
    x_flow = Conv2D(256, (5, 5), strides=2, activation='relu')(x_flow)
    x_flow = MaxPooling2D((2, 2))(x_flow)
    x_flow = Conv2D(512, (3, 3), strides=1, activation='relu')(x_flow)
    x_flow = Conv2D(512, (3, 3), strides=1, activation='relu')(x_flow)
    x_flow = Conv2D(512, (3, 3), strides=1, activation='relu')(x_flow)
    x_flow = MaxPooling2D((2, 2))(x_flow)
    x_flow = Flatten()(x_flow)
    x_flow = Dense(4096, activation='relu')(x_flow)
    x_flow = Dropout(0.5)(x_flow)
    x_flow = Dense(2048, activation='relu')(x_flow)
    x_flow = Dropout(0.5)(x_flow)
    flow_output = Dense(len(CLASSES_LIST), activation='softmax')(x_flow)

    # Combine class scores
    combined = Average([rgb_output, flow_output])
    final_output = Dense(len(CLASSES_LIST), activation='softmax')(combined)

    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=final_output)

    model.summary()

    return model
70/19:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
70/20:
# Specify the number of epochs for training.
EPOCHS = 5

# Specify the batch size.
BATCH_SIZE = 4

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
71/1:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
71/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
71/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
72/1:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
72/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
72/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
72/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
72/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 120, 120

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
72/6:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them, and compute optical flow between consecutive frames.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
72/7:
def save_frames(class_name, video_name, rgb_frames, flow_frames):
    '''
    This function saves the RGB and optical flow frames to their respective class directories.
    Args:
        class_name: Name of the class to which the frames belong.
        video_name: Name of the video file being processed.
        rgb_frames: List of RGB frames.
        flow_frames: List of optical flow frames.
    '''
    # Create directories for RGB and optical flow frames if they don't exist.
    rgb_output_dir = os.path.join('processed_frames', 'rgb', class_name, video_name)
    flow_output_dir = os.path.join('processed_frames', 'flow', class_name, video_name)
    os.makedirs(rgb_output_dir, exist_ok=True)
    os.makedirs(flow_output_dir, exist_ok=True)

    # Save each frame as an image file.
    for i, (rgb_frame, flow_frame) in enumerate(zip(rgb_frames, flow_frames)):
        rgb_frame_path = os.path.join(rgb_output_dir, f'frame_{i+1:04d}.jpg')
        flow_frame_path = os.path.join(flow_output_dir, f'frame_{i+1:04d}.jpg')
        cv2.imwrite(rgb_frame_path, (rgb_frame * 255).astype(np.uint8))
        cv2.imwrite(flow_frame_path, (flow_frame * 255).astype(np.uint8))
72/8:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

                # Save the frames to the disk.
                video_name = os.path.splitext(file_name)[0]
                save_frames(class_name, video_name, rgb_frames, flow_frames)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
72/9:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
72/10:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
72/11:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
72/12:
# Define a function to create the CNN-LSTM model.
def create_combined_cnn_lstm_model():
    # Define the input layer for RGB stream
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    # Define the CNN layers for RGB stream
    rgb_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(rgb_input)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(rgb_cnn_layer)
    rgb_cnn_layer = TimeDistributed(Flatten())(rgb_cnn_layer)
    
    # Define the LSTM layers for RGB stream
    rgb_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(rgb_cnn_layer)
    
    # Define the input layer for optical flow stream
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    # Define the CNN layers for optical flow stream
    flow_cnn_layer = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(flow_input)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu'))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(MaxPooling2D((2, 2)))(flow_cnn_layer)
    flow_cnn_layer = TimeDistributed(Flatten())(flow_cnn_layer)
    
    # Define the LSTM layers for optical flow stream
    flow_lstm_layer = LSTM(64, return_sequences=False, dropout=0.5)(flow_cnn_layer)
    
    # Merge the outputs from both streams
    combined = concatenate([rgb_lstm_layer, flow_lstm_layer])
    
    # Define the dense layers
    dense_layer = Dense(128, activation='relu')(combined)
    dense_layer = Dropout(0.5)(dense_layer)
    output_layer = Dense(len(CLASSES_LIST), activation='softmax')(dense_layer)
    
    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output_layer)
    
    model.summary()
    
    return model
72/13:
# Create the combined CNN-LSTM model.
convlstm_model = create_combined_cnn_lstm_model()
# Display the success message. 
print("Model Created Successfully!")
72/14:
# Specify the number of epochs for training.
EPOCHS = 10

# Specify the batch size.
BATCH_SIZE = 4

# Define EarlyStopping callback to prevent overfitting.
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Compile the model.
convlstm_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Start training the model.
convlstm_model_training_history = convlstm_model.fit([rgb_features_train, flow_features_train], labels_train, 
          epochs=EPOCHS, batch_size=BATCH_SIZE, 
          validation_split=0.2, 
          callbacks=[early_stopping_callback])
72/15:
# Evaluate the trained model.
model_evaluation_history = convlstm_model.evaluate([rgb_features_test, flow_features_test], labels_test)
72/16:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save your Model.
convlstm_model.save(model_file_name)
72/17:
def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):
    '''
    This function will plot the metrics passed to it in a graph.
    Args:
        model_training_history: A history object containing a record of training and validation 
                                loss values and metrics values at successive epochs
        metric_name_1:          The name of the first metric that needs to be plotted in the graph.
        metric_name_2:          The name of the second metric that needs to be plotted in the graph.
        plot_name:              The title of the graph.
    '''
    
    # Get metric values using metric names as identifiers.
    metric_value_1 = model_training_history.history[metric_name_1]
    metric_value_2 = model_training_history.history[metric_name_2]
    
    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.
    epochs = range(len(metric_value_1))

    # Plot the Graph.
    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)

    # Add title to the plot.
    plt.title(str(plot_name))

    # Add legend to the plot.
    plt.legend()
72/18:
# Visualize the training and validation loss metrices.
plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
72/19:
# Visualize the training and validation accuracy metrices.
plot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
72/20:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/21:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/22:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'v-applyeyemakeup-g01-c01_wuNUmB5y'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/23:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/24:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/25:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'test'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/26:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/27:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/28:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'aaa'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/29:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/30:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/31:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'bbb'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/32:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/33:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/34:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'bbb'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/35:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/36:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/37:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'ccc'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/38:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/39:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/40:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/41:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/42:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/43:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby2'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/44:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/45:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/46:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby3'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/47:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/48:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/49:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby4'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/50:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/51:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/52:
def create_LRCN_model():
    '''
    This function will construct the required two-stream LRCN model.
    Returns:
        model: It is the required constructed LRCN model.
    '''
    
    # Define the Model Architecture for RGB stream
    ########################################################################################################################
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(rgb_input)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Flatten())(x)
    
    x = TimeDistributed(Dense(128, activation='relu'))(x)
    x = Dropout(0.5)(x)

    x = LSTM(32, return_sequences=True)(x)
    x = LSTM(32)(x)
    
    ########################################################################################################################

    # Define the Model Architecture for Optical Flow stream
    ########################################################################################################################
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    y = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(flow_input)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Flatten())(y)
    
    y = TimeDistributed(Dense(128, activation='relu'))(y)
    y = Dropout(0.5)(y)

    y = LSTM(32, return_sequences=True)(y)
    y = LSTM(32)(y)
    
    ########################################################################################################################

    # Merge the two streams
    merged = concatenate([x, y])

    # Add final Dense layer for classification
    output = Dense(len(CLASSES_LIST), activation='softmax')(merged)

    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output)

    # Display the model's summary
    model.summary()
    
    # Return the constructed LRCN model.
    return model
72/53:
# Construct the required LRCN model.
LRCN_model = create_LRCN_model()

# Display the success message.
print("Model Created Successfully!")
72/54:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
LRCN_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LRCN_model_training_history = LRCN_model.fit(
    x=[rgb_features_train, flow_features_train], 
    y=labels_train, 
    epochs=50, 
    batch_size=4,
    shuffle=True, 
    validation_split=0.2, 
    callbacks=[early_stopping_callback]
)
72/55:
# Evaluate the trained model.
model_evaluation_history = LRCN_model.evaluate(features_test, labels_test)
72/56:
# Evaluate the trained model.
model_evaluation_history = LRCN_model.evaluate(features_test, labels_test)
72/57:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save the Model.
LRCN_model.save(model_file_name)
72/58:
# Visualize the training and validation loss metrices.
plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
72/59:
# Visualize the training and validation accuracy metrices.
plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
72/60:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby3'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/61:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/62:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/63:
# Evaluate the trained model.
model_evaluation_history = LRCN_model.evaluate(rgb_features_test, labels_test)
72/64:
# Evaluate the trained model.
model_evaluation_history = LRCN_model.evaluate(rgb_features_test,flow_features_test, labels_test)
72/65:
# Evaluate the trained model.
model_evaluation_history = LRCN_model.evaluate([rgb_features_test,flow_features_test], labels_test)
72/66:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save the Model.
LRCN_model.save(model_file_name)
72/67:
# Visualize the training and validation loss metrices.
plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
72/68:
# Visualize the training and validation accuracy metrices.
plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
72/69:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby3'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/70:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)
    
# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save the Model.
LRCN_model.save(model_file_name)
72/71:
# Visualize the training and validation loss metrices.
plot_metric(LRCN_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
72/72:
# Visualize the training and validation accuracy metrices.
plot_metric(LRCN_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
72/73:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby3'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/74:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/75:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/76:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'test'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
72/77:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the LRCN model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = LRCN_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
72/78:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
72/79:
def create_model():
    '''
    This function will construct the required two-stream LSTM model.
    Returns:
        model: It is the required constructed LSTM model.
    '''
    
    # Define the Model Architecture for RGB stream
    ########################################################################################################################
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(rgb_input)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Flatten())(x)
    
    x = TimeDistributed(Dense(128, activation='relu'))(x)
    x = Dropout(0.5)(x)

    x = LSTM(32, return_sequences=True)(x)
    x = LSTM(32)(x)
    
    ########################################################################################################################

    # Define the Model Architecture for Optical Flow stream
    ########################################################################################################################
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    y = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(flow_input)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Flatten())(y)
    
    y = TimeDistributed(Dense(128, activation='relu'))(y)
    y = Dropout(0.5)(y)

    y = LSTM(32, return_sequences=True)(y)
    y = LSTM(32)(y)
    
    ########################################################################################################################

    # Merge the two streams
    merged = concatenate([x, y])

    # Add final Dense layer for classification
    output = Dense(len(CLASSES_LIST), activation='softmax')(merged)

    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output)

    # Display the model's summary
    model.summary()
    
    # Return the constructed LSTM model.
    return model
72/80:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LSTM_model_training_history = convlstm_model.fit(
    x=[rgb_features_train, flow_features_train], 
    y=labels_train, 
    epochs=50, 
    batch_size=4,
    shuffle=True, 
    validation_split=0.2, 
    callbacks=[early_stopping_callback]
)
74/1:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
74/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
74/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
74/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
74/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 120, 120

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
74/6:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them, and compute optical flow between consecutive frames.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
74/7:
def save_frames(class_name, video_name, rgb_frames, flow_frames):
    '''
    This function saves the RGB and optical flow frames to their respective class directories.
    Args:
        class_name: Name of the class to which the frames belong.
        video_name: Name of the video file being processed.
        rgb_frames: List of RGB frames.
        flow_frames: List of optical flow frames.
    '''
    # Create directories for RGB and optical flow frames if they don't exist.
    rgb_output_dir = os.path.join('processed_frames', 'rgb', class_name, video_name)
    flow_output_dir = os.path.join('processed_frames', 'flow', class_name, video_name)
    os.makedirs(rgb_output_dir, exist_ok=True)
    os.makedirs(flow_output_dir, exist_ok=True)

    # Save each frame as an image file.
    for i, (rgb_frame, flow_frame) in enumerate(zip(rgb_frames, flow_frames)):
        rgb_frame_path = os.path.join(rgb_output_dir, f'frame_{i+1:04d}.jpg')
        flow_frame_path = os.path.join(flow_output_dir, f'frame_{i+1:04d}.jpg')
        cv2.imwrite(rgb_frame_path, (rgb_frame * 255).astype(np.uint8))
        cv2.imwrite(flow_frame_path, (flow_frame * 255).astype(np.uint8))
74/8:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

                # Save the frames to the disk.
                video_name = os.path.splitext(file_name)[0]
                save_frames(class_name, video_name, rgb_frames, flow_frames)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
74/9:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
73/1:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
74/10:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
74/11:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
73/2:
# Import the required libraries.
import os
73/3:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
74/12:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
74/13:
def create_model():
    '''
    This function will construct the required two-stream LSTM model.
    Returns:
        model: It is the required constructed LSTM model.
    '''
    
    # Define the Model Architecture for RGB stream
    ########################################################################################################################
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(rgb_input)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Flatten())(x)
    
    x = TimeDistributed(Dense(128, activation='relu'))(x)
    x = Dropout(0.5)(x)

    x = LSTM(32, return_sequences=True)(x)
    x = LSTM(32)(x)
    
    ########################################################################################################################

    # Define the Model Architecture for Optical Flow stream
    ########################################################################################################################
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    y = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(flow_input)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Flatten())(y)
    
    y = TimeDistributed(Dense(128, activation='relu'))(y)
    y = Dropout(0.5)(y)

    y = LSTM(32, return_sequences=True)(y)
    y = LSTM(32)(y)
    
    ########################################################################################################################

    # Merge the two streams
    merged = concatenate([x, y])

    # Add final Dense layer for classification
    output = Dense(len(CLASSES_LIST), activation='softmax')(merged)

    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output)

    # Display the model's summary
    model.summary()
    
    # Return the constructed LSTM model.
    return model
74/14:
# Create the combined CNN-LSTM model.
convlstm_model = create_model()
# Display the success message. 
print("Model Created Successfully!")
74/15:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LSTM_model_training_history = convlstm_model.fit(
    x=[rgb_features_train, flow_features_train], 
    y=labels_train, 
    epochs=50, 
    batch_size=4,
    shuffle=True, 
    validation_split=0.2, 
    callbacks=[early_stopping_callback]
)
73/4:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/5:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/6:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/7:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/8:
# Import the required libraries.
import os
import cv2
73/9:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/10:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/11:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/12:
# Import the required libraries.
import os
import cv2
from collections import deque
73/13:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/14:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/15:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/16:
# Import the required libraries.
import os
import cv2
from collections import deque
from moviepy.editor import *
%matplotlib inline
73/17:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/18:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/19:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/20:
# Import the required libraries.
import os
import cv2
from collections import deque
from moviepy.editor import *
%matplotlib inline
73/21:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/22:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/23:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/24:
# Make the Output directory if it does not exist
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/25:
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/26:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/27:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/28:
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/29:
# Import the required libraries.
import os
import cv2
from collections import deque
from moviepy.editor import *
%matplotlib inline
import tensorflow as tf
73/30:
#Load Model
# Create a new model instance
model = create_model()
# Load the previously saved weights
model = tf.keras.models.load_model('path/to/your_model.h5')
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
convlstm_model 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/31:
#Load Model
# Load the previously saved weights
convlstm_model = tf.keras.models.load_model('path/to/your_model.h5')
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/32:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/33:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/34:
#Load Model
convlstm_model = tf.keras.models.load_model('OutputModel.h5')
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/35:
#Load Model
convlstm_model = tf.keras.models.load_model('OutputModel')
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/36:
#Load Model
convlstm_model = tf.keras.models.load_model('OutputModel.h5')
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/37:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/38:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/39:
#Load Model
convlstm_model = tf.keras.models.load_model('model.h5')
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/40:
#Load Model
convlstm_model = tf.keras.models.load_model('model.h5')
convlstm.summary()
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/41:
#Load Model
convlstm_model = tf.keras.models.load_model('model.h5')
convlstm_model.summary()
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/42:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/43:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/44:
# Import the required libraries.
import os
import cv2
from collections import deque
from moviepy.editor import *
%matplotlib inline
import tensorflow as tf
import tensorflow as tf
from tensorflow.keras.models import load_model
73/45:
#Load Model
convlstm_model = tf.keras.models.load_model('model.h5')
convlstm_model.summary()
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/46:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/47:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
convlstm_model.compile()
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/48:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
# Compile the model with the same configuration used during training
convlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
print("Model Loaded and Compiled Successfully!")
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/49:
# Import the required libraries.
import os
import cv2
from collections import deque
from moviepy.editor import *
%matplotlib inline
import tensorflow as tf
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam
73/50:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
# Compile the model with the same configuration used during training
convlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
print("Model Loaded and Compiled Successfully!")
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/51:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
# Compile the model with the same configuration used during training
convlstm_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])
print("Model Loaded and Compiled Successfully!")
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/52:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/53:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/54:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/55:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/56:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
convlstm.summary()
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/57:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
convlstm_model.summary()
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/58:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/59:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/60:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
convlstm_model.summary()
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/61:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/62:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/63:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
# compile the model
loaded_model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
convlstm_model.summary()
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/64:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
# compile the model
convlstm_model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
convlstm_model.summary()
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/65:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
# compile the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])
convlstm_model.summary()
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/66:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/67:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
74/16:
# Evaluate the trained model.
model_evaluation_history = convlstm_model.evaluate([rgb_features_test, flow_features_test], labels_test)
74/17:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define the string date format.
# Get the current Date and Time in a DateTime Object.
# Convert the DateTime object to string according to the style mentioned in date_time_format string.
date_time_format = '%Y_%m_%d__%H_%M_%S'
current_date_time_dt = dt.datetime.now()
current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = f'convlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'

# Save your Model.
convlstm_model.save(model_file_name)
74/18:
def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):
    '''
    This function will plot the metrics passed to it in a graph.
    Args:
        model_training_history: A history object containing a record of training and validation 
                                loss values and metrics values at successive epochs
        metric_name_1:          The name of the first metric that needs to be plotted in the graph.
        metric_name_2:          The name of the second metric that needs to be plotted in the graph.
        plot_name:              The title of the graph.
    '''
    
    # Get metric values using metric names as identifiers.
    metric_value_1 = model_training_history.history[metric_name_1]
    metric_value_2 = model_training_history.history[metric_name_2]
    
    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.
    epochs = range(len(metric_value_1))

    # Plot the Graph.
    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)
    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)

    # Add title to the plot.
    plt.title(str(plot_name))

    # Add legend to the plot.
    plt.legend()
74/19:
# Visualize the training and validation loss metrices.
plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
74/20:
# Visualize the training and validation loss metrices.
plot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
74/21:
# Visualize the training and validation accuracy metrices.
plot_metric(LSTM_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
74/22:
# Visualize the training and validation loss metrices.
plot_metric(LSTM_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
74/23:
# Visualize the training and validation accuracy metrices.
plot_metric(LSTM_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
74/24:
#Load video from samplevideos
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
74/25:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'avi'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
74/26:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
74/27:
#Load video from samplevideos
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'test'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
74/28:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'avi'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
74/29:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
74/30:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
74/31:
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
74/32:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = 'model.h5'

# Save your Model.
convlstm_model.save(model_file_name)
74/33:
# Get the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

# Define a useful name for our model to make it easy for us while navigating through multiple saved models.
model_file_name = 'model.h5'

# Save your Model.
convlstm_model.save(model_file_name)
73/68:
# Import the required libraries.
import os
import cv2
from collections import deque
from moviepy.editor import *
%matplotlib inline
import tensorflow as tf
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam
73/69:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
# compile the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])
convlstm_model.summary()
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
74/34:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
74/35:
# Construct the output video path.
output_video_file_path = 'output.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
74/36:
# Create the combined CNN-LSTM model.
classification_model = create_model()
# Display the success message. 
print("Model Created Successfully!")
74/37:
#Load video from samplevideos
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)

# Choose Test Video.
video_title = 'test'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
74/38:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'mp4v'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
74/39:
# Construct the output video path.
output_video_file_path = 'output.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
74/40:
# Visualize the training and validation loss metrices.
plot_metric(LSTM_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
74/41:
# Visualize the training and validation loss metrices.
plot_metric(LSTM_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')
74/42:
# Visualize the training and validation accuracy metrices.
plot_metric(LSTM_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
74/43:
# Visualize the training and validation accuracy metrices.
plot_metric(LSTM_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')
74/44:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
74/45:
def create_model():
    '''
    This function will construct the required two-stream LSTM model.
    Returns:
        model: It is the required constructed LSTM model.
    '''
    
    # Define the Model Architecture for RGB stream
    ########################################################################################################################
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(rgb_input)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Flatten())(x)
    
    x = TimeDistributed(Dense(128, activation='relu'))(x)
    x = Dropout(0.5)(x)

    x = LSTM(32, return_sequences=True)(x)
    x = LSTM(32)(x)
    
    ########################################################################################################################

    # Define the Model Architecture for Optical Flow stream
    ########################################################################################################################
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    y = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(flow_input)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Flatten())(y)
    
    y = TimeDistributed(Dense(128, activation='relu'))(y)
    y = Dropout(0.5)(y)

    y = LSTM(32, return_sequences=True)(y)
    y = LSTM(32)(y)
    
    ########################################################################################################################

    # Merge the two streams
    merged = concatenate([x, y])

    # Add final Dense layer for classification
    output = Dense(len(CLASSES_LIST), activation='softmax')(merged)

    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output)

    # Display the model's summary
    model.summary()
    
    # Return the constructed LSTM model.
    return model
74/46:
# Create the combined CNN-LSTM model.
classification_model = create_model()
# Display the success message. 
print("Model Created Successfully!")
74/47:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
LSTM_model_training_history = convlstm_model.fit(
    x=[rgb_features_train, flow_features_train], 
    y=labels_train, 
    epochs=50, 
    batch_size=4,
    shuffle=True, 
    validation_split=0.2, 
    callbacks=[early_stopping_callback]
)
75/1:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
75/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
75/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
75/4:
def save_frames(class_name, video_name, rgb_frames, flow_frames):
    '''
    This function saves the RGB and optical flow frames to their respective class directories.
    Args:
        class_name: Name of the class to which the frames belong.
        video_name: Name of the video file being processed.
        rgb_frames: List of RGB frames.
        flow_frames: List of optical flow frames.
    '''
    # Create directories for RGB and optical flow frames if they don't exist.
    rgb_output_dir = os.path.join('processed_frames', 'rgb', class_name, video_name)
    flow_output_dir = os.path.join('processed_frames', 'flow', class_name, video_name)
    os.makedirs(rgb_output_dir, exist_ok=True)
    os.makedirs(flow_output_dir, exist_ok=True)

    # Save each frame as an image file.
    for i, (rgb_frame, flow_frame) in enumerate(zip(rgb_frames, flow_frames)):
        rgb_frame_path = os.path.join(rgb_output_dir, f'frame_{i+1:04d}.jpg')
        flow_frame_path = os.path.join(flow_output_dir, f'frame_{i+1:04d}.jpg')
        cv2.imwrite(rgb_frame_path, (rgb_frame * 255).astype(np.uint8))
        cv2.imwrite(flow_frame_path, (flow_frame * 255).astype(np.uint8))
76/1:
# Import the required libraries.
import os
import cv2
import math
import random
import numpy as np
import datetime as dt
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt

from moviepy.editor import *
%matplotlib inline

from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import plot_model
76/2:
seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)
76/3:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
76/4:
# Create a Matplotlib figure and specify the size of the figure.
plt.figure(figsize = (20, 20))

# Get the names of all classes/categories in UCF50.
all_classes_names = os.listdir('Dataset/UCF-101')

# Generate a list of 8 random values. The values will be between 0-101, 
# where 101 is the total number of class in the dataset. 
random_range = random.sample(range(len(all_classes_names)), 8)

# Iterating through all the generated random values.
for counter, random_index in enumerate(random_range, 1):

    # Retrieve a Class Name using the Random Index.
    selected_class_Name = all_classes_names[random_index]

    # Retrieve the list of all the video files present in the randomly selected Class Directory.
    video_files_names_list = os.listdir(f'Dataset/UCF-101/{selected_class_Name}')

    # Randomly select a video file from the list retrieved from the randomly selected Class Directory.
    selected_video_file_name = random.choice(video_files_names_list)

    # Initialize a VideoCapture object to read from the video File.
    video_reader = cv2.VideoCapture(f'Dataset/UCF-101/{selected_class_Name}/{selected_video_file_name}')
    
    # Read the first frame of the video file.
    _, bgr_frame = video_reader.read()

    # Release the VideoCapture object. 
    video_reader.release()

    # Convert the frame from BGR into RGB format. 
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Write the class name on the video frame.
    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # Display the frame.
    plt.subplot(5, 4, counter);plt.imshow(rgb_frame);plt.axis('off')
76/5:
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 120, 120

# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 20

# Specify the directory containing the UCF50 dataset. 
DATASET_DIR = "dataset/UCF-101"

# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
76/6:
def frames_extraction(video_path):
    '''
    This function will extract the required frames from a video after resizing and normalizing them, and compute optical flow between consecutive frames.
    Args:
        video_path: The path of the video in the disk, whose frames are to be extracted.
    Returns:
        rgb_frames_list: A list containing the resized and normalized RGB frames of the video.
        flow_frames_list: A list containing the resized and normalized optical flow frames of the video.
    '''

    # Declare a list to store video frames.
    rgb_frames_list = []
    flow_frames_list = []
    
    # Read the Video File using the VideoCapture object.
    video_reader = cv2.VideoCapture(video_path)

    # Get the total number of frames in the video.
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))

    # Calculate the the interval after which frames will be added to the list.
    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return rgb_frames_list, flow_frames_list
    
    # Convert to grayscale for optical flow
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate through the Video Frames.
    for frame_counter in range(SEQUENCE_LENGTH):

        # Set the current frame position of the video.
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)

        # Reading the frame from the video. 
        success, current_frame = video_reader.read() 

        # Check if Video frame is not successfully read then break the loop
        if not success:
            break
            
        # Convert to grayscale for optical flow
        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        # Resize the Frame to fixed height and width.
        resized_frame = cv2.resize(current_frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255.0
        
        # Append the normalized frame into the frames list
        rgb_frames_list.append(normalized_frame)
        flow_frames_list.append(normalized_flow)

        # Set the current frame to the previous frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture object. 
    video_reader.release()

    # Return the frames list.
    return rgb_frames_list, flow_frames_list
76/7:
def save_frames(class_name, video_name, rgb_frames, flow_frames):
    '''
    This function saves the RGB and optical flow frames to their respective class directories.
    Args:
        class_name: Name of the class to which the frames belong.
        video_name: Name of the video file being processed.
        rgb_frames: List of RGB frames.
        flow_frames: List of optical flow frames.
    '''
    # Create directories for RGB and optical flow frames if they don't exist.
    rgb_output_dir = os.path.join('processed_frames', 'rgb', class_name, video_name)
    flow_output_dir = os.path.join('processed_frames', 'flow', class_name, video_name)
    os.makedirs(rgb_output_dir, exist_ok=True)
    os.makedirs(flow_output_dir, exist_ok=True)

    # Save each frame as an image file.
    for i, (rgb_frame, flow_frame) in enumerate(zip(rgb_frames, flow_frames)):
        rgb_frame_path = os.path.join(rgb_output_dir, f'frame_{i+1:04d}.jpg')
        flow_frame_path = os.path.join(flow_output_dir, f'frame_{i+1:04d}.jpg')
        cv2.imwrite(rgb_frame_path, (rgb_frame * 255).astype(np.uint8))
        cv2.imwrite(flow_frame_path, (flow_frame * 255).astype(np.uint8))
76/8:
def create_dataset():
    '''
    This function will extract the data of the selected classes and create the required dataset.
    Returns:
        features:          A list containing the extracted frames of the videos.
        labels:            A list containing the indexes of the classes associated with the videos.
        video_files_paths: A list containing the paths of the videos in the disk.
    '''

    # Declared Empty Lists to store the features, labels and video file path values.
    rgb_features = []
    flow_features = []
    labels = []
    video_files_paths = []
    
    # Iterating through all the classes mentioned in the classes list
    for class_index, class_name in enumerate(CLASSES_LIST):
        
        # Display the name of the class whose data is being extracted.
        print(f'Extracting Data of Class: {class_name}')
        
        # Get the list of video files present in the specific class name directory.
        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))
        
        # Iterate through all the files present in the files list.
        for file_name in files_list:
            
            # Get the complete video path.
            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)

            # Extract the frames of the video file.
            rgb_frames, flow_frames = frames_extraction(video_file_path)

            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.
            # So ignore the vides having frames less than the SEQUENCE_LENGTH.
            if len(rgb_frames) == SEQUENCE_LENGTH and len(flow_frames) == SEQUENCE_LENGTH:

                # Append the data to their repective lists.
                rgb_features.append(rgb_frames)
                flow_features.append(flow_frames)
                labels.append(class_index)
                video_files_paths.append(video_file_path)

                # Save the frames to the disk.
                video_name = os.path.splitext(file_name)[0]
                save_frames(class_name, video_name, rgb_frames, flow_frames)

    # Converting the list to numpy arrays
    rgb_features = np.asarray(rgb_features)
    flow_features = np.asarray(flow_features)
    labels = np.array(labels)  
    
    # Return the frames, class index, and video file path.
    return rgb_features, flow_features, labels, video_files_paths
76/9:
# Construct the dataset using the create_dataset method.
rgb_features, flow_features, labels, video_files_paths = create_dataset()
76/10:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
76/11:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
76/12:
def create_model():
    '''
    This function will construct the required two-stream LSTM model.
    Returns:
        model: It is the required constructed LSTM model.
    '''
    
    # Define the Model Architecture for RGB stream
    ########################################################################################################################
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(rgb_input)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Flatten())(x)
    
    x = TimeDistributed(Dense(128, activation='relu'))(x)
    x = Dropout(0.5)(x)

    x = LSTM(32, return_sequences=True)(x)
    x = LSTM(32)(x)
    
    ########################################################################################################################

    # Define the Model Architecture for Optical Flow stream
    ########################################################################################################################
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    y = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(flow_input)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Flatten())(y)
    
    y = TimeDistributed(Dense(128, activation='relu'))(y)
    y = Dropout(0.5)(y)

    y = LSTM(32, return_sequences=True)(y)
    y = LSTM(32)(y)
    
    ########################################################################################################################

    # Merge the two streams
    merged = concatenate([x, y])

    # Add final Dense layer for classification
    output = Dense(len(CLASSES_LIST), activation='softmax')(merged)

    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output)

    # Display the model's summary
    model.summary()
    
    # Return the constructed LSTM model.
    return model
76/13:
# Create the combined CNN-LSTM model.
classification_model = create_model()
# Display the success message. 
print("Model Created Successfully!")
76/14:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
classification_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
model_training_history = classification_model.fit(
    x=[rgb_features_train, flow_features_train], 
    y=labels_train, 
    epochs=50, 
    batch_size=4,
    shuffle=True, 
    validation_split=0.2, 
    callbacks=[early_stopping_callback]
)
76/15:
# Perform one-hot encoding on the labels.
one_hot_encoded_labels = to_categorical(labels)
76/16:
# Split the data into train and test sets using the train_test_split method.
rgb_features_train, rgb_features_test, flow_features_train, flow_features_test, labels_train, labels_test = train_test_split(
    rgb_features, flow_features, one_hot_encoded_labels, test_size=0.25, shuffle=True, random_state=seed_constant)
76/17:
def create_model():
    '''
    This function will construct the required two-stream LSTM model.
    Returns:
        model: It is the required constructed LSTM model.
    '''
    
    # Define the Model Architecture for RGB stream
    ########################################################################################################################
    rgb_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))
    
    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(rgb_input)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((4, 4)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Dropout(0.25))(x)
    
    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(x)
    x = TimeDistributed(MaxPooling2D((2, 2)))(x)
    x = TimeDistributed(Flatten())(x)
    
    x = TimeDistributed(Dense(128, activation='relu'))(x)
    x = Dropout(0.5)(x)

    x = LSTM(32)(x)
    
    ########################################################################################################################

    # Define the Model Architecture for Optical Flow stream
    ########################################################################################################################
    flow_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 1))
    
    y = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(flow_input)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((4, 4)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Dropout(0.25))(y)
    
    y = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(y)
    y = TimeDistributed(MaxPooling2D((2, 2)))(y)
    y = TimeDistributed(Flatten())(y)
    
    y = TimeDistributed(Dense(128, activation='relu'))(y)
    y = Dropout(0.5)(y)

    y = LSTM(32)(y)
    
    ########################################################################################################################

    # Merge the two streams
    merged = concatenate([x, y])

    # Add final Dense layer for classification
    output = Dense(len(CLASSES_LIST), activation='softmax')(merged)

    # Create the model
    model = Model(inputs=[rgb_input, flow_input], outputs=output)

    # Display the model's summary
    model.summary()
    
    # Return the constructed LSTM model.
    return model
76/18:
# Create the combined CNN-LSTM model.
classification_model = create_model()
# Display the success message. 
print("Model Created Successfully!")
76/19:
# Create an Instance of Early Stopping Callback.
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)
 
# Compile the model and specify loss function, optimizer and metrics to the model.
classification_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])

# Start training the model.
model_training_history = classification_model.fit(
    x=[rgb_features_train, flow_features_train], 
    y=labels_train, 
    epochs=50, 
    batch_size=4,
    shuffle=True, 
    validation_split=0.2, 
    callbacks=[early_stopping_callback]
)
73/70:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
# compile the model
convlstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ["accuracy"])
convlstm_model.summary()
# loaded model also has same accuracy, metrics and loss
loss, acc,loss1, loss2 = loaded_model.evaluate(x_test, y_test,verbose=1)
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/71:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
convlstm_model.summary()
# loaded model also has same accuracy, metrics and loss
loss, acc,loss1, loss2 = loaded_model.evaluate(x_test, y_test,verbose=1)
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/72:
# Import the required libraries.
import os
import cv2
from collections import deque
from moviepy.editor import *
%matplotlib inline
import tensorflow as tf
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam
73/73:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
convlstm_model.summary()
# loaded model also has same accuracy, metrics and loss
loss, acc,loss1, loss2 = loaded_model.evaluate(x_test, y_test,verbose=1)
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/74:
# Import the required libraries.
import os
import cv2
from collections import deque
from moviepy.editor import *
%matplotlib inline
import tensorflow as tf
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam
73/75:
# Load the model
model_file_name = 'model.h5'
convlstm_model = load_model(model_file_name)
convlstm_model.summary()
# loaded model also has same accuracy, metrics and loss
loss, acc,loss1, loss2 = loaded_model.evaluate(x_test, y_test,verbose=1)
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/76:
# Load the model
model_file_name = 'test_model.h5'
convlstm_model = load_model(model_file_name)
convlstm_model.summary()
# loaded model also has same accuracy, metrics and loss
loss, acc,loss1, loss2 = loaded_model.evaluate(x_test, y_test,verbose=1)
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/77:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/78:
# Load the model
model_file_name = 'test_model.h5'
convlstm_model = load_model(model_file_name)
convlstm_model.summary()
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'baby'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/79:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/80:
# Load the model
model_file_name = 'test_model.h5'
convlstm_model = load_model(model_file_name)
convlstm_model.summary()
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'test'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/81:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/82:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/83:
# Import the required libraries.
import os
import cv2
from collections import deque
from moviepy.editor import *
%matplotlib inline
import tensorflow as tf
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam
import numpy as np
73/84:
# Load the model
model_file_name = 'test_model.h5'
convlstm_model = load_model(model_file_name)
convlstm_model.summary()
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'test'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
73/85:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/86:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
73/87:
# Load the model
model_file_name = 'test_model.h5'
convlstm_model = load_model(model_file_name)
convlstm_model.summary()
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["ApplyEyeMakeup", "ApplyLipstick", "Archery", "BabyCrawling"]
#Take input video
test_videos_directory = 'SampleVideos'
os.makedirs(test_videos_directory, exist_ok = True)
 
# Choose Test Video.
video_title = 'test'
# Get the Video's path we just chose.
input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'
# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 120, 120
73/88:
def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):
    '''
    This function will perform action recognition on a video using the model.
    Args:
    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.
    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.
    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.
    '''

    # Initialize the VideoCapture object to read from the video file.
    video_reader = cv2.VideoCapture(video_file_path)

    # Get the width and height of the video.
    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize the VideoWriter Object to store the output video in the disk.
    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 
                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))

    # Declare a queue to store video frames.
    rgb_frames_queue = deque(maxlen=SEQUENCE_LENGTH)
    flow_frames_queue = deque(maxlen=SEQUENCE_LENGTH)

    # Initialize a variable to store the predicted action being performed in the video.
    predicted_class_name = ''

    # Read the first frame
    success, previous_frame = video_reader.read()
    if not success:
        print("Error: Could not read the first frame")
        return

    # Convert to grayscale
    previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
    
    # Iterate until the video is accessed successfully.
    while video_reader.isOpened():

        # Read the frame.
        ok, frame = video_reader.read() 
        
        # Check if frame is not read properly then break the loop.
        if not ok:
            break

        # Convert to grayscale
        current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow between the previous frame and the current frame
        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 
                                            0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Normalize the optical flow
        flow_magnitude, flow_angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitude = cv2.normalize(flow_magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_magnitude = np.uint8(flow_magnitude)
        
        # Resize the Frame to fixed Dimensions.
        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
        resized_flow = cv2.resize(flow_magnitude, (IMAGE_HEIGHT, IMAGE_WIDTH))

        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.
        normalized_frame = resized_frame / 255
        normalized_flow = resized_flow / 255

        # Appending the pre-processed frame into the frames list.
        rgb_frames_queue.append(normalized_frame)
        flow_frames_queue.append(normalized_flow)

        # Check if the number of frames in the queue are equal to the fixed sequence length.
        if len(rgb_frames_queue) == SEQUENCE_LENGTH and len(flow_frames_queue) == SEQUENCE_LENGTH:

            # Pass the normalized frames to the model and get the predicted probabilities.
            predicted_labels_probabilities = convlstm_model.predict([np.expand_dims(np.array(rgb_frames_queue), axis=0),
                                                            np.expand_dims(np.array(flow_frames_queue), axis=0)])[0]

            # Get the index of class with highest probability.
            predicted_label = np.argmax(predicted_labels_probabilities)

            # Get the class name using the retrieved index.
            predicted_class_name = CLASSES_LIST[predicted_label]

        # Write predicted class name on top of the frame.
        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write The frame into the disk using the VideoWriter Object.
        video_writer.write(frame)
        
        # Update the previous frame to the current frame for the next iteration
        previous_frame_gray = current_frame_gray

    # Release the VideoCapture and VideoWriter objects.
    video_reader.release()
    video_writer.release()
73/89:
SEQUENCE_LENGTH = 20
# Construct the output video path.
output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'

# Perform Action Recognition on the Test Video.
predict_on_video(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)

# Display the output video.
VideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()
   1: history -g -f history.txt
